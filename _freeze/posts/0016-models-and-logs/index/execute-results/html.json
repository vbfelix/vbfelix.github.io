{
  "hash": "1a756d7cf31d2f500a10ca7c700b6b52",
  "result": {
    "markdown": "---\ntitle: \"An intro to: Linear and log models\"\nauthor: \"Vinícius Félix\"\ndate: \"2023-09-06\"\ncategories: [Intro to,Theory]\nimage: \"intro-models-and-logs.png\"\n---\n\n::: {.cell}\n\n:::\n\n\nIn this post, we will see how logs aren't just for lumberjacks, but also for models.\n\n\n::: {.cell}\n\n:::\n\n\n# Linear-Linear\n\nA linear model (LM) is given by:\n\n$$\nY_i = \\beta_0 + \\beta_1x_1 + ...+\\beta_nx_n + \\varepsilon_i,\n$$ {#eq-linear-linear}\n\nwhere:\n\n-   $Y_i$ is the response variable;\n\n-   $\\beta_0$ is the intercept;\n\n-   $x_1$ is one explanatory variable;\n\n-   $\\beta_1$ is the slope coefficient of the explanatory variable $x_1$ ;\n\n-   $x_n$ is one explanatory variable;\n\n-   $\\beta_n$ is the slope coefficient of the explanatory variable $x_n$ ;\n\n-   $\\varepsilon_i$ is the error term\n\nTo understand how the model behavior let's see in a simple regression model\n\n$$\nY_i = \\beta_0 + \\beta_1x_1 + \\varepsilon_i.\n$$ {#eq-linear-linear-simple}\n\nA 1 unit increase in $x_1$ impact in $Y_i$ is:\n\n$$\n\\begin{align}\n\\beta_1(x_1+1) - \\beta_1x_1\n& = \\beta_1x_1+\\beta_1 - \\beta_1x_1 \\\\\n& = \\beta_1.\n\\end{align}\n$$ {#eq-linear-linear-coef}\n\n## Example\n\nSo, for example, if we adjust a linear model of human weight and height, first let's see how they behave.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nBecause we see a linear relationship between the variables, a linear model is not absurd; after adjusting a linear model, we get:\n\n$$\n\\mathrm{weight} = -47.6283 + 0.6995*\\mathrm{height},\n$$ {#eq-linear-linear-example}\n\nThat is, for every centimeter a person grows taller, they are estimated to be 0.6995 kg heavier. As a result, a one-unit increase in height results in a 0.6995-unit increase in weight.\n\n\n\n\n\nHowever, there are times when the relationship between our variables is not linear or additive. Given its properties, the logarithmic scale can be very useful, for more details check my post [An intro to: Logarithmic Scale](https://vbfelix.github.io/posts/0003-log-scale/).\n\n# Log-Linear\n\nA log-linear model implies that we apply the log to the response variable ($Y_i$),\n\n$$\n\\log{(Y_i)} = \\beta_0 + \\beta_1x_1 + ...+\\beta_nx_n + \\varepsilon_i,\n$$ {#eq-log-linear}\n\nTo understand how the model behavior let's see in a simple regression model\n\n$$\n\\log{(Y_i)} = \\beta_0 + \\beta_1x_1 + \\varepsilon_i,\n$$ {#eq-log-linear-simple}\n\nTo see how a 1 unit increase in $x_1$ implies in $Y_i$, we have to exponentiate it\n\n$$\n\\begin{align}\nY_i\n& = e^{\\log{(Y_i)}} \\\\\n& = e^{\\beta_0 + \\beta_1x_1 + \\varepsilon_i}\\\\\n& = e^{\\beta_0}e^{\\beta_1x_1}e^{\\varepsilon_i}.\n\\end{align}\n$$ {#eq-log-linear-exp}\n\nA 1 unit increase in $x_1$ impact in $Y_i$ is:\n\n$$\n\\begin{align}\n\\varepsilon^{\\beta_1(x_1+1)} - \\varepsilon^{\\beta_1x_1}\n& = \\varepsilon^{\\beta_1x_1+\\beta_1} - \\varepsilon^{\\beta_1x_1} \\\\\n& = \\varepsilon^{\\beta_1x_1+\\beta_1-\\beta_1x_1} \\\\\n& = \\varepsilon^{\\beta_1}. \n\\end{align}\n$$ {#eq-log-linear-coef}\n\nFor small values we have that\n\n$$\n\\varepsilon^\\beta \\approx 1 + \\beta.\n$$ {#eq-exp-approx}\n\nUsing the approximation of @eq-exp-approx in @eq-log-linear-coef we can say that for a positive coefficient $\\beta_1$, a one-unit increase in $x_1$ is associated with an approximate increase in $Y_i$ of\n\n$$\n\\begin{align}\n[100*(e^{\\beta_1}-1)]\\% \n& \\approx [100*(1+ \\beta_1-1)]\\% \\\\\n& \\approx (100*\\beta_1)\\%. \n\\end{align}\n$$ {#eq-log-linear-coef-pos}\n\nFor a negative coefficient $\\beta_1$, a one-unit increase is associated with an approximate decrease in $Y_i$ of\n\n$$\n\\begin{align}\n[100*(1-e^{\\beta_1})]\\% \n& \\approx [100*(1- 1 - \\beta_1)]\\% \\\\\n& \\approx -(100*\\beta_1)\\%. \n\\end{align}\n$$ {#eq-log-linear-coef-neg}\n\n# Linear-log\n\nA linear-log model implies that we apply the log to one or more explanatory variables ($x_i$),\n\n$$\nY_i = \\beta_0 + \\beta_1\\log{(x_1)} + ...+\\beta_n\\log{(x_n)} + \\varepsilon_i,\n$$ {#eq-linear-log}\n\nTo understand how the model behavior let's see in a simple regression model\n\n$$\nY_i = \\beta_0 + \\beta_1\\log{(x_1)} + \\varepsilon_i,\n$$ {#eq-linear-log-simple}\n\nLet's see how a 1 unit change in the log of $x_1$, considering $x_2 = x_1+1$,\n\n\\\n$$\n\\begin{align}\n\\log{(x_2)} - \\log{(x_1)}\n& = \\log{\\left(\\frac{x_2}{x_1}\\right)}. \\\\\n\\end{align}\n$$ {#eq-log-diff}\n\nTo see how the percent change is a linear approximation of the log difference, consider two values, $a$ and $b$, where the percent change is given by:\n\n$$\n\\frac{b-a}{a}.\n$$ {#eq-percent-change}\n\nConsidering the first order of the Taylor expansion of $\\log{(z)}$ around $z=1$ we have that\n\n$$\n\\log{(z)} \\approx z - 1.\n$$ {#eq-log-taylor-diff}\n\nAssuming that $\\frac{b}{a} \\approx 1$, we can apply the concept of @eq-log-taylor-diff to @eq-log-diff\n\n$$\n\\begin{align}\n\\log{\\left(\\frac{x_2}{x_1}\\right)} \n& \\approx \\frac{x_2}{x_1} - 1 \\\\\n& \\approx \\frac{x_2-x_1}{x_1}  \\\\\n& \\approx \\frac{(x_1+1)-x_1}{x_1}.  \\\\\n\\end{align}\n$$ {#eq-log-diff-approx-to-percent-change}\n\nApplying the @eq-percent-change in the @eq-log-diff-approx-to-percent-change, that is the equivalent to a 1 percent change, so for every 1% increase in $x_1$, $Y_i$ increases by about $\\beta_1/100$.\n\n# Log-log\n\nA linear-log model implies that we apply the log to both response ($Y_i$) and explanatory ($x_i$),\n\n$$\n\\log{(Y_i)} = \\beta_0 + \\beta_1\\log{(x_1)} + ...+\\beta_n\\log{(x_n)} + \\varepsilon_i,\n$$ {#eq-log-log}\n\nTo understand how the model behavior let's see in a simple regression model\n\n$$\n\\log{(Y_i)} = \\beta_0 + \\beta_1\\log{(x_1)} + \\varepsilon_i,\n$$ {#eq-log-log-simple}\n\nSince the log is applied to $x_1$ we can apply the same logic of @eq-log-diff-approx-to-percent-change and @eq-log-linear-coef-pos, for every 1% increase in $x_1$, $Y_i$ increases by $\\beta_1\\%$.\n\n## Example\n\nTo understand in an example, we will use a data with the average brain and body weights for 28 species of land animals.\n\nFirst, we will do a scatter plot of the two variables.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe relationship between the two variables is difficult to discern, as shown in the figure above, because some of the animals are outliers in terms of brain and body weight. As a result, we can \"compress\" this difference using the logarithm.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nAfter applying the logarithm, we can see in the log-log scale that the relationship between the animals' body and brain weight is linear. As a result, we will model them using a linear model, and for the sake of the example, we will use the brain as the response variable.\n\n\n\n\n\n$$\n\\log{(\\mathrm{brain})} = 2.555 + 0.496*\\log{(\\mathrm{body})}.\n$$ {#eq-log-log-example}\n\nSo for every 1% increase in body weight, the brain weight increases by 0.496%.\n\n# Considerations\n\nTo summarize we can we how the interpretation change for change scale\n\n| Scale         | Example                                                      | Interpretation                                                                           |\n|------------------|---------------------------|---------------------------|\n| Linear-linear | $Y_i = \\beta_0 + \\beta_1x_1 + \\varepsilon_i$                 | A 1 unit increase in $x_1$ implies in a $\\beta_1$ increase in $Y_i$.                     |\n| Log-Linear    | $\\log{(Y_i)} = \\beta_0 + \\beta_1x_1 + \\varepsilon_i$         | A 1 percent increase in $x_1$ implies in a $\\beta_1/100$ approximate increase in $Y_i$.  |\n| Linear-log    | $Y_i = \\beta_0 + \\beta_1\\log{(x_1)} + \\varepsilon_i$         | A 1 unit increase in $x_1$ implies in a $(100*\\beta_1)\\%$ approximate increase in $Y_i$. |\n| Log-log       | $\\log{(Y_i)} = \\beta_0 + \\beta_1\\log{(x_1)} + \\varepsilon_i$ | A 1 percent increase in $x_1$ implies in a $\\beta_1\\%$ approximate increase in $Y_i$.    |\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}