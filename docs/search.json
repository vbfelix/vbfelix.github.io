[
  {
    "objectID": "header-about.html",
    "href": "header-about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Vinícius. I am a statistician with experience in both technical and management positions."
  },
  {
    "objectID": "header-about.html#professional-experience",
    "href": "header-about.html#professional-experience",
    "title": "About me",
    "section": "Professional Experience",
    "text": "Professional Experience\n [02/2023 - Present] Ponta | Services Manager\n [04/2022 - 01/2023] GA + Intergado | Strategy Manager\n [01/2021 - 03/2022] GA | Head of R&D\n [09/2020 - 12/2020] GA | Head of Data\n [10/2018 - 11/2019] Unicesumar | Statistics Professor\n [09/2016 - 08/2020] H0 | Data Scientist Consultant and Co-founder\n [07/2013 - 12/2015] Estats Consultoria | Data Analyst and Co-founder"
  },
  {
    "objectID": "header-about.html#academic-education",
    "href": "header-about.html#academic-education",
    "title": "About me",
    "section": "Academic Education",
    "text": "Academic Education\n [2017 - 2019] Master of Science in Biostatistics – State University of Maringá – Brazil – Spatio-temporal geostatistics: modelling of natural phenomena in spacetime\n [2013 - 2017] Bachelor of Science in Statistics – State University of Maringá – Brazil"
  },
  {
    "objectID": "header-about.html#more-about-me",
    "href": "header-about.html#more-about-me",
    "title": "About me",
    "section": "More about me",
    "text": "More about me\n\n[2013-2017] Graduation\nDuring my graduate studies I focused mainly on time-series, geostatistics, and data visualization. I received two scholarships for the Brazilian Program of Scientific Initiation (PBIC), which required students to conduct a research and present them in a congress after one year, and I completed both of them:\n\n[2014] Topics on wavelet variance;\n[2015] Wavelet multiple cross-correlation.\n\nBesides that, I was also a member of the study group of spatial and temporal statistics, having worked with techniques to apply in time series and geostatistics, then I decided to do a final paper about clustering methods for temporal, spatio, and spatio-temporal data.\n\n[2013-2015] Junior enterprise\nIn addition, I was one of the founders of Estats Consultoria in 2013, a statistics junior enterprise, where I served as Marketing Director and, later, President, for three years.\nIn the projects I also worked as a data analyst, and later when I became a senior in college I also helped with the training of the freshmen.\nIn my last year I decided to enter the regional Junior Enterprises Nucleus, to help create a census for the junior entreprises in the region.\n\n\n\n[2017-2019] Post graduation\nI decided to pursue a master’s degree in Biostatistics after graduating in the beginning of 2017. My dissertation was about spatio-temporal geostatistics models, that was a natural choice since I had developed projects on temporal and spatial statistics during graduation, so I decided to unite them.\nAlso, I assisted one of my advisors in the development of the geotoolsR R package, providing code to apply bootstrap techniques in the context of geostatistics.\nLastly, I chose psychometrics as my optional credit in an applied area.\n\n\n[2016-2020] Data Science Consultant\nIn 2016, I co-founded the data consulting firm H0 Consultoria with a partner, where we collaborated on over 350 scientific studies and more than 40 business surveys for corporations. We were responsible for assisting others in making the most of statistics, such as doing:\n\nSampling design;\nData analysis;\nQuestionnaire review;\nStatistics training.\n\n\n\n[2018-2019] Statistics Professor\nAs I gained notoriety as a consultant, I was invited to teach statistics in the master of business administration program for business intelligence at Unicesumar.\nI updated the course topics by changing the material to a more modern and applied concept of statistics; despite the fact that it was only an introduction class, I used my hands-on experience to demonstrate real-world application of theoretical concepts.\n\n\n[2020-2020] Head of Data\nIn 2020 I received an invitation to start and grow the data team within GA’s Research & Development (R&D) division in 2020, GA was a Brazilian animal science tech company, that I consulted for some years. My main tasks as Head of Data were:\n\nHiring data scientists and data engineers;\nEstablishing a project methodology;\nHelping in the overall design of the data architeture;\nStarting a data-driven culture within the company.\n\nAlso as I learned more about software development, I also created my own R library for various functions to aid in data cleaning and visualization. The library is named relper, and it is available as an open source package to anyone who is interested.\n\n\n[2021-2022] Head of R&D\nLater, in the end of that year, I was given a promotion to lead the whole R&D division in 2021, and as such, I was also in charge of the software development team in addition to the data, my responsabilities were:\n\nImplementing automation projects using robots (Jira Automation), web scraping data (Python/R), and improving data monitoring (Metabase) and data-driven decision-making (Dremio);\nApplying agile methodologies and software development concepts in the execution of R&D projects (Jira Software) to drive continuous process improvement;\nManaging the department’s budget and expenses while analyzing and reporting on key performance indicators and project outcomes to the board of directors;\nBuilding project teams, developing schedules, and setting goals to meet overall needs.\n\n\n\n[2022-2023] Strategy Manager\nIn order to offer better actions through data science, GA merged with Intergado in 2022, a company that made hardware for accurately automating data collection, such as individual animal weight and provided software with the use of intelligence to make data-driven decisions easier.\nAs the company’s new strategy manager, my current responsibilities were:\n\nManaging the implementation of corporate strategy across all products;\nKeeping track of project status and identifying risks;\nCoordinating between technical and non-technical teams to align priorities and goals.\n\n\n\n[2023-2023] Services Manager\nI am in charge of overseeing service delivery within the organization. My responsibilities includes managing service teams, coordinating operations, and ensuring high-quality service delivery to meet customer needs and organizational goals, such as:\n\nHardware installation coordination: Managing the procurement, scheduling, and quality assurance of hardware installations.\nSoftware implementation oversight: Selecting and deploying appropriate software solutions, providing user training, and managing licenses.\nConsultancy: Deriving insights, identify trends, and help make informed decisions. Providing consultancy services based on data analysis to clients.\nTechnical support management: Establishing support procedures, handling inquiries and troubleshooting, and coordinating issue resolution with cross-functional teams.\n\nMy primary activities are:\n\nTeam leadership: Leading and managing a team of technicians, assigning tasks, evaluating performance, and fostering effective communication and collaboration.\nContinuous improvement: Staying updated with technology trends, implementing process enhancements, gathering feedback, and promoting best practices and standardization.\nKnowledge management: Systematic process of creating, organizing, and sharing knowledge to drive innovation and improve organizational performance to the whole team."
  },
  {
    "objectID": "header-certifications.html",
    "href": "header-certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "Data Analyst in SQL\n Data Analyst with Python\n Data Analyst with R\n\n\n\n Data Scientist with R\n\n\n\n R Programmer\n\n\n\n\n\n\n Data Analysis with Python\n Data Analysis with Python\n Data Manipulation with Python\n Data Visualization with Python\n Python Fundamentals\n\n\n\n Data Manipulation with R\n Data Visualization with R\n\n\n\n SQL Fundamentals\n\n\n\n\n\n\n How Google does Machine Learning"
  },
  {
    "objectID": "header-publications.html",
    "href": "header-publications.html",
    "title": "Publications",
    "section": "",
    "text": "SOUZA, E. M; FÉLIX, V. B. Wavelet Cross-correlation in Bivariate Time-Series Analysis. TEMA. Tendências em Matemática Aplicada e Computacional, v. 3, p. 391-403, 2018.\nFÉLIX, V. B.; MENEZES, A. F. B. Comparisons of ten corrections methods for t-test in multiple comparisons via Monte Carlo study. Electronic Journal of Applied Statistical Analysis, v. 11, p. 1, 2018.\nHENRIQUES, M. J.; FÉLIX, V. B.; GONZATTO, O. A.; SCHMIDT, F.; GUERRA, N.; OLIVEIRA NETO, A. M. A influência de herbicidas na reinfestação de plantas daninhas: Uma abordagem Bayesiana. REVISTA DA ESTATÍSTICA UFOP, v. VI, p. 140-144, 2017.\nFÉLIX, V. B.; GONZATTO, O. A.; ROSSONI, D. F.; HENRIQUES, M. J. ESTIMADORES DE SEMIVARIÂNCIA: UMA REVISÃO. CIÊNCIA E NATURA, v. 38, p. 1157, 2016.\n\n\n\n\n\nFÉLIX, V. B.; FERNANDES, L. B.; POSE, R. A. Open Source, o novo jeito de fazer ciência. Revista Ser Médico."
  },
  {
    "objectID": "header-publications.html#complete-works",
    "href": "header-publications.html#complete-works",
    "title": "Publications",
    "section": "Complete works",
    "text": "Complete works\n\nAUGUSTO JUNIOR, S. N.; FÉLIX, V. B. Survival analysis of the brazilian Spotify ranking: Differences between national and international artists. In: III International Seminar on Statistics with R, 2018, Rio de Janeiro. Survival analysis of the brazilian Spotify ranking: Differences between national and interionational artists, 2018.\nROSSONI, D. F.; FÉLIX, V. B. Métodos Bootstrap Para Dados Com Dependência Espacial. In: 60ª Reunião Anual da Região Brasileira da Sociedade Internacional de Biometria (RBras) e o 16º Simpósio de Estatística Aplicada a Experimentação Agronômica (SEAGRO), 2015, Presidente Prudente. Métodos Bootstrap Para Dados Com Dependência Espacial, 2015."
  },
  {
    "objectID": "header-publications.html#expanded-abstracts",
    "href": "header-publications.html#expanded-abstracts",
    "title": "Publications",
    "section": "Expanded abstracts",
    "text": "Expanded abstracts\n\nFÉLIX, V. B.; MENEZES, A. F. B. Monte Carlo study of multiple comparisons corrections in t-test. In: 5th Workshop on Probabilistic and Statistical Methods, 2017, São Carlos. Monte Carlo study of multiple comparisons corrections in t-test, 2017.\nFÉLIX, V. B.; ALVARENGA, B.; FERNANDES, L. B. Impacto causal de uma visita técnica no processo de fornecimento de uma fazenda via modelo estrutural temporal Bayesiano. In: I Encontro de Modelagem Estatística, 2017, Maringá. Impacto causal de uma visita técnica no processo de fornecimento de uma fazenda via modelo estrutural temporal Bayesiano, 2017.\nFÉLIX, V. B.; GARCIA, F.; FERNANDES, L. B. Controle de consumo bovino com intervalos de tolerância via modelos não-paramétricos. In: I Encontro de Modelagem Estatística, 2017, Maringá. Controle de consumo bovino com intervalos de tolerância via modelos não-paramétricos, 2017.\nFÉLIX, V. B.; HENRIQUES, M. J. ; GONZATTO, O. A. Modelos Espaciais para Predição de Dados Batimétricos. In: VII Congresso Científico Da Região Centro-Ocidental Do Paraná - CONCCEPAR, 2016, Campo Mourão. Modelos Espaciais para Predição de Dados Batimétricos, 2016.\nHENRIQUES, M. J. ; FÉLIX, V. B. ; GONZATTO, O. A. Abordagem Bayesiana no Controle de Brachiaria Plantaginea, Euphorbia Heterophylla e Richardia Brasiliensis com o Uso de Flumioxazin, Amicarbazone, Clomazone e Atrazine. In: VII Congresso Científico Da Região Centro-Ocidental Do Paraná - CONCCEPAR, 2016, Campo Mourão. Abordagem Bayesiana no Controle de Brachiaria Plantaginea, Euphorbia Heterophylla e Richardia Brasiliensis com o Uso de Flumioxazin, Amicarbazone, Clomazone e Atrazine, 2016.\nFÉLIX, V. B.; SOUZA, E. M. Correlação Múltipla Wavelet. In: XXV EAIC e V EAIC Jr, 2016, Maringá. Correlação Múltipla Wavelet, 2016.\nFÉLIX, V. B.; GUEDES, T. A. O impacto de diferentes concentrações de reguladores vegetais 2,4-D nos efeitos fisiológicos em Citrus sinensis com cancro cítrico, via regressão multivariada. In: I Workshop em Bioestatística, 2016, Maringá. O impacto de diferentes concentrações de reguladores vegetais 2,4-D nos efeitos fisiológicos em Citrus sinensis com cancro cítrico, via regressão multivariada, 2016.\nHENRIQUES, M. J.; GONZATTO, O. A.; FÉLIX, V. B.; SCHMIDT, F.; OLIVEIRA NETO, A. M. A Influência De Herbicidas Na Reinfestação De Plantas Daninhas: Uma Abordagem Bayesiana. In: 60ª Reunião Anual da Região Brasileira da Sociedade Internacional de Biometria (RBras) e o 16º Simpósio de Estatística Aplicada a Experimentação Agronômica (SEAGRO), 2015, Presidente Prudente. A Influência De Herbicidas Na Reinfestação De Plantas Daninhas: Uma Abordagem Bayesiana, 2015.\nSOUZA, E. M.; SAPUCCI, L.; FÉLIX, V. B. Inter-relation of time series from Cross Correlation Wavelets. In: XVI Escola de Séries Temporais e Econometria, 2015, Campos do Jordão. Inter-relation of time series from Cross Correlation Wavelets, 2015.\nFÉLIX, V. B.; ROSSONI, D. F. Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, através de métodos bootstrap. In: VI SEEMI - VI Simpósio de Estatística Espacial e Modelagem de Imagens, 2015, Toledo. Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, Através de métodos bootstrap, 2015.\nFÉLIX, V. B.; SOUZA, E. M. Tópicos em Variância Wavelet. In: XXIV EAIC e IV EAIC Jr, 2015, Maringá. Tópicos em Variância Wavelet, 2015.\nFÉLIX, V. B.; SOUZA, E. M. Análise De Variância Wavelet Aplicada Em Séries Temporais. In: 60ª Reunião Anual da Região Brasileira da Sociedade Internacional de Biometria (RBras) e o 16º Simpósio de Estatística Aplicada a Experimentação Agronômica (SEAGRO), 2015, Presidente Prudente. Análise De Variância Wavelet Aplicada Em Séries Temporais, 2015."
  },
  {
    "objectID": "header-publications.html#abstracts",
    "href": "header-publications.html#abstracts",
    "title": "Publications",
    "section": "Abstracts",
    "text": "Abstracts\n\nFÉLIX, V. B.; SOUZA, E. M.; ROSSONI, D. F. Classes de modelos de covariância para Geoestatística espaço-temporal. In: XIV Semana da Estatística da UEM, 2018, Maringá. Classes de modelos de covariância para Geoestatística espaço-temporal, 2018.\nMENEZES, A. F. B.; FÉLIX, V. B. Estudo de simulação Monte Carlo para testes post hoc. In: XIII Semana da Estatística, 2016, Maringá. Estudo de simulação Monte Carlo para testes post hoc, 2016.\nFÉLIX, V. B.; FURRIEL, W. O. Análise de perfil de Twitter dos 7 candidatos mais votados na eleição presidencial brasileira de 2014. In: XIII Semana da Estatística, 2016, Maringá. Análise de perfil de Twitter dos 7 candidatos mais votados na eleição presidencial brasileira de 2014, 2016.\nHENRIQUES, M. J.; GONZATTO, O. A.; FÉLIX, V. B. Uso do software R para análise da variabilidade espacial do teor de pH no solo em uma área experimental. In: VI Congresso Científico da Região Centro-Ocidental do Paraná, 2015, Campo Mourão. Uso do software R para análise da variabilidade espacial do teor de pH no solo em uma área experimental, 2015.\nGONZATTO, O. A.; HENRIQUES, M. J.; FÉLIX, V. B. Análise da variabilidade espacial da quantidade de argila no solo em uma parcela experimental. In: VI Congresso Científico da Região Centro-Ocidental do Paraná, 2015, Campo Mourão. Análise da variabilidade espacial da quantidade de argila no solo em uma parcela experimental, 2015.\nSOUZA, E. M.; SAPUCCI, L. F.; NEGRI, T. T.; FÉLIX, V. B. Low cost GPS-wavelet-based methodologies to advertise climate and environmental extreme events. In: 60th World Statistics Congress, 2015, Rio de Janeiro. Low cost GPS-wavelet-based methodologies to advertise climate and environmental extreme events, 2015.\nFÉLIX, V. B.; SOUZA, E. M.; MENEZES, A. F. B. Análise de cluster para séries temporais de internações por bronquiolite nas Regionais de saúde do Paraná. In: XII Semana da Estatística, 2015, Maringá. Análise de cluster para séries temporais de internações por bronquiolite nas Regionais de saúde do Paraná, 2015.\nFÉLIX, V. B.; SOUZA, E. M. Análise de Intervenção na importação/exportação de combustível nos Estados Unidos da América (EUA). In: XII Semana da Estatística, 2015, Maringá. Análise de Intervenção na importação/exportação de combustível nos Estados Unidos da América (EUA), 2015.\nFÉLIX, V. B.; GONZATTO, O. A.; HENRIQUES, M. J.; LANDGRAF, G. O.; ARAUJO, I. M.; ROSSONI, D. F. Comparação de Robustez dos Estimadores de Semivariância Aplicados a Dados Batimétricos. In: XI Semana da Estatística, 2014, Maringá. Comparação de Robustez dos Estimadores de Semivariância Aplicados a Dados Batimétricos, 2014.\nFÉLIX, V. B.; SOUZA, E. M. Correção Múltipla e Cruzada de Wavelets. In: XI Semana de Estatística, 2014, Maringá. Correção Múltipla e Cruzada de Wavelets, 2014.\nLANDGRAF, G. O.; ROSSONI, D. F.; FÉLIX, V. B. Existe diferença em mapas preditos por interpolação produzidos por diferentes softwares?. In: 45ª reunião regional da ABE e X semana de Estatística, 2013, Maringá. Existe diferença em mapas preditos por interpolação produzidos por diferentes softwares?, 2013."
  },
  {
    "objectID": "header-talks.html",
    "href": "header-talks.html",
    "title": "Events",
    "section": "",
    "text": "Gestão orientada a dados - Podcast Na Ponta da Língua #8\n\n\n\n\n\n\n Pecuária orientada a dados - 2º Encontro de Consultores Zoo Jr.\n\n\n\n\n\n Confinamento de bovinos, interpretando as respostas nutricionais através de dados e informações - SEVAM 2021 (Semana de Extensão Veterinária da Anhembi Morumbi)\n\n\n\n\n\n A Era da Convergência: O Peso da Gestão Analítica - ECR 2020 (Encontro de Confinamento e de Recriadores)\n\n\n\n\n\n Ciência de dados e IA, como se preparar? - 1º Action Time\n Degustando ferramentas de BI - 5º PowerBI Maringá\n Gráficos: O Limiar entre uma mensagem e uma mentira - FrontIn Maringá\n Ciência de Dados: Um Alicerce para Pecuária de Precisão - TICNOVA 2019\n Validando seu produto: Evitando uma morte prematura com dados - IxDA Maringá #3\n A Pecuária de Precisão - Semana de Gestão de Confinamento 2019 (Zoo Jr. - UEM)\n Data-Driven Product Development ft. Luis Berns - FEMUG #23\n O cenário da tecnologia em números - 1º AfroTech MGA\n Estatística: Um Arsenal para Tomada de Decisão - Eureka Moment\n Dissecando Métricas Ágeis - Maringá Agile #4\n\n\n\n\n\n Músicas no Spotify: Como vivem? E quanto tempo sobrevivem? - Cerveja com Dados Maringá #1\n Data Science: Magia ou Ciência? - Semana do programador DB1\n Data Science: Magia ou Ciência? - GDG Maringá: Ciência de dados\n Trabalhando com Consultoria Estatística - VIII SEst UFSCar/USP\n\n\n\n\n\n\n\n\n Correlação Múltipla Wavelet - XXV EAIC e V EAIC Jr.\n The Concept of 4D Datasets - I Workshop em Bioestatística\n\n\n\n\n\n Tópicos em Variância Wavelet - XXIV EAIC e IV EAIC Jr.\n Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, através de métodos bootstrap - VI SEEMI"
  },
  {
    "objectID": "header-talks.html#section-9",
    "href": "header-talks.html#section-9",
    "title": "Events",
    "section": "2023",
    "text": "2023\n\n Mentor at InovaAgro Hackathon - Maringá - Brazil"
  },
  {
    "objectID": "header-talks.html#section-10",
    "href": "header-talks.html#section-10",
    "title": "Events",
    "section": "2019",
    "text": "2019\n\n Mentor at Nasa Space Apps - Maringá - Brazil"
  },
  {
    "objectID": "header-talks.html#section-11",
    "href": "header-talks.html#section-11",
    "title": "Events",
    "section": "2018",
    "text": "2018\n\n Mentor at Nasa Space Apps - Maringá - Brazil\n Participant at Hackathon Romagnole - Maringá - Brazil (2th place)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my personal website!",
    "section": "",
    "text": "An intro to Monty Hall problem\n\n\n\n\n\n\nIntro to\n\n\nProbability\n\n\nR\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to Benford’s Law\n\n\n\n\n\n\nIntro to\n\n\nTheory\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn analysis of The King James Bible\n\n\n\n\n\n\nAn analysis of\n\n\nText Mining\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to normal distribution\n\n\n\n\n\n\nIntro to\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn analysis of Settlers of Catan\n\n\n\n\n\n\nAn analysis of\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to tidyverse operators\n\n\n\n\n\n\nIntro to\n\n\nTools\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to logarithmic scale\n\n\n\n\n\n\nIntro to\n\n\nTheory\n\n\nDataViz\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to chi-square test\n\n\n\n\n\n\nIntro to\n\n\nTheory\n\n\nHypothesis test\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to dplyr::across\n\n\n\n\n\n\nIntro to\n\n\nTools\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html",
    "href": "posts/0001-chi-square-test/index.html",
    "title": "An intro to chi-square test",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the chi-square test, one of the most well-known statistical tests."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#introduction",
    "href": "posts/0001-chi-square-test/index.html#introduction",
    "title": "An intro to chi-square test",
    "section": "Introduction",
    "text": "Introduction\nAfter all of these equations, we’ll perform a real-world example.\nLet’s say we have 200 animals in our random sample, and we want to see if race has anything to do with frame size. In order to see our observed values in each class, i.e., race x frame, we will first look at a contingency table with the absolute frequency of animals:\n\n\n\nFrame\nRace 1\nRace 2\nRace 3\nFrame Total\n\n\n\n\nSmall\n10\n20\n10\n40\n\n\nMedium\n20\n30\n20\n70\n\n\nLarge\n30\n50\n10\n90\n\n\nRace Total\n60\n100\n40\n200"
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#test-statistic",
    "href": "posts/0001-chi-square-test/index.html#test-statistic",
    "title": "An intro to chi-square test",
    "section": "Test statistic",
    "text": "Test statistic\nTo compute our statistic, as given by the equation Equation 1, we have to:\n\nCompute the expected value for each class (cell in terms of a contingency table);\nCompute the component \\(\\frac{(O_i -E_i)^2}{E_i}\\) fo each class;\nCompute the \\(X^2\\) statistic by summing all values of step 2.\n\nTo begin, we will perform the calculus for a single cell to demonstrate each step, and we will select the Race 1 x Small frame class. In this case, our observed value is 10, so we do the following to calculate the expected value:\n\\[\nE_1 = (40 \\times 60)/200 = 12.\n\\tag{15}\\]\nWe multiplied our marginal results and divided them by our sample size because one of our assumptions is that the variables are independent. We can now compute the component for the first cell using our expected value.\n\\[\n\\begin{align}\n& =  \\frac{(O_1 - E_1)^2}{E_1}  \\\\\n& =  \\frac{(10 - 12)^2}{12}  \\\\\n& =  \\frac{(-2)^2}{12}  \\\\\n& =  \\frac{4}{12} \\\\\n& =  1/3.\\\\\n\\end{align}\n\\tag{16}\\]\nNow we apply the calculus to each cell, computing the expected value for every class:\n\n\n\nFrame\nRace 1\nRace 2\nRace 3\n\n\n\n\nSmall\n12\n20\n8\n\n\nMedium\n21\n35\n14\n\n\nLarge\n27\n45\n18\n\n\n\nAnd then we can also compute \\(\\frac{(O_i -E_i)^2}{E_i}\\) for each one:\n\n\n\nFrame\nRace 1\nRace 2\nRace 3\n\n\n\n\nSmall\n0.33\n0\n0.50\n\n\nMedium\n0.05\n0.71\n2.57\n\n\n\n0.33\n0.56\n3.56\n\n\n\nLastly, we sum all the values to obtain the statistic \\(X^2\\), which is equal to 8.61."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#p-value",
    "href": "posts/0001-chi-square-test/index.html#p-value",
    "title": "An intro to chi-square test",
    "section": "p-value",
    "text": "p-value\nNow, if we want to obtain the p value we have to look at the chi-sqaure distribution, so first we need to obtain the number of degrees of freedom \\((q)\\), in this cases that is given by:\n\\[\nq = (n_r-1)\\times(n_c-1),\n\\tag{17}\\]\nwhere\n\n\\(n_r\\) is the number of rows in the contingency table, i.e., the number of levels of the respective categorical variable;\n\\(n_c\\) is the number of columns in the contingency table, i.e., the number of levels of the respective categorical variable.\n\nThen, we have in our example that\n\\[\n\\begin{align}\nq & =  (n_r-1)\\times(n_c-1)  \\\\\n& =  (3-1) \\times (3-1) \\\\\n& =  (2) \\times (2) \\\\\n& =  4.\\\\\n\\end{align}\n\\tag{18}\\]\nWith an established \\(q\\), we now can compute the p value given by:\n\\[\nP(\\chi_4^2 > X^2|H_0),\n\\tag{19}\\]\nor the probability that a value is larger than \\(X^2\\) given a \\(\\chi_4^2\\) distribution and a true null hypothesis, as we can see in the figure below:\n\n\n\n\n\nWith a p value of 0.0716, we have that the p value is greater than 0.05, so we do not reject the null hypothesis, i.e., we do not have sample evidence to reject the null hypothesis that race and size frame are independent.."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#contingency-table",
    "href": "posts/0001-chi-square-test/index.html#contingency-table",
    "title": "An intro to chi-square test",
    "section": "Contingency table",
    "text": "Contingency table\nIf you have your contingency table ready, an easy and quick way to apply the test is to create a matrix with the observed values of each class.\n\n#matrix with the count \ndata <- matrix(data = c(10,20,30,20,30,50,10,20,10),ncol = 3)\ndata\n\n     [,1] [,2] [,3]\n[1,]   10   20   10\n[2,]   20   30   20\n[3,]   30   50   10\n\nchisq.test(data)\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 8.6111, df = 4, p-value = 0.07159\n\n\nWe can see that the function already show us the statistic, number of degrees of freedom and the p value."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#raw-data",
    "href": "posts/0001-chi-square-test/index.html#raw-data",
    "title": "An intro to chi-square test",
    "section": "Raw data",
    "text": "Raw data\nWe usually work with raw data as data analysts, where each row represents one observation. In this case, we can transform our data to perform the same function as in the previous example.\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n#Raw data simulation\ndata <-\n  expand_grid(\n    race  = c(1:3), \n    frame = factor(c(\"S\",\"M\",\"L\"))\n  ) %>% \n  arrange(race,frame) %>% \n  mutate(n  = c(10,20,30,20,30,50,10,20,10)) %>% \n  uncount(n)\n\ndata\n\n# A tibble: 200 x 2\n    race frame\n   <int> <fct>\n 1     1 L    \n 2     1 L    \n 3     1 L    \n 4     1 L    \n 5     1 L    \n 6     1 L    \n 7     1 L    \n 8     1 L    \n 9     1 L    \n10     1 L    \n# ... with 190 more rows\n\ndata %>% \n  #Number of observed values for each class\n  count(race,frame) %>% \n  #Pivot table to make a contingency table\n  pivot_wider(names_from = race,values_from = n) %>% \n  #Removal of the variable as column\n  select(-1) %>% \n  #Chi-square test\n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 8.6111, df = 4, p-value = 0.07159"
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html",
    "href": "posts/0002-dplyr-across/index.html",
    "title": "An intro to dplyr::across",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the function across of the R package dplyr."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#before-across",
    "href": "posts/0002-dplyr-across/index.html#before-across",
    "title": "An intro to dplyr::across",
    "section": "Before across",
    "text": "Before across\nAs one of thre greatest R packages, dplyr possesses a lot functions, but it has two main verbs to manipulate data, they are:\n\nsummarise: allows us to apply a transformation to data that reduce the number of observations, e.g., mean;\nmutate: allows us to apply a transformation to our existing variables or even creating new ones with the same size, e.g., multiplying one variable by 2.\n\nLet’s see how they work in practice:\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               <fct> male, female, female, NA, female, male, female, male~\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nWe will summarize every numerical variable using the dataset penguins from the palmerpenguins package, computing the mean for each. The mean function can then be applied to each variable inside the verb summarize.\n\npenguins %>% \n  summarise(\n    mean(bill_length_mm,na.rm = TRUE),\n    mean(bill_depth_mm,na.rm = TRUE),\n    mean(flipper_length_mm,na.rm = TRUE),\n    mean(body_mass_g,na.rm = TRUE)\n  ) %>% \n  glimpse()\n\nRows: 1\nColumns: 4\n$ `mean(bill_length_mm, na.rm = TRUE)`    <dbl> 43.92193\n$ `mean(bill_depth_mm, na.rm = TRUE)`     <dbl> 17.15117\n$ `mean(flipper_length_mm, na.rm = TRUE)` <dbl> 200.9152\n$ `mean(body_mass_g, na.rm = TRUE)`       <dbl> 4201.754\n\n\nIn the example above we see that it works, but have some problems:\n\nDue to its manual nature and increased risk of human error from writing numerous lines of code or even copying and pasting it, it would become a tiresome task if there were many columns;\nThe function will be given to the new tvariables as their names if their names are not set.\n\nA smarter approach is the use of a summarise variant, called summarise_if.\n\npenguins %>% \n  summarise_if(\n    .predicate = is.numeric,\n    .funs = ~mean(.,na.rm = TRUE)\n  ) %>% \n  glimpse()\n\nRows: 1\nColumns: 5\n$ bill_length_mm    <dbl> 43.92193\n$ bill_depth_mm     <dbl> 17.15117\n$ flipper_length_mm <dbl> 200.9152\n$ body_mass_g       <dbl> 4201.754\n$ year              <dbl> 2008.029\n\n\nIn the example above we see that inside summarise_if we define two argumens:\n\n.predicate: the condition to check which variables we are going to apply the functions;\n.funs: a function or list of functions.\n\nEven though these variables are the means of the originals, unlike the first method, the function here kept the names of the original variables. The fact that we can now apply a function to 5 columns with only 2 lines of code is another advantage.\nSo it was successful, but what is the issue? What if I also wanted to learn the mode of the variable species? How could we go about doing that?\n\npenguins %>% \n  summarise(\n    species = relper::calc_mode(species),\n    across(.cols = where(is.numeric),.fns = ~mean(.,na.rm = TRUE))\n  ) %>% \n  glimpse()\n\nRows: 1\nColumns: 6\n$ species           <fct> Adelie\n$ bill_length_mm    <dbl> 43.92193\n$ bill_depth_mm     <dbl> 17.15117\n$ flipper_length_mm <dbl> 200.9152\n$ body_mass_g       <dbl> 4201.754\n$ year              <dbl> 2008.029\n\n\nIn the example above we apply across , we see that it is used inside the conventional verb summarise , meaning we can still apply other functions even using across.\nSo across is a function that is complementary to mutate and summarise, that allows us to apply multiples functions across multiples variables.\nJust as curiosity, even though this old functions are superseeded they still exists, and their suffixes are _at() , _if() and _all() ."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#cols",
    "href": "posts/0002-dplyr-across/index.html#cols",
    "title": "An intro to dplyr::across",
    "section": ".cols",
    "text": ".cols\nThe first argument of across determine which columns of the data.frame we are going to apply our functions, this argument is:\n\nNon-optional\nThe default is every single variable of the data.frame, by using the function everything.\nAccepts as input:\n\nIntegers, referencing the variables positions;\nStrings, referencing the variables names;\nSelect helpers functions, e.g., contains, as we will see below.\n\n\n\nDefault\n\npenguins %>% \n  summarise(across(.fns = as.character)) %>% \n  glimpse()\n\nRows: 344\nColumns: 8\n$ species           <chr> \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A~\n$ island            <chr> \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", ~\n$ bill_length_mm    <chr> \"39.1\", \"39.5\", \"40.3\", NA, \"36.7\", \"39.3\", \"38.9\", ~\n$ bill_depth_mm     <chr> \"18.7\", \"17.4\", \"18\", NA, \"19.3\", \"20.6\", \"17.8\", \"1~\n$ flipper_length_mm <chr> \"181\", \"186\", \"195\", NA, \"193\", \"190\", \"181\", \"195\",~\n$ body_mass_g       <chr> \"3750\", \"3800\", \"3250\", NA, \"3450\", \"3650\", \"3625\", ~\n$ sex               <chr> \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f~\n$ year              <chr> \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"200~\n\n\nIn the example above we apply the function as.character to every column, since we did not use an input to the argument .cols.\n\npenguins %>% \n  summarise(across(.cols = everything(),.fns = as.character)) %>% \n  glimpse()\n\nRows: 344\nColumns: 8\n$ species           <chr> \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A~\n$ island            <chr> \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", ~\n$ bill_length_mm    <chr> \"39.1\", \"39.5\", \"40.3\", NA, \"36.7\", \"39.3\", \"38.9\", ~\n$ bill_depth_mm     <chr> \"18.7\", \"17.4\", \"18\", NA, \"19.3\", \"20.6\", \"17.8\", \"1~\n$ flipper_length_mm <chr> \"181\", \"186\", \"195\", NA, \"193\", \"190\", \"181\", \"195\",~\n$ body_mass_g       <chr> \"3750\", \"3800\", \"3250\", NA, \"3450\", \"3650\", \"3625\", ~\n$ sex               <chr> \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f~\n$ year              <chr> \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"200~\n\n\nIn the example above we see that same result is obtained from the previous, since the default of .cols is everything.\n\n\nBy type\nIf we want to select variables by their type we can use the function where + a function that check the variable type.\n\npenguins %>% \n  mutate(across(.cols = where(is.factor),.fns = toupper)) %>% \n  glimpse()\n\nRows: 344\nColumns: 8\n$ species           <chr> \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"A~\n$ island            <chr> \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", ~\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               <chr> \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F~\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nIn the example above we made all factor variables to be uppercase.\nOther functions can also be used, such as:\n\nis.numeric: check if the variable is numeric;\n\nis.integer check if the variable is an integer;\nis.double check if the variable is a double;\n\nis.factor check if the variable is a factor;\nis.character check if the variable is a character;\nis.logical check if the variable is a boolean (TRUE/FALSE).\n\nWe can also combine more than one function in the same across:\n\npenguins %>% \n  mutate(across(.cols = where(is.factor) | where(is.character),.fns = toupper)) %>%   glimpse()\n\nRows: 344\nColumns: 8\n$ species           <chr> \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"A~\n$ island            <chr> \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", ~\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               <chr> \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F~\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nIn the example above we made all factor (species and island) and character (sex) variables to be uppercase.\n\n\nBy name\nAnother method of column selection is by using their name.\n\npenguins %>% \n  summarise(across(.cols = ends_with(\"_mm\"),.fns = ~mean(.,na.rm = TRUE))) %>% \n  glimpse()\n\nRows: 1\nColumns: 3\n$ bill_length_mm    <dbl> 43.92193\n$ bill_depth_mm     <dbl> 17.15117\n$ flipper_length_mm <dbl> 200.9152\n\n\nIn the example above we compute the mean for the variables that ends with the pattern _mm.\nSo all the the selection helpers can be used:\n\nall_of: allows us to pass a string vector to select specific variables, that helps when we are looking for a group of variables, which not obey a simples check condition such as been of the same type or having the a name pattern, e.g., all_of(vector_of_variables) ;\nany_of: is a similar function to all_of , but it can be used to remove variables with the operator -, e.g., any_of(-vector_of_variables) ;\ncontains: allows to select variables that contains a specific string in their names. e.g., contains(length);\nends_with: variables that ends with a specific string pattern, e.g., ends_with(\"_mm\");\neverything: all variables, and already the default of the argument .cols;\nlast_col: the last variable of the data.frame;\nmatches: variables with a name that matches a given regular expression;\nnum_range: variables that have a numeric sequence in their name, e.g., var1, var2 and var3 then we can use num_range(\"var\",1:3);\nstarts_with: variables that starts with a specific string pattern, e.g., starts_with(\"bill_\").\n\n\n\nBy order\nAnother method of column selection is using the name of the variables and the operator : to apply the function to a sequence of variables.\n\npenguins %>% \n  summarise(across(.cols = bill_length_mm:body_mass_g,.fns = ~mean(.,na.rm = TRUE))) %>%\n  glimpse()\n\nRows: 1\nColumns: 4\n$ bill_length_mm    <dbl> 43.92193\n$ bill_depth_mm     <dbl> 17.15117\n$ flipper_length_mm <dbl> 200.9152\n$ body_mass_g       <dbl> 4201.754\n\n\nIn the example above we compute the mean to every variable from bill_length_mm to body_mass_g.\nWe can see that this variables are third to sixth of the data.frame, then can also use a method to reference them by their position.\n\npenguins %>% \n  summarise(across(.cols = 3:6,.fns = ~mean(.,na.rm = TRUE))) %>%\n  glimpse()\n\nRows: 1\nColumns: 4\n$ bill_length_mm    <dbl> 43.92193\n$ bill_depth_mm     <dbl> 17.15117\n$ flipper_length_mm <dbl> 200.9152\n$ body_mass_g       <dbl> 4201.754\n\n\nIn the example above we computed the mean for the same variables as before, but now using their column position instead."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#fns",
    "href": "posts/0002-dplyr-across/index.html#fns",
    "title": "An intro to dplyr::across",
    "section": ".fns",
    "text": ".fns\nThe argument .fns determine which functions are going to be applied, this argument is:\n\nNon-optional\nNo default\nAccepts as input:\n\nSingle function;\nList of functions.\n\n\n\npenguins %>% \n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = list(\n        ~mean(.,na.rm = TRUE),\n        ~median(.,na.rm = TRUE)\n      )\n    )\n  ) %>% \n  glimpse()\n\nRows: 1\nColumns: 10\n$ bill_length_mm_1    <dbl> 43.92193\n$ bill_length_mm_2    <dbl> 44.45\n$ bill_depth_mm_1     <dbl> 17.15117\n$ bill_depth_mm_2     <dbl> 17.3\n$ flipper_length_mm_1 <dbl> 200.9152\n$ flipper_length_mm_2 <dbl> 197\n$ body_mass_g_1       <dbl> 4201.754\n$ body_mass_g_2       <dbl> 4050\n$ year_1              <dbl> 2008.029\n$ year_2              <dbl> 2008\n\n\nThe mean and median are computed in the aforementioned example, but since more than one function is applied to the same variable, a numerical suffix is added based on the order in which our functions were defined inside the list, making mean 1 and median 2. This can be confusing and lead to errors later on.\n\npenguins %>% \n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = list(\n        mean = ~mean(.,na.rm = TRUE),\n        median = ~median(.,na.rm = TRUE)\n      )\n    )\n  ) %>% \n  glimpse()\n\nRows: 1\nColumns: 10\n$ bill_length_mm_mean      <dbl> 43.92193\n$ bill_length_mm_median    <dbl> 44.45\n$ bill_depth_mm_mean       <dbl> 17.15117\n$ bill_depth_mm_median     <dbl> 17.3\n$ flipper_length_mm_mean   <dbl> 200.9152\n$ flipper_length_mm_median <dbl> 197\n$ body_mass_g_mean         <dbl> 4201.754\n$ body_mass_g_median       <dbl> 4050\n$ year_mean                <dbl> 2008.029\n$ year_median              <dbl> 2008\n\n\nSince we defined the names of the functions in the example above and added them automatically as suffixes, it is now clearer what we are doing."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#names",
    "href": "posts/0002-dplyr-across/index.html#names",
    "title": "An intro to dplyr::across",
    "section": ".names",
    "text": ".names\nThe argument .names determines the name of resultant the variables after the functions are applied, so it allows us to change the names of the variables, this argument is:\n\nOptional\nThe default is NULL\nAccepts as input:\n\nA string, where we can use {.col} and {.fn} as variables to receive the respective names of the columns and/or functions.\n\n\n\npenguins %>% \n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = list(\n        mean = ~mean(.,na.rm = TRUE),\n        median = ~median(.,na.rm = TRUE)\n      ),\n      .names = \"{.fn}----{.col}\"\n    )\n  ) %>% \n  glimpse()\n\nRows: 1\nColumns: 10\n$ `mean----bill_length_mm`      <dbl> 43.92193\n$ `median----bill_length_mm`    <dbl> 44.45\n$ `mean----bill_depth_mm`       <dbl> 17.15117\n$ `median----bill_depth_mm`     <dbl> 17.3\n$ `mean----flipper_length_mm`   <dbl> 200.9152\n$ `median----flipper_length_mm` <dbl> 197\n$ `mean----body_mass_g`         <dbl> 4201.754\n$ `median----body_mass_g`       <dbl> 4050\n$ `mean----year`                <dbl> 2008.029\n$ `median----year`              <dbl> 2008\n\n\nIn the example above we change the variables names so they start with the function applied followed by 4 hyphens and then the original columns names."
  },
  {
    "objectID": "posts/0003-log-scale/index.html",
    "href": "posts/0003-log-scale/index.html",
    "title": "An intro to logarithmic scale",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the logarithmic scale, and how useful it can be in data visualization."
  },
  {
    "objectID": "posts/0003-log-scale/index.html#exponentiation",
    "href": "posts/0003-log-scale/index.html#exponentiation",
    "title": "An intro to logarithmic scale",
    "section": "Exponentiation",
    "text": "Exponentiation\nWhen learning math, we usually start by the fundamental arithmetic operations:\n\nAddiction and Subtraction;\nMultiplication and Division.\n\nAfter the basics, the next step is the exponentiation, which is essentially a two-number math operation, where:\n\n\\(b\\) is the base, the value that will be exponentiated;\n\\(n\\) is the exponent, the value that the base will be raised by the power of.\n\nThen, we say that \\(b\\) is raised to the power of \\(n\\), meaning that:\n\\[\nb^n = b_{[1]} \\times b_{[2]} \\times ... \\times b_{[n-1]} \\times b_{[n]} = x, \\quad n \\in \\mathbb{Z}^{++}.\n\\tag{1}\\]\nAs stated in the Equation 1, \\(b\\) is multiplied by itself \\(n\\) times, and this property of the exponentiation is given for any positive integer \\(n\\).\nSo, let’s say we want to exponentiate the number 10 to the power of 3, that is:\n\\[\n10^3 = 10\\times 10 \\times 10 = 100 \\times 10 = 1000.\n\\tag{2}\\]\nIn the example above, 3 is the exponent and 10 is the base, resulting in 1,000.\nNext, let’s understand how the exponential function works in general. For the first scenario, we will use 3 exponents (2, 3 and 4) applied to a sequence of bases from -1 to 1.\n\n\n\n\n\nLooking at the figure above we can see some interesting behaviors:\n\n\\(b^n = 0\\) when \\(b = 0\\);\n\\(b^n = 1\\) when \\(b = 1\\), for all \\(n\\);\n\\(b^n = 1\\) when \\(b = -1\\) and \\(n\\) is even;\n\\(b^n = -1\\) when \\(b = -1\\) and \\(n\\) is odd;\n\\(b^n > 0\\) when \\(n\\) is even, since multiplying an negative value for an even number of times yields a positive result;\n\\(b^n > b\\) when \\(1 > b > 0\\), to help our visualization of this behavior, we will plot a identity line dashed with the color red, i.e., \\(b^n = b\\).\n\n\n\n\n\n\nWe see that the values are below the line for positives bases, hence:\n\\[\n1 >  b > 0  \\longrightarrow b^n > b, \\quad n \\in \\mathbb{Z}^{++}.\n\\tag{3}\\]\nBut let’s keep in mind that until now we worked with bases between -1 and 1, will see how the exponentiation behavior for \\(b >= 1\\) next.\n\n\n\n\n\nLooking at values > 1 for our base, we see how fast the results of \\(b^n\\) grows for a higher \\(n\\).\nAfter seeing how the behavior is for differents bases, we will do the same for the exponents. As we applied integer positive values for our exponents in the examples before, let’s see how fractional exponents behavior.\n\n\n\n\n\nWe can see that for negatives values of \\(b\\) there are not defined values of \\(b^n\\), but why is that?\n\\[\nb^{\\frac{n}{m}} = (b^n)^{\\frac{1}{m}} = \\sqrt[m]{b^n}.\n\\tag{4}\\]\nIn the Equation 4 we look how a fractional exponent is actually the \\(m\\)th root of \\(b^n\\), so if \\(m\\) is even, there would be no real solution, since a even root of a negative value is not defined for real numbers.\nAfter understanding the basics of the the exponentiation we can jump to the inverse operation, the logarithm (\\(\\log\\))."
  },
  {
    "objectID": "posts/0003-log-scale/index.html#logarithm",
    "href": "posts/0003-log-scale/index.html#logarithm",
    "title": "An intro to logarithmic scale",
    "section": "Logarithm",
    "text": "Logarithm\nSo, let’s see how \\(\\log\\) works, since it is the inverse of the exponentiation, we can write it as:\n\\[\n\\log_b(x) = n \\longleftrightarrow b^n = x.\n\\tag{5}\\]\nAs stated in the equation above, the \\(\\log\\) function gives what is the exponent (\\(n\\)) we have to raise our base (\\(b\\)) to result in \\(x\\).\nApplying this logic, we can see the Equation 2 as:\n\\[\n\\log_{10}(1000) = 3.\n\\tag{6}\\]\nSo the \\(\\log\\) is which number 10 has to be exponiented to result in 1,000, then we can easily expand this to show other results:\n\n\n\n\n\n\n\n\\(\\log_{10}(x)\\)\n\\(x\\)\n\n\n\n\n-5\n0.00001\n\n\n-4\n0.0001\n\n\n-3\n0.001\n\n\n-2\n0.01\n\n\n-1\n0.1\n\n\n0\n1\n\n\n1\n10\n\n\n2\n100\n\n\n3\n1000\n\n\n4\n10,000\n\n\n5\n100,000\n\n\n\nIn the table above we see thjat the result of the \\(\\log_{10}(x)\\) increase 1 unit as the value in \\(x\\) is multiplied by 10. This is a very helpful property, that will explore more later.\nJust as we did with the exponentiation, let’s see how the \\(\\log\\) behavior, so we will apply the function to a \\(x\\) varying from -1 to 11 with three different bases (2, \\(\\mathcal{e}\\) and 10).\n\n\n\n\n\nLooking at the figure above we can see some interesting behaviors:\n\n\\(n\\) is nonexistent when \\(x < 0\\), as we saw earlier for somes cases in the exponentiation would result in a undefined number;\n\\(n\\) is equal to 1 when \\(x = b\\);\n\\(n\\) is equal to 0 when \\(x = 1\\);\n\\(n\\) is negative when \\(x < 1\\).\n\nBefore exploring more of the logarithm properties, you can be asking what it is the \\(\\mathcal{e}\\) used in the example before?\n\nNatural logarithm\nThis logarithm is a special case, where the base of the \\(\\log\\) is the number \\(\\mathcal{e}\\), also known as Euler’s number or Napier’s constant, defined by:\n\\[\n\\mathcal{e} = \\sum_{n = 0}^{\\infty}\\frac{1}{n!} = \\frac{1}{1} + \\frac{1}{1\\times2}+ \\frac{1}{1\\times2\\times3} + ... \\approx 2.718282.\n\\tag{7}\\]\nSo the natural logarithm can be written as:\n\\[\n\\log_{\\mathcal{e}}(x) = \\mathrm{ln}(x).\n\\tag{8}\\]\n\n\nProperties\nThe \\(\\log\\) function has many properties that helps us in many situations, let’s see the main properties.\n\nProduct\nThe first property is that the \\(\\log\\) of the products of \\(x\\) and \\(y\\) is the same as the sum of the \\(\\log\\)’s of \\(x\\) and \\(y\\), that is:\n\\[\n\\log_b(xy) = \\log_b(x) + \\log_b(y).\n\\tag{9}\\]\nTo see that, let’s use the Equation 5 and define a second logarithm as:\n\\[\n\\log_b(y) = m \\longleftrightarrow y = b^m.\n\\tag{10}\\]\nBy applying the product property of the exponentiation we have that:\n\\[\nxy = b^n \\ b^ m = b^{(n+m)}.\n\\tag{11}\\]\nNow, if we apply the Equation 11 in the Equation 9, our result is:\n\\[\n\\begin{align}\n\\log_b(xy)\n&= \\log_b(b^{(n + m)}) \\\\\n&= n + m \\\\\n&= \\log_b(x) + \\log_b(y).\n\\end{align}\n\\tag{12}\\]\n\n\nQuotient\nThe second property is that the \\(\\log\\) of the division of \\(x\\) and \\(y\\) is the same as the subtraction of the \\(\\log\\)’s of \\(x\\) and \\(y\\), that is:\n\\[\n\\log_b\\left(\\frac{x}{y}\\right) = \\log_b(x) - \\log_b(y).\n\\tag{13}\\]\nUsing the same definitions of Equation 5 and Equation 10, we have that:\n\\[\n\\frac{x}{y} = \\frac{b^n}{b^m} = b^{n-m},\n\\tag{14}\\]\nif we aplly the Equation 14 in the Equation 13, our result is:\n\\[\n\\begin{align}\n\\log_b\\left(\\frac{x}{y}\\right)\n&= \\log_b(b^{n-m}) \\\\\n&= n-m \\\\\n&= \\log_b(x) - \\log_b(y).\n\\end{align}\n\\tag{15}\\]\n\n\nPower\nThe third property is that the \\(\\log\\) of a value \\(x\\) raised by a power \\(a\\), is equal to the product of \\(a\\) times the \\(\\log\\) of \\(x\\).\n\\[\n\\log_b(x^a) = a\\log_b(x).\n\\tag{16}\\]\nExpanding the definition in Equation 5:\n\\[\n\\log_b(x) = n \\longrightarrow x = b^n \\longrightarrow x^a = (b^{n})^a = b^{an},\n\\tag{17}\\]\nNow, if we use the definition in the Equation 16, we have that:\n\\[\n\\begin{align}\n\\log_b(x^a)\n&= \\log_b(b^{an}) \\\\\n&= an \\\\\n&= a\\log_b(x).\n\\end{align}\n\\tag{18}\\]"
  },
  {
    "objectID": "posts/0003-log-scale/index.html#real-data-application",
    "href": "posts/0003-log-scale/index.html#real-data-application",
    "title": "An intro to logarithmic scale",
    "section": "Real data application",
    "text": "Real data application\nWe will look at the number of deaths from COVID–19 (Guidotti and Ardia 2020) of Argentina (ARG), Brazil (BRA) and the United States of America (USA), in 2020.\nAs a disclaimer, the goal of this analysis is to demonstrate how the logarithm can be useful, not to gain any insight into how COVID actually behaved in these countries, as that would necessitate more in-depth research.\n\n\n\n\n\n\n\n\nLooking at the figure above we see that number of deaths are bigger in USA, Brazil and lastly Argentina, that is no surprise since it follows the same order of population size.\nSo if we want to compare the behavior of the countries it can be hard, for example, Argentina has less deaths and the curve become “squished”, making it hard to see what really happened.\nSince we have data with different magnitudes, we can apply the logarithm.\n\n\n\n\n\nAfter the application of the \\(\\log_10\\), we can see each country’s behavior more clearly. For example, the number of deaths in the United States began earlier and slowed down faster than in Brazil, whereas Argentina had a steady number of deaths until November, when it began to “stabilize”."
  },
  {
    "objectID": "posts/0004-tidyverse-operators/index.html",
    "href": "posts/0004-tidyverse-operators/index.html",
    "title": "An intro to tidyverse operators",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the tidyverse operators and how we can make functions with them."
  },
  {
    "objectID": "posts/0004-tidyverse-operators/index.html#curly-curly",
    "href": "posts/0004-tidyverse-operators/index.html#curly-curly",
    "title": "An intro to tidyverse operators",
    "section": "{{}} Curly-curly",
    "text": "{{}} Curly-curly\nThe first operator we will learn is the curly-curly, using the command {{}}, the goal of this operator is to allow us to have an argument passed to our function refering to a column inside a dataframe.\nSo, we will create the function penguin_summary, where the variable used to count the penguins, in the example before species, will be generalized By the argument grp_var.\n\npenguin_summary <- function(grp_var){\n  penguins %>% \n  filter(!is.na(sex)) %>% \n  group_by({{grp_var}},sex) %>%\n  summarise(\n    n = n(),\n    mean_body_mass_g = mean(body_mass_g,na.rm = TRUE)\n    ) %>% \n  group_by({{grp_var}}) %>% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\nWe can see that inside the dplyr verbs we write the argument grp_var inside the operator {{}} in the verb group_by.\nLet’s now apply the variable species to see if the result is the same as before.\n\npenguin_summary(grp_var = species)\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n mean_body_mass_g     p\n  <fct>     <fct>  <int>            <dbl> <dbl>\n1 Adelie    female    73            3369. 0.5  \n2 Adelie    male      73            4043. 0.5  \n3 Chinstrap female    34            3527. 0.5  \n4 Chinstrap male      34            3939. 0.5  \n5 Gentoo    female    58            4680. 0.487\n6 Gentoo    male      61            5485. 0.513\n\n\nYes! We got the same result, but there is also another interesting fact, the variable species was passed without quotes, so no need to use functions such as quo, enquote, etc.\nAnd now we can pass other variable to our function, let’s give it a try.\n\npenguin_summary(grp_var = island)\n\n`summarise()` has grouped output by 'island'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   island [3]\n  island    sex        n mean_body_mass_g     p\n  <fct>     <fct>  <int>            <dbl> <dbl>\n1 Biscoe    female    80            4319. 0.491\n2 Biscoe    male      83            5105. 0.509\n3 Dream     female    61            3446. 0.496\n4 Dream     male      62            3987. 0.504\n5 Torgersen female    24            3396. 0.511\n6 Torgersen male      23            4035. 0.489\n\n\nOk, after generalizing the species variable, we will do the same for the body_mass_g creating another argument, num_var.\n\npenguin_summary <- function(grp_var,num_var){\n  penguins %>% \n  filter(!is.na(sex)) %>% \n  group_by({{grp_var}},sex) %>%\n  summarise(\n    n = n(),\n    mean = mean({{num_var}},na.rm = TRUE)\n    ) %>% \n  group_by({{grp_var}}) %>% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\n\npenguin_summary(\n  grp_var = species,\n  num_var = body_mass_g\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n  mean     p\n  <fct>     <fct>  <int> <dbl> <dbl>\n1 Adelie    female    73 3369. 0.5  \n2 Adelie    male      73 4043. 0.5  \n3 Chinstrap female    34 3527. 0.5  \n4 Chinstrap male      34 3939. 0.5  \n5 Gentoo    female    58 4680. 0.487\n6 Gentoo    male      61 5485. 0.513\n\n\nOkay, we kind of succeeded, but we had to give the new variable for the mean a generic name; to make this dynamic, we’ll need the assistance of another operator."
  },
  {
    "objectID": "posts/0004-tidyverse-operators/index.html#walrus",
    "href": "posts/0004-tidyverse-operators/index.html#walrus",
    "title": "An intro to tidyverse operators",
    "section": ":= Walrus",
    "text": ":= Walrus\nThe second operator is the walrus, using the command :=, the goal of this operator is to allow us to create new variables using the argument dynamically in the name of the variable created.\n\npenguin_summary <- function(grp_var,num_var){\n  penguins %>% \n  filter(!is.na(sex)) %>% \n  group_by({{grp_var}},sex) %>%\n  summarise(\n    n = n(),\n    \"mean_{{num_var}}\" := mean({{num_var}},na.rm = TRUE)\n    ) %>% \n  group_by({{grp_var}}) %>% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\n\npenguin_summary(\n  grp_var = species,\n  num_var = body_mass_g\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n mean_body_mass_g     p\n  <fct>     <fct>  <int>            <dbl> <dbl>\n1 Adelie    female    73            3369. 0.5  \n2 Adelie    male      73            4043. 0.5  \n3 Chinstrap female    34            3527. 0.5  \n4 Chinstrap male      34            3939. 0.5  \n5 Gentoo    female    58            4680. 0.487\n6 Gentoo    male      61            5485. 0.513\n\n\nThe walrus operator substitute the = operator, and we can use the argument num_var inside the {{}} operator to generalize our variable name, not only that, but we can also set other characters such as a prefix or suffix.\nNow that we’ve finished our function, what if we want to make it even more generalized? For example, our dataframe and the variable sex are still inside the function, that is easy we just need create two more arguments:\n\npenguin_summary <- function(df = penguins,main_var = sex,grp_var,num_var){\n  df %>% \n  filter(!is.na({{main_var}})) %>% \n  group_by({{grp_var}},{{main_var}}) %>%\n  summarise(\n    n = n(),\n    \"mean_{{num_var}}\" := mean({{num_var}},na.rm = TRUE)\n    ) %>% \n  group_by({{grp_var}}) %>% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\n\npenguin_summary(\n  grp_var = species,\n  num_var = body_mass_g\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n mean_body_mass_g     p\n  <fct>     <fct>  <int>            <dbl> <dbl>\n1 Adelie    female    73            3369. 0.5  \n2 Adelie    male      73            4043. 0.5  \n3 Chinstrap female    34            3527. 0.5  \n4 Chinstrap male      34            3939. 0.5  \n5 Gentoo    female    58            4680. 0.487\n6 Gentoo    male      61            5485. 0.513\n\n\nSo we created an argument called df to be our data.frame, without any operator since it is been called “directly”, and already left the penguins dataset as the default. We did the same with the sex variable with the argument main_var.\nAnd even though we created a function called penguin_summary now we can apply it to another dataframe:\n\npenguin_summary(\n  df = mtcars,\n  main_var = vs,\n  grp_var = cyl,\n  num_var = drat\n  )\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 5 x 5\n# Groups:   cyl [3]\n    cyl    vs     n mean_drat      p\n  <dbl> <dbl> <int>     <dbl>  <dbl>\n1     4     0     1      4.43 0.0909\n2     4     1    10      4.04 0.909 \n3     6     0     3      3.81 0.429 \n4     6     1     4      3.42 0.571 \n5     8     0    14      3.23 1     \n\n\nOk, now we got a function that is completely generalized, with only arguments inside of it, but there is still way to make an even more powerful function, let’s say we want to apply our function to two numerical variables.\n\npenguin_summary(\n  grp_var = species,\n  num_var = c(body_mass_g,bill_depth_mm)\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n `mean_c(body_mass_g, bill_depth_mm)`     p\n  <fct>     <fct>  <int>                                <dbl> <dbl>\n1 Adelie    female    73                                1693. 0.5  \n2 Adelie    male      73                                2031. 0.5  \n3 Chinstrap female    34                                1772. 0.5  \n4 Chinstrap male      34                                1979. 0.5  \n5 Gentoo    female    58                                2347. 0.487\n6 Gentoo    male      61                                2750. 0.513\n\n\nSo it is not what we expected, right? To pass multiple variables into a single argument, we will need the help of an old friend."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html",
    "href": "posts/0005-settlers-of-catan/index.html",
    "title": "An analysis of Settlers of Catan",
    "section": "",
    "text": "In this post of the series An analysis of, I’ll do a data analysis of my favorite board game, Settlers of Catan."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#is-the-dice-fair",
    "href": "posts/0005-settlers-of-catan/index.html#is-the-dice-fair",
    "title": "An analysis of Settlers of Catan",
    "section": "Is the dice fair?",
    "text": "Is the dice fair?\nEvery turn, each player rolls two dice and adds their totals together to determine which location will grant resources to players who have cities or settlements there. This will be the subject of the first analysis.\nWe will compute the probability of each result for the sum of the two dices, taking into account that there are six possible outcomes for each face of each die.\n\n\n\n\n\nIn the picture above, we can see a graph where each axis represents the outcome of a single die, and we can also see all possible outcomes of the sum of those dices.\nSome outcomes are more common than others, for example, the number 7 is the most common outcome because it appears six times.\nAnother intriguing finding is that the results exhibit symmetry; to further explore this, let’s use another visual representation.\n\n\n\n\n\nThe extreme results, 2, and 12, with only one combination for each, are symmetrical and center on the number 7, as was previously mentioned.\nWe will now compare the observed data to the predicted result.\n\n\n\n\n\nWhen comparing the observed values from the real dataset, we can see that the dice results appear to be fairly random because they closely resemble our anticipated result. The number 4 had the biggest discrepancy, with observed values 1.02 percentage points below the predicted probability."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#spending-money-to-make-money.",
    "href": "posts/0005-settlers-of-catan/index.html#spending-money-to-make-money.",
    "title": "An analysis of Settlers of Catan",
    "section": "“Spending money to make money.”",
    "text": "“Spending money to make money.”\nIn the dataset we have the following concepts, as described by the author:\n\nProduction gain: Cards gained from structures;\nTrade gain: Cards gained from peer or bank trade;\nNon-production gain: Cards gained from stealing with the robber, plus cards gained with non-knight development cards, e.g., a road building card is +4 resources;\nTotal gain: Production + Trade + Non-production;\n\nAlso we have the ways to loss cards\n\nTrade loss: Cards lost from peer or bank trades;\nRobber loss: Cards lost directly from robbers, knights, and other players’ monopoly cards;\nTribute loss: Cards lost when player had to discard on a 7 roll;\nTotal loss: Trade + Robber + Tribute.\n\nFirst of all, let’s see how the total gain and loss relate.\n\n\n\n\n\nWe see in the figure above that:\n\nThe loss and gain are positive correlated, that means that players that gained more also lost more cards;\nThere is no player that lost more than gained, as no point is below the identity line;\nThe winners gained a lot more cards than player that lost.\n\nTo take a better look at the third point, let’s plot the gain/loss cards ratio.\n\n\n\n\n\nLooking at the gain/loss ratio density plot, we see that:\n\nThe density is positive skewed, for winners or losers;\nThe losers have a more concentrated density, using the peak value of the density as a metric, winners gain 2.31 cards to every card lost, whereas losers gain 2.15 cards."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#the-last-will-be-first-and-the-first-last.",
    "href": "posts/0005-settlers-of-catan/index.html#the-last-will-be-first-and-the-first-last.",
    "title": "An analysis of Settlers of Catan",
    "section": "“The last will be first, and the first last.”",
    "text": "“The last will be first, and the first last.”\nThe playing order is important in this game because it gives you the ability to choose the locations of your settlement, but being the last one is not the worst, since you become the first to choose your second settlement in the map.\nTo make the analysis of the importance of the locations choosen, we will use the dice sum results and define as weights, e.g., if the number is 12 the weight will be 1, since the the probability of this results is 1/36, as we saw in the dice analysis.\nThen we will sum the weights of the initial two settlements of each player (\\(wt\\)), and we will do the different of the total weight of the winner minus the maximum weight of that game, given by:\n\\[\nwt_{\\mathrm{winner}} - \\max(wt).\n\\]\n\n\n\n\n \n  \n    Difference \n    Frequency \n  \n \n\n  \n    -5 \n    1 \n  \n  \n    -4 \n    5 \n  \n  \n    -3 \n    8 \n  \n  \n    -2 \n    8 \n  \n  \n    -1 \n    4 \n  \n  \n    0 \n    24 \n  \n\n\n\n\n\nA difference of zero means that the winner was the player with the best location, at least probability-wise. So in almost half of the games the winner had the best location."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#if-your-ship-doesnt-come-in-swim-out-and-meet-it.",
    "href": "posts/0005-settlers-of-catan/index.html#if-your-ship-doesnt-come-in-swim-out-and-meet-it.",
    "title": "An analysis of Settlers of Catan",
    "section": "“If your ship doesn’t come in, swim out and meet it.”",
    "text": "“If your ship doesn’t come in, swim out and meet it.”\n\n\n\nThe port is a unique location. Normally, you can exchange 4 identical resources for another resource at the bank, but with a port you can lower this trade rate, but you lose one resource location in the process.\n\n\n\n\n \n  \n    Location \n    Locations \n    Percentage \n  \n \n\n  \n    Port \n    35 \n    2.92 \n  \n  \n    Resource \n    1,165 \n    97.08 \n  \n  \n    Total \n    1,200 \n    100.00 \n  \n\n\n\n\n\nSo of all the 1,200 initial settlements just 35 of them were ports, so it is not a popular strategy.\n\n\n\n\n \n  \n    Player \n    Games \n    Percentage \n  \n \n\n  \n    Loser \n    27 \n    81.82 \n  \n  \n    Winner \n    6 \n    18.18 \n  \n  \n    Total \n    33 \n    100.00 \n  \n\n\n\n\n\nThose 35 port initial locations were in 33 games, where in only 6 of them the player was the winner, let’s explore why.\n\n\n\n\n\nLooking at the trading behavior we see that in average player with a initial port gained more, so let’s take a look at the trade ratio (gain/loss).\n\n\n\n\n\nPlayers that had a initial port had a higher ratio, that can sound counterintuitive, since we expect they had a better trade-off with the ports advantage, a possibility is the lower quantity of resources making the players make actually worst trade for other resources, since they give up one location when choosing this strategy, but how impactful is that?\n\n\n\n\n\nLastly, when can see how impactful was the production of cards from structures based on the initial port. The result was a lot lower for players with a initial port, with almost 16 cards of difference from players that choosed 3 resources locations."
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html",
    "href": "posts/0006-normal-distribution/index.html",
    "title": "An intro to normal distribution",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the normal distribution."
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#standard-normal",
    "href": "posts/0006-normal-distribution/index.html#standard-normal",
    "title": "An intro to normal distribution",
    "section": "Standard normal",
    "text": "Standard normal\nThe simplest case of a normal distribution is a \\(\\mathcal{N}(0,1)\\), also called a standard normal distribution or Z-distribution:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\mathcal{e}^{-\\frac{x^2}{2}}.\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#simmetry",
    "href": "posts/0006-normal-distribution/index.html#simmetry",
    "title": "An intro to normal distribution",
    "section": "Simmetry",
    "text": "Simmetry\nThe normal distribution has a balanced and mirror-like shape around its center, which is characterized by its symmetry and central tendency. Because of its symmetry, values on either side of the mean have equal probabilities, and the alignment of the mean, median, and mode at the center.\nHere an example of a standard normal distribution:"
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#bell-shape-curve",
    "href": "posts/0006-normal-distribution/index.html#bell-shape-curve",
    "title": "An intro to normal distribution",
    "section": "Bell-shape curve",
    "text": "Bell-shape curve\nThe shape of the normal distribution is also characterized by gradually decreasing probabilities as the values move away equally from the mean in both directions.\nEven tough it has zero skewness the variance can imply in a kurtosis change, here a few example:"
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#chebyshevs-inequality",
    "href": "posts/0006-normal-distribution/index.html#chebyshevs-inequality",
    "title": "An intro to normal distribution",
    "section": "Chebyshev’s inequality",
    "text": "Chebyshev’s inequality\nChebyshev’s inequality is a mathematical inequality that can be applied to any probability distribution with defined mean and variance. It gives us an bound on the likelihood that a random variable deviates from its mean by a certain amount.\n\\[\nP(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}, \\quad k >0; \\quad k \\in \\mathbb{R},\n\\tag{3}\\]\nwhere:\n\n\\(X\\) is a random variable with variance \\(\\sigma^2\\) and expected value \\(\\mu\\);\n\\(\\sigma\\) is a finite non-zero standard deviation;\n\\(\\mu\\) is a finite expected value;\n\\(k\\) is a given real number greater then zero.\n\nWhen applied to the normal distribution we have that:\n\n\n\n\n\nApproximately 68% of values in a normal distribution are within one standard deviation (\\(\\sigma\\)) of the mean, 95% are within two standard deviations, and 99.7% are within three standard deviations."
  },
  {
    "objectID": "posts/0007-king-james-bible/index.html",
    "href": "posts/0007-king-james-bible/index.html",
    "title": "An analysis of The King James Bible",
    "section": "",
    "text": "In this post of the series An analysis of, I’ll do a text analysis of The King James Bible.\n\n\n\n\n\n\n\nContext\nThe King James Bible, first published in 1611, is a crucial English translation of the Bible.\n“Hey, let’s have a fancy new translation!” exclaimed King James I of England. So he enlisted the help of a group of outstanding scholars. They were inspired by old English versions as well as the original Hebrew and Greek texts.\n\nFor the analysis we will use the text from the King James Bible.\n\nDisclaimer: the goal here is just to show and apply some techniques to work with text data.\n\n\n\nHow is the bible built?\nThis Bible is divided into two parts:\n\nThe Old Testament, which contains all of the religious material from before Jesus appeared;\nThe New Testament, which contains everything about Jesus and his followers.\n\nIn addition from the testaments, the bible is also divided in books and verses.\n\nAs we can see, the old testament has books with twice as many verses as the new testament. But, when we look by book, are the numbers of verses consistent?\n\n\nTestament\nBooks\nVerses\nVerses/Books\n\n\n\n\nOld\n39\n23,145\n593.4615\n\n\nNew\n27\n7,957\n294.7037\n\n\nTotal\n66\n31,102\n471.2424\n\n\n\n\n\n\n\n\nClearly not, as the number of verses varies greatly, with an outlier in the old testament, The Book of Psalms, having astounding 2,461 verses.\n\nBook of Psalms\n\n\n\n\nIt is a collection of religious songs, prayers, and poems attributed to King David of Israel as well as other authors such as Asaph, Korah’s sons, Solomon, and Moses.\nPraise, thanksgiving, trust in God, deliverance, longing for God’s presence, justice, and worship are just a few of the themes covered in the psalms. They are a rich source of spiritual reflection, expressing a wide range of human emotions and providing believers with comfort, guidance, and encouragement. The psalms are well-known for their poetic form, vivid imagery, and long-lasting spiritual and literary value, and they are widely used in Jewish and Christian worship.\n\n\n\nWith the exception of the last book, The Revelation of St. John the Divine, the new testament begins with books with a greater number of verses but decreases as the bible progresses.\n\nThe Revelation of St. John the Divine\n\n\n\n\nThe Book of Revelation, attributed to the apostle John, contains apocalyptic visions received by John while he was exiled on the island of Patmos.\nIt contains prophetic messages and symbolic language depicting the end times, final judgment, and God’s victory over evil. The book deals with topics such as faithfulness, persecution, divine sovereignty, and the establishment of a new heaven and earth. It contains messages to seven churches, heavenly worship, and predictions of future events, and it has sparked ongoing interpretation and fascination among Christians.\n\n\n\n\n\nWord-o-Rama\nNow let’s analyze the word frequency, the bible possess 789,649 words in total, where 12,784 are unique words. Here is the top 10 most frequent words:\n\n\n    Word Frequency\n1    the    63,925\n2    and    51,696\n3     of    34,617\n4     to    13,562\n5   that    12,912\n6     in    12,667\n7     he    10,420\n8  shall     9,838\n9   unto     8,997\n10   for     8,970\n\n\nThe result does not show much; to improve the outcome, we can eliminate this type of word; to do so, we have a dataset of stopwords.\n\nStopword\n\n\n\n\nA word that is commonly used in a language that is thought to have little or no meaningful information and is frequently removed from text during natural language processing (NLP) tasks such as text analysis, information retrieval, or text mining. Articles (e.g., “a,” “an,” “the”), pronouns (e.g., “I,” “you,” “he”), prepositions (e.g., “in,” “on,” “at”), and conjunctions (e.g., “and,” “or,” “but”) are examples of stopwords.\n\n\n\nAfter removing this stop words we have 273,394 words total, of which 12,332, so just 452 stopword were removed, but that were used more than 516,255 times.\nNow, last see the top 10 most frequent words:\n\n\n     Word Frequency\n1    lord     7,830\n2    thou     5,474\n3     thy     4,600\n4     god     4,445\n5      ye     3,982\n6    thee     3,826\n7  israel     2,565\n8     son     2,370\n9    hath     2,264\n10   king     2,256\n\n\nWords referring to God appear, as expected, but how much of the old testament influences this?\n\n\n\n\n\nThe graph above shows the relative frequency of the most common words by testament; as a result, we can see that some words are shared, but we can also see which words diverge the most as we plot the identity line.\nFor example, “Jesus” and “Christ” appear only in the New Testament, which is no surprise, given the criteria for such division.\nOn the other hand, the word “lord” appears nearly three times more in the old testament, owing to the fact that God is a more prominent figure there.\n\n\nSentimental Scriptures\nThe text will then be classified as positive, negative, or neutral using sentiment analysis. This is accomplished by employing a third-party dictionary with a score assigned to each word.\n\n\n\n\n\nWe see that most books of the old testament have a negative sentiment, with a huge exception been the The Song of Solomon (book #22).\n\nThe Song of Solomon\n\n\n\n\nIt is a poetic dialogue between a bride and her beloved in which they express their deep affection and longing for one another through metaphorical language. The book celebrates the beauty of romantic love and is frequently interpreted as an allegory for God’s love relationship with His people. It contains vivid imagery and has sparked controversy due to its explicit content. Overall, it delves into themes of love, desire, and the beauty of human relationships, challenging readers to consider the nature of love and intimacy.\n\n\n\nThe first books of the new testament start negative, but became highly positive. But we can see that the penultimate book (The General Epistle of Jude) is more negative than the other final books.\n\nThe General Epistle of Jude\n\n\n\n\nThe General Epistle of Jude is a brief New Testament letter attributed to Jude, the brother of James and a disciple of Jesus Christ. It addresses the presence of false teachers and emphasizes the importance of discernment and faith. Jude warns believers about the consequences of false teachings and encourages them to fight for the true gospel. The letter encourages believers to strengthen their faith, to be compassionate toward those who doubt, and to praise God for His power and ability to keep them from falling. Overall, it is a call to stand firm in the face of false teachings and to rely on God’s grace and truth."
  },
  {
    "objectID": "posts/0008-benford-law/index.html",
    "href": "posts/0008-benford-law/index.html",
    "title": "An intro to Benford’s Law",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the Benford’s Law."
  },
  {
    "objectID": "posts/0008-benford-law/index.html#exponential-distribution",
    "href": "posts/0008-benford-law/index.html#exponential-distribution",
    "title": "An intro to Benford’s Law",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nFirst let’s simulate a set of 10,000 random numbers from a exponential distribution with a rate of 0.25.\n\n\n\n\n\n\n\n\nNext, we extract the first digit of each number and calculate the frequency of each one.\n\n\n\n\n\nSmaller digits are more common, as shown in the graph above, and as the digit grows larger, the frequency decreases. Let us now compare the actual result to the expected result.\n\n\n\n\n\nAs we can see, Benford’s Law and our data are very similar, but is this always the case?"
  },
  {
    "objectID": "posts/0008-benford-law/index.html#uniform-distribution",
    "href": "posts/0008-benford-law/index.html#uniform-distribution",
    "title": "An intro to Benford’s Law",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nLet’s run a simulation of 10,000 random numbers drawn from a uniform distribution with a range of 1 to 100.\n\n\n\nThe simulated data is shown below:\n\n\n\n\n\nLet us now compute the frequency of the first digits.\n\n\n\n\n\nWe can see now that the law differs from the simulated data, but why? Because we are sampling from a set of numbers where the first digit pool is uniform."
  },
  {
    "objectID": "posts/0009-monty-hall/index.html",
    "href": "posts/0009-monty-hall/index.html",
    "title": "An intro to Monty Hall problem",
    "section": "",
    "text": "In this post of the series Intro to, I’ll give an introduction to the Monty Hall problem.\n\nIntroduction\nBecause of its counterintuitive nature, it became a well-known probability scenario based on the game show “Let’s Make a Deal,” which Monty Hall hosted.\n\nIt functioned as follows:\n\nA player is presented with three doors, one of which hides a prize.\nInitially, the player selects one of the doors without knowing what lies behind it.\nAfter the player has made their selection, the host, who knows what is behind each door, opens one of the remaining two doors, always revealing a door that does not contain the prize.\n\nThe player is confronted with a quandary, they have the option of remaining with their original selection or switching to the other unopened door.\n\n\nWhy not 50/50?\nAt first glance, the probability of selecting the prize is 50% given the two remaining doors after the host opens one, so what the heck, right?\nThat is not the case; instead, let us map each scenario, where we have three doors (A, B, and C).\n\n\n\n\n\n\n\n\n\n\nChoosen door\nPrize door\nHost opens\nSwitch the door\nStay with door\n\n\n\n\nA\nA\nB/C\nLose\nWin\n\n\nB\nA\nC\nWin\nLose\n\n\nC\nA\nB\nWin\nLose\n\n\nA\nB\nC\nWin\nLose\n\n\nB\nB\nA/C\nLose\nWin\n\n\nC\nB\nA\nWin\nLose\n\n\nA\nC\nB\nWin\nLose\n\n\nB\nC\nA\nWin\nLose\n\n\nC\nC\nA/B\nLose\nWin\n\n\n\nSwitching the door reveals 6 winning outcomes out of the 9 possibilities, whereas sticking with the original choice offers only 3 winning scenarios. This means that switching has a higher chance of success, with a 2/3 chance of success.\n\n\nStill skeptical?\nFor those who are still skeptical, here we simulate 200,000 games in R, where I chose to stay with the first door in the first half and switch the door in the second half.\n\ndoors <- c(\"A\",\"B\",\"C\")\n\nmonty_hall <- function(stay = TRUE){\n  \n  prize_door <- sample(x = doors,size = 1)\n  \n  initial_door <- sample(x = doors,size = 1)\n  \n  if(prize_door == initial_door){\n    switch_door <- sample(doors[doors != prize_door],1)\n  }else{\n    switch_door <- prize_door\n  }\n  \n  if(stay){\n    output <- initial_door == prize_door\n  }else{\n    output <- switch_door == prize_door\n  }\n  \n  return(output)  \n}\n\n#Probability of winning, by keeping the initial door\nset.seed(1234);mean(replicate(100000,monty_hall(stay = TRUE)))\n\n[1] 0.33282\n\n#Probability of winning, by switching the initial door\nset.seed(1234);mean(replicate(100000,monty_hall(stay = FALSE)))\n\n[1] 0.66718\n\n\nAs we can see, the simulation’s success probability is nearly equal to the previously calculated.\n\n\nConsiderations\nProbability and conditional reasoning are central to the Monty Hall problem. Choosing door A initially gives it a 1/3 chance of containing the prize, while the other unopened door, B, now has a 2/3 chance.\nDespite the fact that it seems counterintuitive, switching doors increases your chances of winning the prize. The puzzle is an enthralling example of how our intuition can lead us astray, emphasizing the importance of understanding probability in decision-making."
  }
]