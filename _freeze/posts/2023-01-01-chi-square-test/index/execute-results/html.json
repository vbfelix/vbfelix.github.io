{
  "hash": "cafb02ce5c57677f7a62e742ef0b4f0d",
  "result": {
    "markdown": "---\ntitle: \"An intro to chi-square test\"\nauthor: \"Vinícius Félix\"\ndate: \"2023-01-01\"\ncategories: [Theory,Intro to,Chi-square, Inference]\nbibliography: references.bib\nimage: \"intro-to-chi-square-test.png\"\n---\n\n\nIn this post of the series **Intro to**, I'll give an introduction to the chi-square test, one of the most well-known statistical tests.\n\n# History\n\nTo investigate a probability criterion on any theory of an observed system of errors and to apply it in order to establish a goodness of fit (GoF) measure, Karl Pearson published a paper [@pearson1900], the reason why the test is also known as Pearson's chi-square test[.]{.smallcaps}\n\nWe can test how likely a variable is to come from a specified distribution in this application because we have a metric to check whether or not our observed values are close to or not to the expected values, in this case we can define hypotheses as:\n\n-   $H_0:$ The variable follows a specific distribution;\n\n-   $H_1:$ The variable does not follows a specific distribution.\n\nDespite Pearson's primary goal, the chi-square test is most commonly used to test the association or independence of two categorical variables. To do so, we look at how common a particular categorical feature is among two or more groups. So we can establish the hypotheses as:\n\n-   $H_0:$ The two variables are independent;\n\n-   $H_1:$ The two variables are not independent.\n\nMeaning that the test does not differentiate one variable from another in any causal way.\n\n# Math\n\nTo begin the math, we will dissect the test statistic, given by the @eq-chi-square-stat:\n\n\n$$\n\\begin{equation}\nX^2 = \\sum_{i = 1}^{k} \\frac{(O_i - E_i)^2}{E_i},\n\\end{equation}\n$$ {#eq-chi-square-stat}\n\n\nwhere:\n\n-   $X^2$ is the test statistic;\n\n-   $O_i$ is the observed value of the class $i$;\n\n-   $E_i$ is the expected value of the class $i$;\n\n-   $k$ is the total number of classes, i.e., number of combinations of the two variables .\n\nThe pre-requisites behind the test are:\n\n-   Two categorical variables;\n\n-   Two or more levels for each variable;\n\n-   The observations are independent, i.e., no paired observations or longitudinal data;\n\n-   A random sample of size $n$ where each $O_i$ falls in one of the $k$ mutually exclusives classes;\n\n-   A null hypothesis that with a probability of $p_i$ that each $O_i$ belongs to one of the $k$'s classes.\n\nSo considering the test statistic, Pearson proposed that under a true null hypothesis and an asymptotic $n$, the test statistic will follow a $X_{q}^2$ distribution, where $q$ is the number of degrees of freedom. But how?\n\nThe reasoning is that we can describe the observed values following a Binomial distribution, $O_i \\sim Binomial(n,p_i)$, since this distribution describes the phenomenon of the number of sucesses given a total number of trials ($n$), where the true success rate is ($p_i$), giving this distribution we have that:\n\n\n$$\n\\begin{equation}\nE(O_i) = E_i =  np_i,\n\\end{equation}\n$$ {#eq-binomial-mean}\n\n\nand\n\n\n$$\n\\begin{equation}\nVar(O_i) = np_i(1-p_i).\n\\end{equation}\n$$ {#eq-binomial-var}\n\n\nGiven a sample size that is sufficient large, we can use a result of the central limit theorem [@pearson1900; @cam1986] that gives us that:\n\n\n$$\n\\begin{equation}\nZ = \\frac{Y-\\mu}{\\sqrt{\\sigma^2}} \\longrightarrow \\mathcal{N}(0,1),\n\\end{equation}\n$$ {#eq-clt}\n\n\nwhere:\n\n-   $Y$ is the sum of random independent and identically distributed variables, with the same mean and variance;\n\n-   $\\mu$ is the mean;\n\n-   $\\sigma^2$ is the variance ;\n\n-   $\\mathcal{N}$ is a normal distribution.\n\nSo applying the approximation of the normal to a binomial distribution:\n\n\n$$\nBinomial(n,p_i) \\approx N(np_i,np_i(1-p_i)).\n$$ {#eq-normal-to-binomial}\n\n\nNow, if we square a random variable $Z_1$ that follows a standard normal distribution $\\mathcal{N}(0,1)$, we have that:\n\n\n$$\nZ_1^2 = \\chi_1^2,\n$$ {#eq-z1-to-chi1}\n\n\nand if we sum two squared variables, such as $Z_1$ and $Z_2$:\n\n\n$$\nZ_1^2 + Z_2^2 = \\chi_2^2.\n$$ {#eq-z2-to-chi2}\n\n\nThen,\n\n\n$$\n\\sum_{i = 1}^{q} Z_i^2  = \\chi_q^2,\n$$ {#eq-normal-chi-square}\n\n\nNow using @eq-clt and @eq-normal-to-binomial we can write that:\n\n\n$$\n\\begin{align}\nZ \n&= \\frac{Y-\\mu}{\\sqrt{\\sigma^2}} \\\\\n&=  \\frac{O_i - np_i}{\\sqrt{np_i(1-p_i)}}. \\\\\n\\end{align}\n$$ {#eq-z}\n\n\nSo having a variable $X_1 = Z^2$, we apply the results from @eq-normal-chi-square and @eq-z :\n\n\n$$\n\\begin{align}\nZ^2 \n&= \\left[ \\frac{Y-\\mu}{\\sqrt{\\sigma^2}} \\right]^2 \\\\\n&= \\left[ \\frac{O_1 - np_1}{\\sqrt{np_1(1-p_1)}}  \\right]^2 \\\\\n&= \\frac{ (O_1 - np_1) ^2}{(\\sqrt{np_1(1-p_1)})^2}   \\\\\n&= \\frac{ (O_1 - np_1) ^2}{np_1(1-p_1)}   \\\\\n&= \\frac{ (O_1 - np_1) ^2}{np_1(1-p_1)}  \\times[(1-p_1)+p_1] \\\\\n&= \\frac{ (O_1 - np_1) ^2(1-p_1)}{np_1(1-p_1)} + \\frac{ (O_1 - np_1)^2(p_1)}{np_1(1-p_1)}    \\\\\n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ (O_1 - np_1)^2}{n(1-p_1)}.    \\\\\n\\end{align}\n$$ {#eq-x1}\n\n\nConsidering now a second variable, $X_2$, where:\n\n\n$$\nX_1 = n - X_2 \\longrightarrow X_2 = n-X_1,\n$$ {#eq-x2}\n\n\nand,\n\n\n$$\np_1 = 1- p_2 \\longrightarrow p_2 = 1 - p_1.\n$$ {#eq-p2}\n\n\nWe apply the @eq-x2 and @eq-p2 to @eq-x1 and continue our desmonstration:\n\n\n$$\n\\begin{align}\nZ^2 \n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ (O_1 - np_1)^2}{n(1-p_1)}    \\\\\n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ [(n - O_2)- n(1-p_2)]^2}{n(p_2)}    \\\\\n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ [n - O_2- n+np_2]^2}{np_2}    \\\\\n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ [-O_2+np_2]^2}{np_2}    \\\\\n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ [-(O_2-np_2)]^2}{np_2}    \\\\\n&= \\frac{ (O_1 - np_1)^2}{np_1} + \\frac{ (O_2-np_2)^2}{np_2}    \\\\\n&= \\frac{ (O_1 - E_1)^2}{E_1} + \\frac{ (O_2-E_2)^2}{E_2}    \\\\\n&= \\sum_{i=1}^{2}\\frac{ (O_i - E_i)^2}{E_i}.    \\\\\n\\end{align}\n$$ {#eq-z2}\n\n\nSo we show that the sum of chi-squared variables results in the same formula as the test statistic, as shown in @eq-chi-square-stat.\n\nLastly, since we can also rewrite our statistic in @eq-chi-square-stat, using @eq-binomial-mean:\n\n\n$$\n\\begin{align} &= \\sum_{i = 1}^{k} \\left[ \\frac{(O_i - E_i)^2}{E_i} \\right] \\\\ & = \\sum_{i = 1}^{k} \\left[  \\frac{O_i^2}{E_i} - \\frac{E_i^2}{E_i} \\right] \\\\ & = \\sum_{i = 1}^{k} \\left[  \\frac{O_i^2}{E_i} -  E_i \\right] \\\\ & = \\sum_{i = 1}^{k} \\left[  \\frac{O_i^2}{E_i} \\right] - \\sum_{i = 1}^{k}\\left[ E_i \\right] \\\\ & = \\sum_{i = 1}^{k} \\left[  \\frac{O_i^2}{E_i} \\right] - \\sum_{i = 1}^{k}\\left[ n\\times p_i \\right] \\\\ & = \\sum_{i = 1}^{k} \\left[  \\frac{O_i^2}{E_i} \\right] - n\\sum_{i = 1}^{k}\\left[ p_i \\right] \\\\ & = \\sum_{i = 1}^{k} \\left[  \\frac{O_i^2}{E_i} \\right] - n. \\\\ \\end{align}\n$$ {#eq-chi-square-stat-alt}\n\n\n# Example\n\n## Introduction\n\nAfter all of these equations, we'll perform a real-world example.\n\nLet's say we have 200 animals in our random sample, and we want to see if race has anything to do with frame size. In order to see our observed values in each class, i.e., race x frame, we will first look at a contingency table with the absolute frequency of animals:\n\n| Frame          | Race 1 | Race 2  | Race 3 | Frame Total |\n|----------------|--------|---------|--------|-------------|\n| Small          | 10     | 20      | 10     | **40**      |\n| Medium         | 20     | 30      | 20     | **70**      |\n| Large          | 30     | 50      | 10     | **90**      |\n| **Race Total** | **60** | **100** | **40** | **200**     |\n\n## Test statistic\n\nTo compute our statistic, as given by the equation @eq-chi-square-stat, we have to:\n\n1.  Compute the expected value for each class (cell in terms of a contingency table);\n\n2.  Compute the component $\\frac{(O_i -E_i)^2}{E_i}$ fo each class;\n\n3.  Compute the $X^2$ statistic by summing all values of step 2.\n\nTo begin, we will perform the calculus for a single cell to demonstrate each step, and we will select the Race 1 x Small frame class. In this case, our observed value is 10, so we do the following to calculate the expected value:\n\n\n$$\nE_1 = (40 \\times 60)/200 = 12.\n$$ {#eq-example-part-01}\n\n\nWe multiplied our marginal results and divided them by our sample size because one of our assumptions is that the variables are independent. We can now compute the component for the first cell using our expected value.\n\n\n$$\n\\begin{align}\n& =  \\frac{(O_1 - E_1)^2}{E_1}  \\\\\n& =  \\frac{(10 - 12)^2}{12}  \\\\\n& =  \\frac{(-2)^2}{12}  \\\\\n& =  \\frac{4}{12} \\\\\n& =  1/3.\\\\\n\\end{align}\n$$ {#eq-example-part-02}\n\n\nNow we apply the calculus to each cell, computing the expected value for every class:\n\n| Frame  | Race 1 | Race 2 | Race 3 |\n|--------|--------|--------|--------|\n| Small  | 12     | 20     | 8      |\n| Medium | 21     | 35     | 14     |\n| Large  | 27     | 45     | 18     |\n\nAnd then we can also compute $\\frac{(O_i -E_i)^2}{E_i}$ for each one:\n\n| Frame  | Race 1 | Race 2 | Race 3 |\n|--------|--------|--------|--------|\n| Small  | 0.33   | 0      | 0.50   |\n| Medium | 0.05   | 0.71   | 2.57   |\n|        | 0.33   | 0.56   | 3.56   |\n\nLastly, we sum all the values to obtain the statistic $X^2$, which is equal to 8.61.\n\n## p-value\n\nNow, if we want to obtain the p value we have to look at the chi-sqaure distribution, so first we need to obtain the number of degrees of freedom $(q)$, in this cases that is given by:\n\n\n$$\nq = (n_r-1)\\times(n_c-1),\n$$ {#eq-dof-formula}\n\n\nwhere\n\n-   $n_r$ is the number of rows in the contingency table, i.e., the number of levels of the respective categorical variable;\n\n-   $n_c$ is the number of columns in the contingency table, i.e., the number of levels of the respective categorical variable.\n\nThen, we have in our example that\n\n\n$$\n\\begin{align}\nq & =  (n_r-1)\\times(n_c-1)  \\\\\n& =  (3-1) \\times (3-1) \\\\\n& =  (2) \\times (2) \\\\\n& =  4.\\\\\n\\end{align}\n$$ {#eq-dof-example}\n\n\nWith an established $q$, we now can compute the p value given by:\n\n\n$$\nP(\\chi_4^2 > X^2|H_0), \n$$ {#eq-p-value}\n\n\nor the probability that a value is larger than $X^2$ given a $\\chi_4^2$ distribution and a true null hypothesis, as we can see in the figure below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWith a p value of 0.0716, we have that the p value is greater than 0.05, so we do not reject the null hypothesis, i.e., we do not have sample evidence to reject the null hypothesis that race and size frame are independent..\n\n# In practice with R\n\nIn real life we are not going to do all of this calculations step by step, let's how we can use R to help us.\n\n## Contingency table\n\nIf you have your contingency table ready, an easy and quick way to apply the test is to create a matrix with the observed values of each class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#matrix with the count \ndata <- matrix(data = c(10,20,30,20,30,50,10,20,10),ncol = 3)\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]   10   20   10\n[2,]   20   30   20\n[3,]   30   50   10\n```\n:::\n\n```{.r .cell-code}\nchisq.test(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  data\nX-squared = 8.6111, df = 4, p-value = 0.07159\n```\n:::\n:::\n\n\nWe can see that the function already show us the statistic, number of degrees of freedom and the p value.\n\n## Raw data\n\nWe usually work with raw data as data analysts, where each row represents one observation. In this case, we can transform our data to perform the same function as in the previous example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(dplyr)\n\n#Raw data simulation\ndata <-\n  expand_grid(\n    race  = c(1:3), \n    frame = factor(c(\"S\",\"M\",\"L\"))\n  ) %>% \n  arrange(race,frame) %>% \n  mutate(n  = c(10,20,30,20,30,50,10,20,10)) %>% \n  uncount(n)\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 x 2\n    race frame\n   <int> <fct>\n 1     1 L    \n 2     1 L    \n 3     1 L    \n 4     1 L    \n 5     1 L    \n 6     1 L    \n 7     1 L    \n 8     1 L    \n 9     1 L    \n10     1 L    \n# ... with 190 more rows\n```\n:::\n\n```{.r .cell-code}\ndata %>% \n  #Number of observed values for each class\n  count(race,frame) %>% \n  #Pivot table to make a contingency table\n  pivot_wider(names_from = race,values_from = n) %>% \n  #Removal of the variable as column\n  select(-1) %>% \n  #Chi-square test\n  chisq.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  .\nX-squared = 8.6111, df = 4, p-value = 0.07159\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}