---
title: "Intro to: R²"
author: "Vinícius Félix"
date: "2024-04-16"
categories: [Intro to]
image: "banner.png"
---

```{r setup,echo=FALSE,message=FALSE,warning=FALSE}
suppressWarnings(library(ggplot2))
suppressWarnings(library(relper))
suppressWarnings(library(dplyr))
suppressWarnings(library(tidyr))
suppressWarnings(library(janitor))
suppressWarnings(library(knitr))
suppressWarnings(library(kableExtra))
suppressWarnings(library(forcats))

set.seed(123);data <- rpearson(n = 10,pearson = .75,mean = 5,sd = 10)

data_model <- 
  lm(formula = y~x,data = data) %>% 
  broom::augment()

y_avg <- mean(data$y)

```

In this post, we explore the infamous metric R².

# Context

The R² or the coefficient of determination is a metric commonly used to measure the goodness of fit of a model, it is given by:

$$
R^2 = 1- \frac{SS_{\mathrm{res}}}{SS_{\mathrm{tot}}},
$$ {#eq-r2}

where:

-   $SS_{\mathrm{res}}$ is the sum of squares of residuals;

-   $SS_{\mathrm{tot}}$ is the total sum of squares.

The sum of squares of residuals is given by: $$
SS_{\mathrm{res}} = \sum_\limits{i=1}^{n}(y_i - \hat{y}_i)^2,
$$ {#eq-ssres}

where:

-   $y_i$ is the response variable;

-   $\hat{y}_i$ is the fitted value for $y_i$.

The graphic below shows the difference between the original values and the model ($y_i - \hat{y}_i$).

```{r, echo = FALSE}

base_plot <-
  
  data_model %>% 
  ggplot(aes(x,y))+
  plt_theme_xy()+
  plt_regression_line(color = "royalblue4",linetype = "solid")+
  geom_hline(aes(yintercept = y_avg,col = "Average"))+
  geom_point(size = 2)+
  plt_flip_y_title+
  scale_x_continuous(breaks = 1:10, limits = c(1,10))+
  scale_y_continuous(breaks = 1:10, limits = c(1,7))+
  scale_color_manual(values = "firebrick3")+
  labs(col = "")+
  plt_water_mark(vfx_watermark)

base_plot+
  geom_segment(aes(x = x,xend = x,y = .fitted,yend = y), linetype = "dashed")


```

The total sum of squares is given by: $$
SS_{\mathrm{tot}} = \sum_\limits{i=1}^{n}(y_i - \bar{y})^2,
$$ {#eq-sstot}

where:

-   $y_i$ is the response variable;

-   $\bar{y}$ is the average value of $y$.

The graphic below shows the difference between the response variable's original values and its mean ($y_i - \bar{y}$).

```{r, echo = FALSE}

base_plot+
    geom_segment(aes(x = x,xend = x,y = y_avg,yend = y), linetype = "dashed")


  
```

To put it simply, the coefficient measures the relative difference between the sum of squares of your model compared to a simplistic model (the average), where its values is considered good if it equals 1, meaning that the $SS_{\mathrm{res}}$ is close to 0 and is considered bad as it approaches 0, since the squared sum of the residuals would be close as using the average as a model.

## 

## When R² = (r)²?

A well-known fact is that for simple linear regression, we have a direct relationship between the coefficient of determination and the Pearson linear correlation coefficient. To show the relationship between the $R^2$ and $r$ , first we have that

$$
SS_{\mathrm{tot}} = SS_{\mathrm{res}} + SS_{\mathrm{reg}}.  
$$ {#eq-sstot-ssres-ssreg}

where $SS_{\mathrm{reg}}$ is the sum of squares due to regression, also known as the explained sum of squares, giving by:

$$
SS_{\mathrm{reg}} = \sum_\limits{i=1}^{n}(\hat{y}_i - \bar{y})^2.
$$ {#eq-ssreg}

Or graphically,

```{r, echo = FALSE}
base_plot + 
    geom_segment(aes(x = x,xend = x,y = .fitted,yend = y_avg), linetype = "dashed")


```

Then, applying the @eq-sstot-ssres-ssreg to the @eq-r2 :

$$
\begin{align}
R^2
&=  1- \frac{SS_{\mathrm{res}}}{SS_{\mathrm{tot}}}\\
&=  \frac{SS_{\mathrm{tot}}}{SS_{\mathrm{tot}}}- \frac{SS_{\mathrm{res}}}
{SS_{\mathrm{tot}}}\\
&=  \frac{SS_{\mathrm{tot}} - SS_{\mathrm{res}}}{SS_{\mathrm{tot}}}\\
&=  \frac{SS_{\mathrm{reg}} + \cancel{{SS_{\mathrm{res} } - SS_{\mathrm{res}}}}}{SS_{\mathrm{tot}}}\\
&=  \frac{SS_{\mathrm{reg}}}{SS_{\mathrm{tot}}}.\\
\end{align}
$$ {#eq-r2-to-ssreg}

So with the @eq-ssreg applied to the @eq-r2-to-ssreg, we have that

$$ \begin{align} R^2 &=  \frac{SS_{\mathrm{reg}}}{SS_{\mathrm{tot}}}\\ &=  \frac{\sum_\limits{i=1}^{n}(\hat{y}_i - \bar{y})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}. \end{align} $$ {#eq-r2-as-ssreg}

For a simple linear regression, we can compute the fitted value as

$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i,
$$ {#eq-pred-value}

where:

-   $\hat{\beta}_0$ is the estimated value of the intercept;

-   $\hat{\beta}_1$ is the estimated value of the slope coefficient;

-   $x_i$ is the explanatory variable.

Applying the @eq-pred-value in the @eq-r2-as-ssreg we have that

$$
\begin{align}
R^2
&=  \frac{\sum_\limits{i=1}^{n}(\hat{y}_i - \bar{y})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\\
&=  \frac{\sum_\limits{i=1}^{n}(\hat{\beta}_0 + \hat{\beta}_1x_i - \bar{y})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}.
\end{align}
$$ {#eq-r2-as-yhat}

We also have a result for the ordinary least squares of the simple linear regresson that:

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}.
$$ {#eq-b0}

So the @eq-b0 applied to the @eq-r2-as-yhat results in:

$$
\begin{align}
R^2
&=  \frac{\sum_\limits{i=1}^{n}(\hat{\beta}_0 + \hat{\beta}_1x_i - \bar{y})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\\
&=  \frac{\sum_\limits{i=1}^{n}(\cancel{\bar{y}} - \hat{\beta}_1\bar{x} + \hat{\beta}_1x_i \cancel{-\bar{y}})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\\
&=  \frac{\sum_\limits{i=1}^{n}(- \hat{\beta}_1\bar{x} + \hat{\beta}_1x_i)^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\\
&=  \frac{\sum_\limits{i=1}^{n}[\hat{\beta}_1 (x_i- \bar{x})]^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\\
&=  \frac{\sum_\limits{i=1}^{n}\hat{\beta}^2_1(x_i- \bar{x})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\\
&=  \hat{\beta}^2_1\left(\frac{\sum_\limits{i=1}^{n}(x_i- \bar{x})^2}{ \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\right).
\end{align}
$${#eq-r2-as-b0}

Since we have that the variance of $x$ is given by

$$
s^2_x = \frac{1}{n-1}\sum_\limits{i=1}^{n}(x_i- \bar{x})^2.
$$ {#eq-sx}

We can divide both terms of the @eq-r2-as-b0 by $(n-1)$ and use the @eq-sx to rewrite it as

$$
\begin{align}
R^2
&=  \hat{\beta}^2_1\left(\frac{\frac{1}{n-1}\sum_\limits{i=1}^{n}(x_i- \bar{x})^2}{\frac{1}{n-1} \sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\right)\\
&=  \hat{\beta}^2_1\frac{s^2_x}{s^2_y}\\
&=  \left(\hat{\beta}_1\frac{s_x}{s_y}\right)^2.\\
\end{align}
$$ {#eq-r2-as-sx-sy}

Such as we have a result for $\hat{\beta}_0$ in the @eq-b0, we also have one for $\hat{\beta}_1$

$$
\hat{\beta}_1 = \frac{s_{xy}}{s^2_x},
$$ {#eq-b1}

where

-   $s_{xy}$ is the covariance between $x$ and $y$.

Finally, applying the @eq-b1 to the @eq-r2-as-sx-sy

$$
\begin{align}
R^2
&=  \left(\hat{\beta}_1\frac{s_x}{s_y}\right)^2\\
&=  \left(\frac{s_{xy}}{s^2_x}\frac{s_x}{s_y}\right)^2\\
&=  \left(\frac{s_{xy}}{s_xs_y}\right)^2\\
&=  (r)^2.\\
\end{align}
$$ {#eq-r2-as-r2}

At last in @eq-r2-as-r2 we can show that for the simple linear regression that $R^2 = (r)^2$.

# Adjusted R²

Another version of R², is the ajusted version, where it tries to correct the overestimation, by penalizing the number of variables used in the model.

To attempt that, instead of only the sum of squares we divide the terms by their respectives degrees of freedom:

$$
\frac{SS_{\mathrm{tot}}}{n-1},
$$ {#eq-s2-tot}

and

$$
\frac{SS_{\mathrm{res}}}{n-p-1},
$$ {#eq-s2-res}

where

-   $p$ is the number of explanatory variables.

Then using both @eq-s2-tot and @eq-s2-res applied to @eq-r2 we can compute the adjusted version:

$$
\begin{align}
R^2_{\mathrm{adj}}
&= 1 - \frac{\frac{\sum_\limits{i=1}^{n}(y_i - \hat{y}_i)^2}{n-p-1}}{\frac{\sum_\limits{i=1}^{n}(y_i - \bar{y})^2}{n-1}}\\
&= 1 - \left(\frac{\sum_\limits{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_\limits{i=1}^{n}(y_i - \bar{y})^2}\right) \left(\frac{n-1}{n-p-1}\right)\\
&= 1 - \left(1- R^2\right) \left(\frac{n-1}{n-p-1}\right).
\end{align}
$$ {#eq-r2-adj}

# Why it can be a poor choice

Despite being one of the most well-known metrics in modeling, it is also one of the most criticized, for a variety of reasons, such as:

-   Can be arbitrarily low even if the model follows the assumptions and makes sense; similarly, it can be arbitrarily close to 1 when the model is erroneous. A good example is when nonlinear models achieve a high R² for linear data;

-   Sensitivity to the number of predictors, which increases with the number of predictors, even if the predictors are irrelevant to the outcome. As we saw earlier, the adjusted version helps to mitigate this issue;

-   Because it relies primarily on the mean, outliers can have a substantial impact, making it also a poor prediction indication since it compares your model's error to the error of the average model;

-   It cannot be compared across datasets since it can only be compared when several models are fitted to the same data set with the same untransformed response variable.

**For more details on the points above:**

-   [Lecture 10: F-Tests, R², and Other Distractions](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/10/lecture-10.pdf)

-   [*Why* is R^2^ not a measure of goodness-of-fit?](https://www.quantics.co.uk/blog/r-we-squared-yet-why-r-squared-is-a-poor-metric-for-goodness-of-fit/)

-   [Why R-squared is worse than useless](https://getrecast.com/r-squared/)
