---
title: "An intro to Mean"
author: "Vinícius Félix"
date: "2023-08-13"
categories: [Intro to,Theory]
image: "intro-to-mean.png"
---

```{r setup,echo=FALSE,message=FALSE,warning=FALSE}
suppressWarnings(library(ggplot2))
suppressWarnings(library(relper))
suppressWarnings(library(dplyr))
suppressWarnings(library(tidyr))
suppressWarnings(library(janitor))
suppressWarnings(library(knitr))
suppressWarnings(library(kableExtra))
suppressWarnings(library(forcats))
```

In this post of the series **Intro to**, I'll navigate the Land of the Averages**.**

# **The Meme That Everyone Gets**

The arithmetic mean, also known as the "*mean*," is a fundamental concept in mathematics and statistics that represents the average value of a set of numbers. It is widely used to summarize data in a variety of fields.

To calculate it, add up all of the numbers in a dataset and divide by the total count, yielding a central value that evenly distributes the data points on a number line.

## The simple

The simple arithmetic mean, is given by:
$$
\frac{1}{n}\sum_\limits{i=1}^{n} x_i,
$$ {#eq-arithmetic}

where:

-    $x_i$ is a numeric vector of length $n$.

Because the arithmetic mean is simple to calculate and understand, it is accessible to a wide range of audiences, since it just entails the fundamental arithmetic operations of addition and division.

### **Outlier impact**

The arithmetic mean can be significantly influenced by extreme values. A single value that is unusually high or low can skew the result, resulting in an inaccurate representation of the central tendency. Extreme values in small samples can have a greater impact on the mean than extreme values in large samples.

Consider the following example. To begin, we will compute the mean of a given vector of numbers.

```{r, echo = FALSE}
x1 <- c(1,2,2,3,3,3,4,5,5,5,7)

x1

mean(x1)
```

So the simple arithmetic mean is 3.636364, now we will change the last number to an extreme value.

```{r, echo = FALSE}
x2 <- c(1,2,2,3,3,3,4,5,5,5,100)

x2

mean(x2)
```

As we can see, the mean increases significantly, resulting in a distorted and unrepresentative metric of the data.

### **May Not Reflect True Center**

In skewed distributions, the mean may not accurately represent the typical value experienced by the majority of data points. The mean can be pushed towards the distribution's tail.

Let's take a look at an example from a dataset from a exponential distribution.

```{r, echo = FALSE}
set.seed(123);exp_data <- tibble(x = rexp(n = 1000,rate = .5)) %>% 
  filter(x < 10)

amean <- mean(exp_data$x)
gmean <- relper::calc_mean(exp_data$x,"geometric")
hmean <- relper::calc_mean(exp_data$x,"harmonic")

plot <-
exp_data %>% 
  ggplot(aes(x = x))+
  geom_density(fill = "grey45", alpha = .5)+
  plt_theme_x(margin = .6)+
  plt_water_mark(vfx_watermark)+
  scale_x_continuous(
    limits = c(0,10),
    expand = c(0,0),
    breaks = 0:10
    )+
  scale_y_continuous(expand = c(0,0))+
  labs(
    x = "",
    y = "",
    col = "",
    subtitle = "Density of a exponential distribution"
  )+
  geom_vline(aes(xintercept = amean,col = "Arithmetic"), linewidth = 1)+
  scale_color_manual(values = palette_qua("ted_lasso",F))

plot
```

As we can see, the mean is dragged far away from the peak density value by the larger values.

## The weighted

The weighted arithmetic mean is a variant that considers not only the values in a dataset but also assigns different weights to each value based on its importance or significance.

In other words, rather than treating all values equally, the weighted mean favors some over others based on predetermined weights.

$$
\frac{1}{\sum_\limits{i=1}^{n}w_i}\sum_\limits{i=1}^{n} w_ix_i,
$$ {#eq-weighted}

where:

-    $x_i$ is a numeric vector of length $n$;

-    $w_i$ is a numeric vector of length $n$, with the respectives weights for the values of $x_i$.

A practical application is in academic grading, the weighted mean of exam scores might be used, where different exams carry different weights based on their importance.

A common case is when proportion ($p_i$) are used as weigths, and since:

$$
\sum_\limits{i=1}^{n} p_i = 1.
$$ {#eq-proportion-sum}

When applying @eq-proportion-sum to @eq-weighted we have that:

$$
\sum_\limits{i=1}^{n} w_ix_i.
$$ {#eq-weighted-proportion}
Even if it is an interesting application, assigning weights to data points is frequently subjective and can be influenced by personal judgment or assumptions. The weighted mean's accuracy is heavily dependent on the appropriateness of the weights chosen.

In addition, calculating the weighted mean requires an extra step when compared to the simple arithmetic mean, which may complicate the analysis and calculations, particularly when dealing with large datasets. In some cases, the rationale for assigning specific weights may not be transparent or well-documented, which can make replicating or validating the analysis difficult.

## The trimmed

A trimmed arithmetic mean is a statistical measure that computes the mean of a dataset by excluding a percentage of the lowest and highest values, reducing the impact of outliers and extreme values.

The amount of trimming can be adjusted to strike a balance between retaining meaningful data and reducing the impact of outliers.

Let's go back to our previous outlier example.

```{r, echo = FALSE}

x2

mean(x2,trim = .1)
```

That is the same as applying a simple arithmetic mean to

```{r, echo =FALSE}

x3 <- c(2,2,3,3,3,4,5,5,5)

x3

mean(x3)
```

As the data is trimmed, we achieve a more representative metric for our data, but this is due to an intentional loss of information, which may result in an incomplete representation of the data's full range. Important insights or trends within the data may be overlooked depending on the extent of trimming.

# **When Averages Go Proportional**

The geometric mean is a statistical measure used to determine the central tendency of a set of values, particularly when those values are multiplicatively related.

The geometric mean, as opposed to the arithmetic mean, involves multiplying the values and then taking the $n$th root, where $n$ is the total number of values. This makes it especially useful for data with exponential or multiplicative growth, such as investment returns, population growth rates, or scientific measurements.

$$
\sqrt[n]{\prod_\limits{i=1}^{n} x_i},
$$ {#eq-geometric}

where:

-    $x_i$ is a numeric vector of length $n$.

Another way to write the @eq-geometric is:

$$
\begin{align}
\sqrt[n]{\prod_\limits{i=1}^{n} x_i}
&= \sqrt[n]{x_1x_2...x_n} \\
&= (x_1x_2...x_n)^{1/n} \\
&= \mathcal{e}^{\mathcal{ln}(x_1x_2...x_n)^{1/n}}\\
&= \mathcal{e}^{\frac{1}{n}[\mathcal{ln}(x_1)+\mathcal{ln}(x_2)...+\mathcal{ln}(x_n) ]}\\
&= \mathcal{e}^{\frac{1}{n}\sum_\limits{i=1}^{n}\mathcal{ln}(x_i)}.
\end{align}
$$ {#eq-geometric-log}

So we can see that the geometric mean can be written as the exponetial of the simple arithmetic mean (@eq-arithmetic) of the logarithmic of $x_i$.

***Logarithmic scale***

:   [*If you're unfamiliar with the logarithmic function and its functions, check out our introduction post about it.*](https://vbfelix.github.io/posts/0003-log-scale/)

Let's go back to our first example and see in comparison to the arithmetic mean.

```{r, echo = FALSE}

x1

tibble(
  arithmetic = relper::calc_mean(x1,"arithmetic"),
  geometric = relper::calc_mean(x1,"geometric")
) %>% 
  kable()
```

We can see that the geometric mean yields a slightly lower value.

## **Accurate Representation of Growth**

When using the geometric mean with fractional values, the result represents the *"average growth factor"* between the values. It indicates the factor by which you need to multiply each value to obtain the overall product.

```{r, echo = FALSE}

x5 <- c(.1,.2,.3,.05,.004)

x5

tibble(
  arithmetic = relper::calc_mean(x5,"arithmetic"),
  geometric = relper::calc_mean(x5,"geometric")
) %>% 
  kable()

```

## **Sensitivity to Small Values**

The geometric mean is sensitive to small values in the dataset. This sensitivity can be advantageous when you want to emphasize the impact of small values or identify trends that might be overshadowed by larger values.

Let's add a small value instead of the first value in our first example.

```{r, echo = FALSE}
x4 <- c(.1,2,2,3,3,3,4,5,5,5,7)

x4

tibble(
  arithmetic = relper::calc_mean(x4,"arithmetic"),
  geometric = relper::calc_mean(x4,"geometric")
) %>% 
  kable()

```

As we can see, the presence of the smaller value severely "*penalizes"* the geometric value.

## **Outlier Impact**

Unlike the arithmetic mean, the geometric mean is less affected by outliers. This is due to the fact that the geometric mean is equivalent to taking the arithmetic mean of the logarithms of the values. This property has the effect of compressing the data, making extreme values contribute less to the final result.

Let's go back to our outlier example.

```{r, echo = FALSE}

x2

tibble(
  arithmetic = relper::calc_mean(x2,"arithmetic"),
  geometric = relper::calc_mean(x2,"geometric")
) %>% 
  kable()
```

As we can see, the geometric mean has a significant less impact of the larger value.

## **Zero Gravity: When Geometric Mean Gets Lost in Space**

When a zero value is present in a dataset, calculating the geometric mean becomes problematic because the value will always be zero, since it is the product of values.

## **Undefined for Negative Values**

Because a negative number raised to a non-integer exponent can produce complex results, the concept of a geometric mean for negative values is meaningless in the realm of real numbers.

## **May Not Reflect True Center**

```{r, echo = FALSE}

plot+
  geom_vline(aes(xintercept = gmean,col = "Geometric"), linewidth = 1)

```

In this scenario, the geometric mean approaches the peak value of the density in the example because it is more resistant to larger values and more sensitive to smaller data.

# **The Odd One Out in the Mean Squad**

The harmonic mean is a statistical measure of central tendency used to calculate the average of a set of values when their reciprocal (inverses) is more important than their arithmetic mean.

$$
\frac{n}{\sum_\limits{i=1}^{n}\frac{1}{x_i}}
$$ {#eq-harmonic}

where:

-    $x_i$ is a numeric vector of length $n$.

Let's compare it to the other approaches.

```{r, echo = FALSE}

x1

tibble(
  arithmetic = relper::calc_mean(x1,"arithmetic"),
  geometric = relper::calc_mean(x1,"geometric"),
  harmonic = relper::calc_mean(x1,"harmonic")
) %>% 
  kable()
```

We can see that harmonic provided a smaller value to our fist example.

## **Sensitivity to Small Values**

Since the harmonic mean takes the inverse of the original value, it is even more sensitive to small values than the geometric mean, leading to extremely large results for values close to zero.

```{r, echo = FALSE}

x4

tibble(
  arithmetic = relper::calc_mean(x4,"arithmetic"),
  geometric = relper::calc_mean(x4,"geometric"),
  harmonic = relper::calc_mean(x4,"harmonic")
) %>% 
  kable()
```

## Outlier impact

```{r, echo = FALSE}

x2

tibble(
  arithmetic = relper::calc_mean(x2,"arithmetic"),
  geometric = relper::calc_mean(x2,"geometric"),
  harmonic = relper::calc_mean(x2,"harmonic")

) %>% 
  kable()
```

At the same time, a larger value has less of an impact on the method.

## **Zero Gravity: When Harmonic Mean Gets Lost in Space**

Calculating the harmonic mean when there is a zero value in a dataset becomes difficult because the value is undefined since it is the sum of the inverse of the values and there is no division by zero.

## **May Not Reflect True Center**

```{r, echo = FALSE}

plot+
  geom_vline(aes(xintercept = gmean,col = "Geometric"), linewidth = 1)+
  geom_vline(aes(xintercept = hmean,col = "Harmonic"), linewidth = 1)

```

Because the harmonic mean is more robust to larger values and more sensitive to smaller data, it provides a lower value in the example above than the other methods.

# **Time Travelers' Guide**

A moving average is a statistical technique used to smooth out fluctuations in time series or sequential data by averaging a subset of data points within a moving window. It reduces noise to reveal underlying trends and patterns, and it is available in a variety of forms, including the simple moving average, weighted moving average, and exponential moving average.

Moving averages are used to detect trends and reduce noise in fields such as finance, economics, signal processing, and climate science. However, because of the lag in detecting rapid changes and the potential impact of outliers, the moving average type and window size should be chosen based on data characteristics and analysis objectives.

```{r, echo = FALSE}
n <- 100

set.seed(213);x <- rnorm(n,mean = 3,sd = 3)


ma_data <-
tibble(
  id = 1:n,
  x = x,
  Trend = cumsum(x) + 2*x,
  Seasonality = sin(sqrt(id)*3)
) %>% 
  pivot_longer(cols = c(Trend,Seasonality)) %>%
  group_by(name) %>% 
  mutate(
    mean = mean(value),
    xcut = cut(id,seq(0,100,10))
    ) %>% 
  ungroup()
```

In time series we can have some special patterns in data, the two most common are trends, which represent long-term consistent movements in data, whether upward, downward, or flat, and seasonality, which refers to recurring and predictable patterns that repeat at regular intervals, linked to specific calendar periods.

```{r, echo = FALSE}

ma_plot<-
ma_data %>% 
  ggplot(aes(id,value))+
  geom_line()+
  facet_grid(rows = vars(name),scales = "free_y")+
  plt_theme_y()+
  plt_water_mark(vfx_watermark)+
  scale_x_continuous(breaks = seq(0,100,10))+
  labs(x = "",y = "", col = "")+
  geom_hline(aes(yintercept = mean,col = "Average" ), linewidth = 1)

ma_plot
```

When we apply the mean to the entire period, the result can be meaningless. To use the moving average, we must first define the interval over which the average will be calculated; in the example below, we will first display the moving average for each 10 units of time.

```{r, echo = FALSE}
ma_aux <-
  ma_data %>% 
  group_by(name,xcut) %>% 
  mutate(
    x = mean(id),
    xmin = min(id),
    xmax = max(id),
    y = mean(value)
  )


ma_plot +
  geom_errorbarh(data = ma_aux,aes(y = y, xmin = xmin,xmax = xmax, col = "Moving average"),
                 linewidth = 1)+
  geom_point(data = ma_aux,aes(y = y, x = x, col = "Moving average"),
                 size = 3)
```

The moving average can improve data smoothing, noise reduction, and trend identification by highlighting underlying patterns by averaging subsets of data points. However, due to the equal weighting of all data points, it introduces a lag in detecting rapid changes, is sensitive to window size, and may not accurately capture recent trends.

While moving averages are effective for regular patterns, they may be ineffective for irregular data or abrupt shifts, potentially leading to oversimplification and loss of detail in the analysis.
