{
  "hash": "6ce2ce2ebc1b10058f8d9ce3587bed11",
  "result": {
    "markdown": "---\ntitle: \"Intro to: R²\"\nauthor: \"Vinícius Félix\"\ndate: \"2024-04-16\"\ncategories: [Intro to]\nimage: \"banner.png\"\n---\n\n::: {.cell}\n\n:::\n\n\nIn this post, we explore the infamous metric R².\n\n# Context\n\nThe R² or the coefficient of determination is a metric commonly used to measure the goodness of fit of a model, it is given by:\n\n$$\nR^2 = 1- \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}},\n$$ {#eq-r2}\n\nwhere:\n\n-   $SS_{\\mathrm{res}}$ is the sum of squares of residuals;\n\n-   $SS_{\\mathrm{tot}}$ is the total sum of squares.\n\nThe sum of squares of residuals is given by: $$\nSS_{\\mathrm{res}} = \\sum_\\limits{i=1}^{n}(y_i - \\hat{y}_i)^2,\n$$ {#eq-ssres}\n\nwhere:\n\n-   $y_i$ is the response variable;\n\n-   $\\hat{y}_i$ is the fitted value for $y_i$.\n\nThe graphic below shows the difference between the original values and the model ($y_i - \\hat{y}_i$).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe total sum of squares is given by: $$\nSS_{\\mathrm{tot}} = \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2,\n$$ {#eq-sstot}\n\nwhere:\n\n-   $y_i$ is the response variable;\n\n-   $\\bar{y}$ is the average value of $y$.\n\nThe graphic below shows the difference between the response variable's original values and its mean ($y_i - \\bar{y}$).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nTo put it simply, the coefficient measures the relative difference between the sum of squares of your model compared to a simplistic model (the average), where its values is considered good if it equals 1, meaning that the $SS_{\\mathrm{res}}$ is close to 0 and is considered bad as it approaches 0, since the squared sum of the residuals would be close as using the average as a model.\n\n## \n\n## When R² = (r)²?\n\nA well-known fact is that for simple linear regression, we have a direct relationship between the coefficient of determination and the Pearson linear correlation coefficient. To show the relationship between the $R^2$ and $r$ , first we have that\n\n$$\nSS_{\\mathrm{tot}} = SS_{\\mathrm{res}} + SS_{\\mathrm{reg}}.  \n$$ {#eq-sstot-ssres-ssreg}\n\nwhere $SS_{\\mathrm{reg}}$ is the sum of squares due to regression, also known as the explained sum of squares, giving by:\n\n$$\nSS_{\\mathrm{reg}} = \\sum_\\limits{i=1}^{n}(\\hat{y}_i - \\bar{y})^2.\n$$ {#eq-ssreg}\n\nOr graphically,\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThen, applying the @eq-sstot-ssres-ssreg to the @eq-r2 :\n\n$$\n\\begin{align}\nR^2\n&=  1- \\frac{SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}\\\\\n&=  \\frac{SS_{\\mathrm{tot}}}{SS_{\\mathrm{tot}}}- \\frac{SS_{\\mathrm{res}}}\n{SS_{\\mathrm{tot}}}\\\\\n&=  \\frac{SS_{\\mathrm{tot}} - SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}\\\\\n&=  \\frac{SS_{\\mathrm{reg}} + SS_{\\mathrm{res} } - SS_{\\mathrm{res}}}{SS_{\\mathrm{tot}}}\\\\\n&=  \\frac{SS_{\\mathrm{reg}}}{SS_{\\mathrm{tot}}}.\\\\\n\\end{align}\n$$ {#eq-r2-to-ssreg}\n\nSo with the @eq-ssreg applied to the @eq-r2-to-ssreg, we have that\n\n$$ \\begin{align} R^2 &=  \\frac{SS_{\\mathrm{reg}}}{SS_{\\mathrm{tot}}}\\\\ &=  \\frac{\\sum_\\limits{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}. \\end{align} $$ {#eq-r2-as-ssreg}\n\nFor a simple linear regression, we can compute the fitted value as\n\n$$\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i,\n$$ {#eq-pred-value}\n\nwhere:\n\n-   $\\hat{\\beta}_0$ is the estimated value of the intercept;\n\n-   $\\hat{\\beta}_1$ is the estimated value of the slope coefficient;\n\n-   $x_i$ is the explanatory variable.\n\nApplying the @eq-pred-value in the @eq-r2-as-ssreg we have that\n\n$$\n\\begin{align}\nR^2\n&=  \\frac{\\sum_\\limits{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\frac{\\sum_\\limits{i=1}^{n}(\\hat{\\beta}_0 + \\hat{\\beta}_1x_i - \\bar{y})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}.\n\\end{align}\n$$ {#eq-r2-as-yhat}\n\nWe also have a result for the ordinary least squares of the simple linear regresson that:\n\n$$\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}.\n$$ {#eq-b0}\n\nSo the @eq-b0 applied to the @eq-r2-as-yhat results in:\n\n$$\n\\begin{align}\nR^2\n&=  \\frac{\\sum_\\limits{i=1}^{n}(\\hat{\\beta}_0 + \\hat{\\beta}_1x_i - \\bar{y})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\frac{\\sum_\\limits{i=1}^{n}(\\bar{y} - \\hat{\\beta}_1\\bar{x} + \\hat{\\beta}_1x_i - \\bar{y})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\frac{\\sum_\\limits{i=1}^{n}(- \\hat{\\beta}_1\\bar{x} + \\hat{\\beta}_1x_i)^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\frac{\\sum_\\limits{i=1}^{n}[\\hat{\\beta}_1 (x_i- \\bar{x})]^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\frac{\\sum_\\limits{i=1}^{n}\\hat{\\beta}^2_1(x_i- \\bar{x})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\hat{\\beta}^2_1\\frac{\\sum_\\limits{i=1}^{n}(x_i- \\bar{x})^2}{ \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}.\n\\end{align}\n$$ {#eq-r2-as-b0}\n\nSince we have that the variance of $x$ is given by\n\n$$\ns^2_x = \\frac{1}{n-1}\\sum_\\limits{i=1}^{n}(x_i- \\bar{x})^2.\n$$ {#eq-sx}\n\nWe can divide both terms of the @eq-r2-as-b0 by $(n-1)$ and usethe @eq-sx to rewrite it as\n\n$$\n\\begin{align}\nR^2\n&=  \\hat{\\beta}^2_1\\frac{\\frac{1}{n-1}\\sum_\\limits{i=1}^{n}(x_i- \\bar{x})^2}{\\frac{1}{n-1} \\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\\\\n&=  \\hat{\\beta}^2_1\\frac{s^2_x}{s^2_y}\\\\\n&=  \\left(\\hat{\\beta}_1\\frac{s_x}{s_y}\\right)^2.\\\\\n\\end{align}\n$$ {#eq-r2-as-sx-sy}\n\nSuch as we have a result for $\\hat{\\beta}_0$ in the @eq-b0, we also have one for $\\hat{\\beta}_1$\n\n$$\n\\hat{\\beta}_1 = \\frac{s_{xy}}{s^2x},\n$$ {#eq-b1}\n\nwhere\n\n-   $s_{xy}$ is the covariance between $x$ and $y$.\n\nFinally, applying the @eq-b1 to the @eq-r2-as-sx-sy\n\n$$\n\\begin{align}\nR^2\n&=  \\left(\\hat{\\beta}_1\\frac{s_x}{s_y}\\right)^2\\\\\n&=  \\left(\\frac{s_{xy}}{s^2_x}\\frac{s_x}{s_y}\\right)^2\\\\\n&=  \\left(\\frac{s_{xy}}{s_xs_y}\\right)^2\\\\\n&=  (r)^2.\\\\\n\\end{align}\n$$ {#eq-r2-as-r2}\n\nAt last in @eq-r2-as-r2 we can show that for the simple linear regression that $R^2 = (r)^2$.\n\n# Adjusted R²\n\nAnother version of R², is the ajusted version, where it tries to correct the overestimation, by penalizing the number of variables used in the model.\n\nTo attempt that, instead of only the sum of squares we divide the terms by their respectives degrees of freedom:\n\n$$\n\\frac{SS_{\\mathrm{tot}}}{n-1},\n$$ {#eq-s2-tot}\n\nand\n\n$$\n\\frac{SS_{\\mathrm{res}}}{n-p-1},\n$$ {#eq-s2-res}\n\nwhere\n\n-   $p$ is the number of explanatory variables.\n\nThen using both @eq-s2-tot and @eq-s2-res applied to @eq-r2 we can compute the adjusted version:\n\n$$\n\\begin{align}\nR^2_{\\mathrm{adj}}\n&= 1 - \\frac{\\frac{\\sum_\\limits{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-p-1}}{\\frac{\\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}{n-1}}\\\\\n&= 1 - \\left(\\frac{\\sum_\\limits{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_\\limits{i=1}^{n}(y_i - \\bar{y})^2}\\right) \\left(\\frac{n-1}{n-p-1}\\right)\\\\\n&= 1 - \\left(1- R^2\\right) \\left(\\frac{n-1}{n-p-1}\\right).\n\\end{align}\n$$ {#eq-r2-adj}\n\n# Why it can be a poor choice\n\nDespite being one of the most well-known metrics in modeling, it is also one of the most criticized, for a variety of reasons, such as:\n\n-   Can be arbitrarily low even if the model follows the assumptions and makes sense; similarly, it can be arbitrarily close to 1 when the model is erroneous. A good example is when nonlinear models achieve a high R² for linear data;\n\n-   Sensitivity to the number of predictors, which increases with the number of predictors, even if the predictors are irrelevant to the outcome. As we saw earlier,. the adjusted version helps to mitigate this issue;\n\n-   Because it relies primarily on the mean, outliers can have a significant impact;\n\n-   A poor prediction indicator because it compares the error of your model to the error of the average;\n\n-   It cannot be compared across datasets since it can only be compared when several models are fitted to the same data set with the same untransformed response variable.\n\n**For more details on the points above:**\n\n-   [Lecture 10: F-Tests, R², and Other Distractions](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/10/lecture-10.pdf)\n\n-   [*Why* is R^2^ not a measure of goodness-of-fit?](https://www.quantics.co.uk/blog/r-we-squared-yet-why-r-squared-is-a-poor-metric-for-goodness-of-fit/)\n\n-   [Why R-squared is worse than useless](https://getrecast.com/r-squared/)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}