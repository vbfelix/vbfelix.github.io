{
  "hash": "8901d0d80cfd14cf11193c3051efd0ec",
  "result": {
    "markdown": "---\ntitle: \"An intro to Normal Distribution\"\nauthor: \"Vinícius Félix\"\ndate: \"2023-06-18\"\ncategories: [Intro to, Theory]\nimage: \"intro-to-normal-distribution.png\"\n---\n\n\nIn this post of the series **Intro to**, I'll give an introduction to the normal distribution.\n\n\n::: {.cell}\n\n:::\n\n\n# Introduction\n\nA probability distribution is a function that describes the likelihood of various outcomes in a random event. It assigns probabilities to each possible outcome, indicating how likely each is.\n\nIn statistics the normal distribution holds the most popular position of distributions. It has many names, such as:\n\n-   Bell curve;\n\n-   Gaussian distribution;\n\n-   Laplace-Gauss distribution.\n\nThe term \"normal\" does not mean \"typical\" or \"ordinary\" in this context, but stems from the Latin word *normalis*, which means \"perpendicular\" or \"at right angles.\"\n\nDue to its mathematical properties, symmetry, and prevalence in nature and various phenomena, Carl Friedrich Gauss named it the normal distribution, in the 19th century.\n\n# Math\n\nA Normal distribution possess two parameters, the mean ($\\mu$) and the standard deviation ($\\sigma$), so we can describe a variable $X$ following a normal distribution as $X \\sim \\mathcal{N}(\\mu,\\sigma)$:\n\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\mathcal{e}^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n$$ {#eq-normal-distribution}\n\n## Standard normal\n\nThe simplest case of a normal distribution is a $\\mathcal{N}(0,1)$, also called a standard normal distribution or Z-distribution:\n\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\mathcal{e}^{-\\frac{x^2}{2}}.\n$$ {#eq-z-distribution}\n\n# Properties\n\n## Simmetry\n\nThe normal distribution has a balanced and mirror-like shape around its center, which is characterized by its symmetry and central tendency. Because of its symmetry, values on either side of the mean have equal probabilities, and the alignment of the mean, median, and mode at the center.\n\nHere an example of a standard normal distribution:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Bell-shape curve\n\nThe shape of the normal distribution is also characterized by gradually decreasing probabilities as the values move away equally from the mean in both directions.\n\nEven tough it has zero skewness the variance can imply in a kurtosis change, here a few example:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Chebyshev's inequality\n\nChebyshev's inequality is a mathematical inequality that can be applied to any probability distribution with defined mean and variance. It gives us an bound on the likelihood that a random variable deviates from its mean by a certain amount.\n\n$$\nP(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}, \\quad k >0; \\quad k \\in \\mathbb{R},\n$$ {#eq-chebyshev-inequality}\n\nwhere:\n\n-   $X$ is a random variable with variance $\\sigma^2$ and expected value $\\mu$;\n\n-   $\\sigma$ is a finite non-zero standard deviation;\n\n-   $\\mu$ is a finite expected value;\n\n-   $k$ is a given real number greater then zero.\n\nWhen applied to the normal distribution we have that:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nApproximately 68% of values in a normal distribution are within one standard deviation ($\\sigma$) of the mean, 95% are within two standard deviations, and 99.7% are within three standard deviations.\n\n# The Central Limit Theorem (CLT)\n\nAccording to the CLT, if we combine a large number of independent and identically distributed random variables, the sum will have an approximately normal distribution, regardless of the shape of the original distribution.\n\nFor this theorem to work we have three key assumptions for the random variables: independence, identical distribution, and finite variance.\n\nIn simpler terms, the CLT allows us to approximate the normal distribution when dealing with large sample sizes and sums of random variables.\n\nA simple application is used in my previous post, where we see that [the sum of two dices is approximately a normal distribution](https://vbfelix.github.io/posts/0005-settlers-of-catan/#is-the-dice-fair).\n\n## \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}