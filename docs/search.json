[
  {
    "objectID": "posts/0020-better-presentations/index.html",
    "href": "posts/0020-better-presentations/index.html",
    "title": "Some notes: Better Presentations",
    "section": "",
    "text": "In this post, I describe some notes I took after taking a course to improve my presentation skills."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#the-why",
    "href": "posts/0020-better-presentations/index.html#the-why",
    "title": "Some notes: Better Presentations",
    "section": "The why",
    "text": "The why\nEven if you are being obvious, giving a reason can make you more convincing, so always state the motive."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#scarcity",
    "href": "posts/0020-better-presentations/index.html#scarcity",
    "title": "Some notes: Better Presentations",
    "section": "Scarcity",
    "text": "Scarcity\nMessages with a due date, scarcity quantities, rivalry, and information exclusivity can create a sense of urgency because the fear of losing something generates more engagement than the opportunity to win something."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#social-proof",
    "href": "posts/0020-better-presentations/index.html#social-proof",
    "title": "Some notes: Better Presentations",
    "section": "Social proof",
    "text": "Social proof\nThe collective behavior of individuals in a group acting without centralized direction is described by the herd effect. Because the majority of people follow, when we have initiators, the majority usually copies them. Then we can use techniques like customer logos, influencer testimonials, and numbers to demonstrate results."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#authority",
    "href": "posts/0020-better-presentations/index.html#authority",
    "title": "Some notes: Better Presentations",
    "section": "Authority",
    "text": "Authority\nThe public perceives importance based on image, role, and qualification. Demonstrate your knowledge of the subject."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#affinity",
    "href": "posts/0020-better-presentations/index.html#affinity",
    "title": "Some notes: Better Presentations",
    "section": "Affinity",
    "text": "Affinity\nEmpathy and identification lead to increased engagement. Concentrate on your audience so that they feel important. Focus on the whys and why nots, ask questions, and connect."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#to-present-is-to-plan",
    "href": "posts/0020-better-presentations/index.html#to-present-is-to-plan",
    "title": "Some notes: Better Presentations",
    "section": "To present is to plan",
    "text": "To present is to plan\nDo not wait until the last minute to prepare your presentation; know the location, the audience, the environment, the size of the audience, and their profile. Gather as much information as possible so that you can prepare the most effective format and also adapt your message to the best.\nOverestimation of timing is a common error. Pauses should be planned, and remember to include your estimate if there are a question section. Simulating the presentation is a good way to see what works and what doesn’t, as well as how long it takes.\nHowever, this does not cover the audience effect, so you must anticipate what reactions your presentation will provoke. For example, if you have a funny slide that can cause laughter, you should plan a pause to accommodate that."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#starts-with-a-boom",
    "href": "posts/0020-better-presentations/index.html#starts-with-a-boom",
    "title": "Some notes: Better Presentations",
    "section": "Starts with a boom",
    "text": "Starts with a boom\nIt is very common nowadays to lose one’s attention, so you must captivate your audience from the start.\nPass your overall message at the outset, explain why you’re giving the presentation and why the audience should listen to you, and use an element to really emphasize that.\nBut be careful not to show off all of your content in the first act; save the thriller for the final act."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#the-four-acts",
    "href": "posts/0020-better-presentations/index.html#the-four-acts",
    "title": "Some notes: Better Presentations",
    "section": "The four acts",
    "text": "The four acts\nA method based on the hero’s journey can be used to build your presentation in four distinct moments.\n\nThe Connection\nTo make a genuine connection, you must connect your message to the needs of your audience, so first determine who you are speaking to.\nAsk yourself why someone needs to stop their life to listen to you.\nIt is difficult to respond to this question for the general public; no one pleases everyone, so create a persona that represents your target audience. Then, modify your language, references, and examples.\nAside from the context, you should also be mindful of physical elements such as facial expression, voice tone, and eye contact. Be present; avoid displaying behaviors such as crossing your arms or losing focus.\nFinally, try to create empathy between yourself and your public need by using a personal example.\n\n\nThe Villain\nIn this act, you highlight the issue, pain, or impediment that is causing your audience to suffer.\nThere are three elements that can help you:\n\nStatistics, use numbers that showcase the problem or the consequences of not resolving it;\nAudiente pain, tell your message in a way that the villain is a common enemy for you and your public;\nEmotional reactions,use visual elements that emphasize the villain or even try to personify it.\n\n\n\nThe Hero\nIn this act, you highlight how to defeat the villain, so how to solve the problem.\nThere are elements that can help you:\n\nBe objetive, use short phrases;\nAvoid the obvious, try to innovate and surprise when showing your solution;\nCredibility, bring proof of your solution, such as data or even people testominy, the social proof and authority mental triggers are both welcome.\n\n\n\nThe Story Moral\nIn the final act, you must conclude your presentation with a single message that summarizes the story you have told up to this point.\nThis element, like a slogan, must be captivating, easy to remember, brief, and create an identification.\nThere are elements that can help you:\n\nRhyme, rhymed phrase can be catchy;\nAmbiguity, double meaning can spark interest and debate;\nIrony, this can surprise the public;\nJuxtaposition, the contrast of ideas can generate discussion;\nRepetition, you can use a element that repeat the message passed in the other acts.\n\nAfter the message has been delivered, conclude your presentation as assertively as possible; do not linger, show videos, mention others, or even say that your time is up.\nFinally, include a call to action message in which you ask your audience to follow you, click a link, or do something."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#the-three-layers",
    "href": "posts/0020-better-presentations/index.html#the-three-layers",
    "title": "Some notes: Better Presentations",
    "section": "The three layers",
    "text": "The three layers\nWe have the Triune Brain theory from Paul MacLean, where three brains layers are described as:\n\nLizard: instincts;\nMammal: emotions, memories and habits;\nHuman: language, thought, imagination and rationalising.\n\nWe can use them as a reference to reach the public in differente aspects.\nBe Simple\n\nAvoid long/complex text;\nUse images, graphs, icons and relevant color schemes;\nBe clear and direct.\n\nBe Emotional\n\nUse elements that connect the public to the theme;\nAvoid impersonal data;\nUse stories, examples and analogies;\nSmile, when presenting something positive.\n\nBe Intelligent\n\nUse elements to stimulate the learning;\nUse data and facts, with the referenced source;\nProvoke the thinking."
  },
  {
    "objectID": "posts/0020-better-presentations/index.html#the-layout",
    "href": "posts/0020-better-presentations/index.html#the-layout",
    "title": "Some notes: Better Presentations",
    "section": "The layout",
    "text": "The layout\nOne of the most difficult aspects is the presentation’s aesthetic; even if you minimize the effect, every visual aspect matters.\nA common error is to include everything in your presentation; most information can be spoken and does not need to be shown, avoiding polluted slides and redudancy.\nA few techniques can be used to better show information:\n\nSynthesization, avoid redundancy and keep things as simples as possible;\nHierarquization, separate and organize your information based on their structure;\nContrast, showcase elements that are opposite.\n\nYou must pay close attention to the use of visual elements to aid in the transmission of your message:\n\nBackground, avoid polluted ones, and if you must use an image, consider its resolution;\nColor, be cautious when combining because there is both harmony and contrast between them; try to match your message to the choice;\nShapes, they are typically used to attract attention, particularly with texts. They can also be used in conjunction with numbers to emphasize their magnitude.\nTypography, use caution when selecting and sizing fonts, and avoid using multiple fonts; a font already has a family with variants."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html",
    "href": "posts/0018-measure-what-matters/index.html",
    "title": "Unveiling the pages: Measure what matters",
    "section": "",
    "text": "In this post, we will dive into the pages of John Doerr’s book Measure What Matters."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#google-meet-okrs",
    "href": "posts/0018-measure-what-matters/index.html#google-meet-okrs",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.01.Google, Meet OKR’s",
    "text": "1.01.Google, Meet OKR’s\n\n\nYogi Berra\n\nIf you don’t know where you are going, you might wind up someplace else.\n\n\n\nWhat I learned:\n\nAn objective (O) is a goal that must be met. Because it can be a broad statement, key results (KR) will be used to monitor and predict when the goal will be met, and they must then be measured and concisely expressed in a number.\nOKR is a protocol for defining goals that is useful for businesses, teams, and individuals. Even though he uses the term “saves”, the methodology is merely a guide, not a replacement for aspects such as leadership, creative culture, and common sense."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#the-father-of-okrs",
    "href": "posts/0018-measure-what-matters/index.html#the-father-of-okrs",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.02.The Father of OKR’s",
    "text": "1.02.The Father of OKR’s\n\n\nAndy Grove\n\nThere are so many people working so hard and achieving so little.\n\n\n\nWhat I learned:\n\nLess is more, a few (3 to 5) good objectives are enough, and with of 5 KR maximum each;\nFrom the ground up, begin defining goals from the ground up, rather than from the top;\nTeam play, you can impose goals, but discuss the key results;\nBe brave, comfortable goals can be a safe zone for the team;\nBe adaptable;if external factors change, why not update our OKRs?\nNot a weapon, OKR’s are tools to monitor our goals, and they should not be used as performance indicators."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#operation-crush-an-intel-story",
    "href": "posts/0018-measure-what-matters/index.html#operation-crush-an-intel-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.03.Operation Crush: An Intel Story",
    "text": "1.03.Operation Crush: An Intel Story\nWhat I learned:\n\nOKRs provided cohesion and transparency to the company operation, creating a sense of urgency but not despair, and they were able to reclaim their position as number one through this organization."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#superpower1-focus-and-commit-to-priorities",
    "href": "posts/0018-measure-what-matters/index.html#superpower1-focus-and-commit-to-priorities",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.04.Superpower#1: Focus and Commit to Priorities",
    "text": "1.04.Superpower#1: Focus and Commit to Priorities\nWhat I learned:\n\nKey results must be measurable, either in terms of quantities to measure and achieve, or in terms of factual accomplishments that can be checked to see if they were completed or not by a yes or no question;\nTo not become overly focused on unidimensional OKRs, as this may cause us to overlook other points of view. However, excessively greedy OKRs increase the risk of overlooking a critical aspect;\nIn dynamic markets, OKRs can be set every three months, but the period is flexible and must be tailored to each situation;\nMeasure both the effect and the side effect to avoid unanticipated consequences of achieving your goal;\nTo avoid the implication of having only a number to achieve, link a quantitative goal to a qualitative one;\nSet three levels of OKR goals: achievable, possible challenge, and desire.\nAvoid the temptation to have multiple objectives by limiting them to the most important ones."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#focus-the-remind-story",
    "href": "posts/0018-measure-what-matters/index.html#focus-the-remind-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.05.Focus: The Remind Story",
    "text": "1.05.Focus: The Remind Story\nWhat I learned:\n\nThree preliminary steps to create a businness:\n\nSolve a problem;\nBuild a simple product;\nTalk with your users.\n\nYou will not get everything right the first time, but you must begin;\nConsider your capabilities and be realistic when setting your goals."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#commit-the-nuna-story",
    "href": "posts/0018-measure-what-matters/index.html#commit-the-nuna-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.06.Commit: The Nuna Story",
    "text": "1.06.Commit: The Nuna Story\nWhat I learned:\n\nDo not try to implement OKR at all levels at the same time; it must be viewed as a tool rather than a necessary evil, so the high level must embrace it and it will spread organically;\nIf a goal is too difficult to achieve, it is easier to give up on him;\nPerhaps the first attempt will fail, but keep trying and correcting previous errors."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#superpower2-align-and-connect-for-teamwork",
    "href": "posts/0018-measure-what-matters/index.html#superpower2-align-and-connect-for-teamwork",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.07.Superpower#2: Align and Connect for Teamwork",
    "text": "1.07.Superpower#2: Align and Connect for Teamwork\n\n\nSteve Jobs\n\nIt doesn’t make sense to hire smart people and tell them what to do. We hire smart people so they can tell us what to do.\n\n\n\nWhat I learned:\n\nPublic goals can increase transparency while also facilitating corrections and criticism. This reduces redundancy, which is especially important in larger organizations where two people working on the same task can be common;\nA KR can also be an objective in another level of OKR, but do so with caution because we risk losing velocity and flexibility, as well as the horizontal aspect, because the vertical approach will be used to establish the levels hierarchy."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#align-the-myfitnesspal-story",
    "href": "posts/0018-measure-what-matters/index.html#align-the-myfitnesspal-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.08.Align: The MyFitnessPal Story",
    "text": "1.08.Align: The MyFitnessPal Story\nWhat I learned:\n\nDo not treat OKRs as islands; they are interconnected, so it is critical to share and discuss the goal, particularly when it impacts or is impacted by another area;\nCompanies that frequently change priorities may find the method useful, since it can give you the focus;\nYou can assign an owner to the OKR and be the primary person to discuss it."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#connect-the-intuit-story",
    "href": "posts/0018-measure-what-matters/index.html#connect-the-intuit-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.09.Connect: The Intuit Story",
    "text": "1.09.Connect: The Intuit Story\nWhat I learned:\n\nYou will have a great tool to connect your entire organization if you automate your OKR;\nIf a prioritization is required, raise the importance of the relevant OKR to demonstrate your seriousness while also ensuring that the methodology is accurate."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#superpower3-track-for-accountability",
    "href": "posts/0018-measure-what-matters/index.html#superpower3-track-for-accountability",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.10.Superpower#3: Track for Accountability",
    "text": "1.10.Superpower#3: Track for Accountability\n\n\nWilliam Edwards Deming\n\nIn God we trust. All others must bring data.\n\n\n\nWhat I learned:\n\nOKR are living organisms that can be born, changed, adapted, stopped, and died. Even though they allow for this flexibility, understanding why each action is taken is critical.\nIt is fundamental to regularly review your OKRs and have a plan in place in case some of them fail;\nTo gain a sense of progress and a better understanding of your overall objective reality, you can assign partial achievement to your KRs.\nBecause metrics cannot show the entire story, it is critical to self-assess your progress with the context. Here are some questions to help:\n\nWhat factors contributed to my success?\nWhat challenges did I face if I failed?\nWhat would I change if I could rewrite a completed goal?\nWhat did I learn that will change my approach to OKRs in the next cycle?"
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#track-the-gates-foundation-story",
    "href": "posts/0018-measure-what-matters/index.html#track-the-gates-foundation-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.11.Track: The Gates Foundation Story",
    "text": "1.11.Track: The Gates Foundation Story\nWhat I learned:\n\nOKR can assist you in making a decision by providing a path, but it also carries a higher risk because it emphasizes the importance of setting good goals;\nAvoid conflating goals and missions; an overly greedy OKR may lose credibility.\nSetting lofty goals is easy, but dismembering them is more difficult."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#superpower4-stretch-for-amazing",
    "href": "posts/0018-measure-what-matters/index.html#superpower4-stretch-for-amazing",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.12.Superpower#4: Stretch for Amazing",
    "text": "1.12.Superpower#4: Stretch for Amazing\n\n\nMellody Hudson\n\nThe biggest risk of all is not taking one.\n\n\n\nWhat I learned:\n\nConservative goals stifle innovation, while non-conservative goals accelerate it.\nYou can categorize your objectives, such as the ambitious ones, but defining how many objectives remain in each category is a crucial decision and need to be based in your business, market and culture."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#stretch-the-google-chrome-story",
    "href": "posts/0018-measure-what-matters/index.html#stretch-the-google-chrome-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.13.Stretch: The Google Chrome Story",
    "text": "1.13.Stretch: The Google Chrome Story\nWhat I learned:\n\nEven if you fail, a crazy ambitious goal will teach you something;\nAs a leader, you must challenge your team while not making the goal appear impossible."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#stretch-the-youtube-story",
    "href": "posts/0018-measure-what-matters/index.html#stretch-the-youtube-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "1.14.Stretch: The Youtube Story",
    "text": "1.14.Stretch: The Youtube Story\nWhat I learned:\n\nWhen setting a large and/or long goal, it is also important to set markers along the way to see if you are on the right track."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#continuous-performance-management-okrs-and-cfrs",
    "href": "posts/0018-measure-what-matters/index.html#continuous-performance-management-okrs-and-cfrs",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.01.Continuous Performance Management: OKR’s and CFR’s",
    "text": "2.01.Continuous Performance Management: OKR’s and CFR’s\n\n\nSheryl Sandberg\n\nTalking can transform minds, which can transform behaviors, which can transform institutions.\n\n\n\nWhat I learned:\n\nNumbers are wonderful, but they can easily fail when used to measure people;\nCFR stands for “Conversations, Feedback, and Recognition” and refers to a one-on-one conversation between employees and their managers;\nSeparate OKR and performance evaluation because they have and require different rituals;\nConversations, the leader to:\n\npromote discussion, define and remember goals, and reflect on them;\ntalk about performance and must be updated on a regular basis;\nupdate and discuss the development of one’s career.\n\nFeedback:\n\nit must be incorporated into culture and progress;\nit is necessary to be specific as well as constructive, rather than focusing solely on negative or positive aspects.\n\nRecognition:\n\nestablish recognition among colleagues as part of the culture, and not just for the leader. Make it frequent and friendly to accomplish this;\nif at all possible, connect it to the company’s goals."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#ditching-annual-performance-reviews-the-adobe-story",
    "href": "posts/0018-measure-what-matters/index.html#ditching-annual-performance-reviews-the-adobe-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.02.Ditching Annual Performance Reviews: The Adobe Story",
    "text": "2.02.Ditching Annual Performance Reviews: The Adobe Story\nWhat I learned:\n\nAnnual evaluations can take too long and leave it too late to act;\nTo transition from traditional evaluation, leaders must take on HR responsibilities, but they must be trained to do so, so HR has evolved into a team that prepares leaders rather than hands-on."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#baking-better-every-day-the-zume-pizza-story",
    "href": "posts/0018-measure-what-matters/index.html#baking-better-every-day-the-zume-pizza-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.03.Baking Better Every Day: The Zume Pizza Story",
    "text": "2.03.Baking Better Every Day: The Zume Pizza Story\nWhat I learned:\n\nOKR can provide assistance in areas where project methodologies and management tools cannot;\nOKR can help a company stay on track with its goals.\nOKR is unlikely to work unless the highest levels of the organization invest in them.\nThe most focused leaders are the best leaders."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#culture",
    "href": "posts/0018-measure-what-matters/index.html#culture",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.04.Culture",
    "text": "2.04.Culture\nWhat I learned:\n\nCulture is difficult to change, and while it is related to goals, they are not the same;\nA company’s culture is what moves and signifies its work; a bad culture can stymie any methodology’s ability to work."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#culture-change-the-lumeris-story",
    "href": "posts/0018-measure-what-matters/index.html#culture-change-the-lumeris-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.05.Culture Change: The Lumeris Story",
    "text": "2.05.Culture Change: The Lumeris Story\nWhat I learned:\n\nA culture change may be required before implementing a major process change (e.g., OKR);\nOKR require action; simply collecting data and analyzing metrics will not result in any changes;\nBig changes do not happen overnight."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#culture-change-bonos-one-campaing-story",
    "href": "posts/0018-measure-what-matters/index.html#culture-change-bonos-one-campaing-story",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.06.Culture Change: Bono’s ONE Campaing Story",
    "text": "2.06.Culture Change: Bono’s ONE Campaing Story\nWhat I learned:\n\nAlthough culture is required for OKR to work, it can also change the culture once implemented;\nYou must listen to and understand your client;\nTake care not to let the OKR suffocate you and prevent innovation."
  },
  {
    "objectID": "posts/0018-measure-what-matters/index.html#the-goals-to-come",
    "href": "posts/0018-measure-what-matters/index.html#the-goals-to-come",
    "title": "Unveiling the pages: Measure what matters",
    "section": "2.07.The Goals to Come",
    "text": "2.07.The Goals to Come\n\n\nMuhammad Ali\n\nWhat keeps me going is goals.\n\n\n\nWhat I learned:\n\nAlthough the concept of OKR is simple, successfully implementing it requires a significant amount of effort and involvement, and when done correctly, can provide substantial returns."
  },
  {
    "objectID": "posts/0016-models-and-logs/index.html",
    "href": "posts/0016-models-and-logs/index.html",
    "title": "An intro to: Linear and log models",
    "section": "",
    "text": "In this post, we will see how logs aren’t just for lumberjacks, but also for models."
  },
  {
    "objectID": "posts/0016-models-and-logs/index.html#example",
    "href": "posts/0016-models-and-logs/index.html#example",
    "title": "An intro to: Linear and log models",
    "section": "Example",
    "text": "Example\nSo, for example, if we adjust a linear model of human weight and height, first let’s see how they behave.\n\n\n\n\n\nBecause we see a linear relationship between the variables, a linear model is not absurd; after adjusting a linear model, we get:\n\\[\n\\mathrm{weight} = -47.6283 + 0.6995*\\mathrm{height},\n\\tag{4}\\]\nThat is, for every centimeter a person grows taller, they are estimated to be 0.6995 kg heavier. As a result, a one-unit increase in height results in a 0.6995-unit increase in weight.\nHowever, there are times when the relationship between our variables is not linear or additive. Given its properties, the logarithmic scale can be very useful, for more details check my post An intro to: Logarithmic Scale."
  },
  {
    "objectID": "posts/0016-models-and-logs/index.html#example-1",
    "href": "posts/0016-models-and-logs/index.html#example-1",
    "title": "An intro to: Linear and log models",
    "section": "Example",
    "text": "Example\nTo understand in an example, we will use a dataset (Seheult et al. 1989) with the average brain and body weights for 28 species of land animals.\nFirst, we will do a scatter plot of the two variables.\n\n\n\n\n\nThe relationship between the two variables is difficult to discern, as shown in the figure above, because some of the animals are outliers in terms of brain and body weight. As a result, we can “compress” this difference using the logarithm.\n\n\n\n\n\nAfter applying the logarithm, we can see in the log-log scale that the relationship between the animals’ body and brain weight is linear. As a result, we will model them using a linear model, and for the sake of the example, we will use the brain as the response variable.\n\\[\n\\log{(\\mathrm{brain})} = 2.555 + 0.496*\\log{(\\mathrm{body})}.\n\\tag{20}\\]\nSo for every 1% increase in body weight, the brain weight increases by 0.496%."
  },
  {
    "objectID": "posts/0014-simpson-paradox/index.html",
    "href": "posts/0014-simpson-paradox/index.html",
    "title": "An intro to: Simpson’s paradox",
    "section": "",
    "text": "In this post, we will see how a third party can show us the truth about a relationship.\n\nContext\nSimpson’s Paradox is a statistical phenomenon that occurs when an observed correlation between two variables in separate groups of data is reversed when compared to the overall correlation without taking the group into account.\nWhen analyzing data, this phenomenon, named after statistician Edward Simpson, can lead to incorrect conclusions. Despite Simpson’s discovery in 1951, the concept had previously been noted by other researchers.\n\n\nExample\nAssume we have two numerical variables.\n\n\n\n\n\nAs shown in the figure above, they have a moderate negative linear relationship with a pearson correlation coefficient of -0.589.\n\n\n\n\n\nNow, we look at the data with a third categorical variable in mind, and we see that the correlation is positive for each level of this variable for each subgroup of data.\n\n\n\n\n\n\n\nConsiderations\nThis paradox highlights the importance of understanding biases and data selection in research, warning against drawing conclusions solely from observational data, particularly when dealing with complex variables. A lurking or hidden variable (confounder) is frequently to blame for the paradox. Be aware that this confounder has the potential to distort the apparent relationship between variables, resulting in counterintuitive results.\nTo ensure accurate interpretations of variable relationships, researchers must be cautious, taking into account data complexities and alternative explanations. Incorporate domain expertise as well to identify potential confounders or factors that may contribute to the paradox. Unexpected outcomes can be explained with a thorough understanding of the subject."
  },
  {
    "objectID": "posts/0012-mean/index.html",
    "href": "posts/0012-mean/index.html",
    "title": "An intro to: Mean",
    "section": "",
    "text": "In this post, we will navigate the Land of the Averages."
  },
  {
    "objectID": "posts/0012-mean/index.html#the-simple",
    "href": "posts/0012-mean/index.html#the-simple",
    "title": "An intro to: Mean",
    "section": "The simple",
    "text": "The simple\nThe simple arithmetic mean, is given by: \\[\n\\frac{1}{n}\\sum_\\limits{i=1}^{n} x_i,\n\\tag{1}\\]\nwhere:\n\n\\(x_i\\) is a numeric vector of length \\(n\\).\n\nBecause the arithmetic mean is simple to calculate and understand, it is accessible to a wide range of audiences, since it just entails the fundamental arithmetic operations of addition and division.\nPeople frequently employ the concept, even if they do not formally comprehend it. In my classes, I used to ask how long it usually takes you to get to work. Then someone said they’d take 30 minutes, for example, and I asked if that meant every day would be exactly 30 minutes, and my students said no, that some times would be 30, 31 or 29 minutes, so intuitively they’d do the simple arithmetic mean.\n\n\n [1] 1 2 2 3 3 3 4 5 5 5 7\n\n\n\n\n\narithmetic\n\n\n\n\n3.636364\n\n\n\n\n\n\n\nSo the simple arithmetic mean is 3.636364, now let’s see how outliers impact.\n\nEx. 1: Small but savage\nIn this example, we change the first value to a smaller fractional value.\n\n\n [1] 0.1 2.0 2.0 3.0 3.0 3.0 4.0 5.0 5.0 5.0 7.0\n\n\n\n\n\narithmetic\n\n\n\n\n3.554546\n\n\n\n\n\n\n\nWe can see that the mean has shifted slightly to a smaller value.\n\n\nEx. 2: When numbers go big\nThe arithmetic mean can be significantly influenced by extreme values. A single value that is unusually high or low can skew the result, resulting in an inaccurate representation of the central tendency. Extreme values in small samples can have a greater impact on the mean than extreme values in large samples.\nNow we change the last value to a larger value.\n\n\n [1]   1   2   2   3   3   3   4   5   5   5 100\n\n\n\n\n\narithmetic\n\n\n\n\n12.09091\n\n\n\n\n\n\n\nAs we can see, the mean increases significantly, resulting in a distorted and unrepresentative metric of the data.\n\n\nEx. 3: May not reflect true center\nIn skewed distributions, the mean may not accurately represent the typical value experienced by the majority of data points. The mean can be pushed towards the distribution’s tail.\nLet’s take a look at an example from a dataset from a exponential distribution.\n\n\n\n\n\nAs we can see, the mean is dragged far away from the peak density value by the larger values.\n\n\nEx. 4: Zero gravity, when means gets lost in space\nNext we apply the mean to data from a normal distribution centered around zero.\n\n\n\n\n\nBecause the arithmetic mean uses sum as the base for calculation, a simmetric distribution around negative and positive values will produce a mean close to or equal to zero, which can be misleading, especially when dealing with an error variable, because the mean can be interpreted as having no error at all, which is why the absolute function is commonly used in this scenario.\n\n\nEx. 5: A tale of average and variance\nLet’s run the normal distribution simulation again, but with different variances this time.\n\n\n\n\n\nWe can see that even though the distribution for each data set is very different, we would be in trouble if we only based our decision on the mean."
  },
  {
    "objectID": "posts/0012-mean/index.html#the-weighted",
    "href": "posts/0012-mean/index.html#the-weighted",
    "title": "An intro to: Mean",
    "section": "The weighted",
    "text": "The weighted\nThe weighted arithmetic mean is a variant that considers not only the values in a dataset but also assigns different weights to each value based on its importance or significance.\nIn other words, rather than treating all values equally, the weighted mean favors some over others based on predetermined weights.\n\\[\n\\frac{1}{\\sum_\\limits{i=1}^{n}w_i}\\sum_\\limits{i=1}^{n} w_ix_i,\n\\tag{2}\\]\nwhere:\n\n\\(x_i\\) is a numeric vector of length \\(n\\);\n\\(w_i\\) is a numeric vector of length \\(n\\), with the respectives weights for the values of \\(x_i\\).\n\nA practical application is in academic grading, the weighted mean of exam scores might be used, where different exams carry different weights based on their importance.\nA common case is when proportion (\\(p_i\\)) are used as weigths, and since:\n\\[\n\\sum_\\limits{i=1}^{n} p_i = 1.\n\\tag{3}\\]\nWhen applying Equation 3 to Equation 2 we have that:\n\\[\n\\sum_\\limits{i=1}^{n} w_ix_i.\n\\tag{4}\\] Even if it is an interesting application, assigning weights to data points is frequently subjective and can be influenced by personal judgment or assumptions. The weighted mean’s accuracy is heavily dependent on the appropriateness of the weights chosen.\nIn addition, calculating the weighted mean requires an extra step when compared to the simple arithmetic mean, which may complicate the analysis and calculations, particularly when dealing with large datasets. In some cases, the rationale for assigning specific weights may not be transparent or well-documented, which can make replicating or validating the analysis difficult."
  },
  {
    "objectID": "posts/0012-mean/index.html#the-trimmed",
    "href": "posts/0012-mean/index.html#the-trimmed",
    "title": "An intro to: Mean",
    "section": "The trimmed",
    "text": "The trimmed\nA trimmed arithmetic mean is a statistical measure that computes the mean of a dataset by excluding a percentage of the lowest and highest values, reducing the impact of outliers and extreme values.\nThe amount of trimming can be adjusted to strike a balance between retaining meaningful data and reducing the impact of outliers.\nLet’s go back to our previous outlier example, but now applying a trim of 10% in both ends of the data.\n\n\n [1]   1   2   2   3   3   3   4   5   5   5 100\n\n\n\n\n\ntrimmed\n\n\n\n\n3.555556\n\n\n\n\n\n\n\nThat is the same as applying a simple arithmetic mean to:\n\n\n[1] 2 2 3 3 3 4 5 5 5\n\n\n\n\n\narithmetic\n\n\n\n\n3.555556\n\n\n\n\n\n\n\nAs the data is trimmed, we achieve a more representative metric for our data, but this is due to an intentional loss of information, which may result in an incomplete representation of the data’s full range. Important insights or trends within the data may be overlooked depending on the extent of trimming."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-1-small-but-savage-1",
    "href": "posts/0012-mean/index.html#ex.-1-small-but-savage-1",
    "title": "An intro to: Mean",
    "section": "Ex. 1: Small but savage",
    "text": "Ex. 1: Small but savage\nThe geometric mean is sensitive to small values in the dataset. This sensitivity can be advantageous when you want to emphasize the impact of small values or identify trends that might be overshadowed by larger values.\n\n\n [1] 0.1 2.0 2.0 3.0 3.0 3.0 4.0 5.0 5.0 5.0 7.0\n\n\n\n\n\narithmetic\ngeometric\n\n\n\n\n3.554546\n2.606968\n\n\n\n\n\n\n\nAs we can see, the presence of the smaller value severely “penalizes” the geometric value."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-2-when-numbers-go-big-1",
    "href": "posts/0012-mean/index.html#ex.-2-when-numbers-go-big-1",
    "title": "An intro to: Mean",
    "section": "Ex. 2: When numbers go big",
    "text": "Ex. 2: When numbers go big\nUnlike the arithmetic mean, the geometric mean is less affected by larger outliers. This is due to the fact that the geometric mean is equivalent to taking the arithmetic mean of the logarithms of the values. This property has the effect of compressing the data, making extreme values contribute less to the final result.\nLet’s go back to our outlier example.\n\n\n [1]   1   2   2   3   3   3   4   5   5   5 100\n\n\n\n\n\narithmetic\ngeometric\n\n\n\n\n12.09091\n4.092944\n\n\n\n\n\n\n\nAs we can see, the geometric mean has a significant less impact of the larger value."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-3-may-not-reflect-true-center-1",
    "href": "posts/0012-mean/index.html#ex.-3-may-not-reflect-true-center-1",
    "title": "An intro to: Mean",
    "section": "Ex. 3: May not reflect true center",
    "text": "Ex. 3: May not reflect true center\n\n\n\n\n\nIn this scenario, the geometric mean approaches the peak value of the density in the example because it is more resistant to larger values and more sensitive to smaller data."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-4-zero-gravity-when-means-gets-lost-in-space-1",
    "href": "posts/0012-mean/index.html#ex.-4-zero-gravity-when-means-gets-lost-in-space-1",
    "title": "An intro to: Mean",
    "section": "Ex. 4: Zero gravity, when means gets lost in space",
    "text": "Ex. 4: Zero gravity, when means gets lost in space\nWhen a zero value is present in a dataset, calculating the geometric mean becomes problematic because the value will always be zero, since it is the product of values.\nBut let’s take a look in our previous example where the data follows a normal distribution around zero.\n\n\n\n\n\nThe geometric mean results in value larger than zero, that is because it does not consider negative values, because a negative number raised to a non-integer exponent can produce complex results, the concept of a geometric mean for negative values is meaningless in the realm of real numbers."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-5-a-tale-of-average-and-variance-1",
    "href": "posts/0012-mean/index.html#ex.-5-a-tale-of-average-and-variance-1",
    "title": "An intro to: Mean",
    "section": "Ex. 5: A Tale of average and variance",
    "text": "Ex. 5: A Tale of average and variance\nLet’s run the normal distribution again, but this time with different variances.\n\n\n\n\n\nWe get a similar result, but in the higher variance distribution, the mean becomes more skewed toward the peak density."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-1-small-but-savage-2",
    "href": "posts/0012-mean/index.html#ex.-1-small-but-savage-2",
    "title": "An intro to: Mean",
    "section": "Ex. 1: Small but savage",
    "text": "Ex. 1: Small but savage\nSince the harmonic mean takes the inverse of the original value, it is even more sensitive to small values than the geometric mean, leading to extremely large results for values close to zero.\n\n\n [1] 0.1 2.0 2.0 3.0 3.0 3.0 4.0 5.0 5.0 5.0 7.0\n\n\n\n\n\narithmetic\ngeometric\nharmonic\n\n\n\n\n3.554546\n2.606968\n0.846619"
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-2-when-numbers-go-big-2",
    "href": "posts/0012-mean/index.html#ex.-2-when-numbers-go-big-2",
    "title": "An intro to: Mean",
    "section": "Ex. 2: When numbers go big",
    "text": "Ex. 2: When numbers go big\nNext, we see how it is impacted by a larger value.\n\n\n [1]   1   2   2   3   3   3   4   5   5   5 100\n\n\n\n\n\narithmetic\ngeometric\nharmonic\n\n\n\n\n12.09091\n4.092944\n2.849741\n\n\n\n\n\n\n\nAt the same time that it is most sensitive to small values, it is also the most robust to larger values."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-3-may-not-reflect-true-center-2",
    "href": "posts/0012-mean/index.html#ex.-3-may-not-reflect-true-center-2",
    "title": "An intro to: Mean",
    "section": "Ex. 3: May not reflect true center",
    "text": "Ex. 3: May not reflect true center\n\n\n\n\n\nBecause the harmonic mean is more robust to larger values and more sensitive to smaller data, it provides a lower value in the example above than the other methods."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-4-zero-gravity-when-means-gets-lost-in-space-2",
    "href": "posts/0012-mean/index.html#ex.-4-zero-gravity-when-means-gets-lost-in-space-2",
    "title": "An intro to: Mean",
    "section": "Ex. 4: Zero gravity, when means gets lost in space",
    "text": "Ex. 4: Zero gravity, when means gets lost in space\nCalculating the harmonic mean when there is a zero value in a dataset becomes difficult because the value is undefined since it is the sum of the inverse of the values and there is no division by zero.\nBut let’s take a look in our previous example where the data follows a normal distribution around zero.\n\n\n\n\n\nBecause we are using a dataset with a small magnitude, the harmonic mean provides a value that is even further away from the center than the geometric mean."
  },
  {
    "objectID": "posts/0012-mean/index.html#ex.-5-a-tale-of-average-and-variance-2",
    "href": "posts/0012-mean/index.html#ex.-5-a-tale-of-average-and-variance-2",
    "title": "An intro to: Mean",
    "section": "Ex. 5: A tale of average and variance",
    "text": "Ex. 5: A tale of average and variance\nLet’s run the normal distribution again, but this time with different variances.\n\n\n\n\n\nThe harmonic mean achieves nearly the same result as the arithmetic mean, where the metrics are all centered around 50 when the variance is ignored."
  },
  {
    "objectID": "posts/0010-logistic-regression/index.html",
    "href": "posts/0010-logistic-regression/index.html",
    "title": "An intro to: Logistic Regression",
    "section": "",
    "text": "In this post, we will board the S.S. Logistic and learn about the Titanic passengers’ survival."
  },
  {
    "objectID": "posts/0010-logistic-regression/index.html#odds-ratio",
    "href": "posts/0010-logistic-regression/index.html#odds-ratio",
    "title": "An intro to: Logistic Regression",
    "section": "Odds ratio",
    "text": "Odds ratio\nTo better understand the practical implications of the coefficients, we can apply the exponential function to the coefficients to obtain the odds ratios.\n\n\nOdds ratio (OR)\n\nThe odds ratio indicates how much the outcome odds change for a one-unit increase in the predictor variable (for continuous predictors) or when moving from one category to another (for categorical predictors).\n\n\n\nHere is why, let’s say we have two logit’s (\\(p_1\\) and \\(p_2\\)), and we will subtract one from another, so:\n\\[\n\\begin{align}\n\\mathrm{logit}_{p_1} - \\mathrm{logit}_{p_2}\n&= \\log \\left( \\frac{p_1}{1-p_1}\\right) - \\log \\left( \\frac{p_2}{1-p_2}\\right)\\\\\n&= \\log \\left( \\frac{p_1}{1-p_1}\\middle/ \\frac{p_2}{1-p_2}\\right).\\\\\n\\end{align}\n\\tag{5}\\]\nSo the difference between two logit functions is the logarithm of an odds ratio, which means that we can get the OR by applying an exponential function to the coefficients of a logistic model.\n\n\n\n\n\nterm\nestimate\nOR\n\n\n\n\nageAdult\n-1.0615\n0.3459\n\n\nclass3rd\n-0.9201\n0.3985\n\n\nclass2nd\n-0.1604\n0.8518\n\n\nclass1st\n0.8577\n2.3577\n\n\nsexFemale\n2.4201\n11.2465\n\n\n\n\n\n\n\nNow that we have the OR of each level compared to their respective baseline (reference level), we can interpret them by looking at their magnitude, where if is:\n\nequal to 1, it means that the predictor variable has no effect on the outcome odds;\ngreater than 1, it indicates an increase in the odds of the outcome;\nless than 1, it indicates a decrease in the odds of the outcome.\n\nLet’s take the example of an Adult in the model, it has a 34,59% chance to survive in comparison to a child (reference level), that means that being an adult reduces the odds of the outcome by approximately 65.4%.\nSo we can easily measure the impact by doing OR-1; if negative, the odds of the outcome are reduced; if positive, the odds of the outcome are increased.\n\n\n\n\n\nterm\nestimate\nOR\nOR-1\n\n\n\n\nageAdult\n-1.0615\n0.3459\n-0.6541\n\n\nclass3rd\n-0.9201\n0.3985\n-0.6015\n\n\nclass2nd\n-0.1604\n0.8518\n-0.1482\n\n\nclass1st\n0.8577\n2.3577\n1.3577\n\n\nsexFemale\n2.4201\n11.2465\n10.2465\n\n\n\n\n\n\n\nAs previously stated, being an adult reduced the odds of the outcome by approximately 65.4% when compared to being a child.\nPassengers in the third class have their odds reduced by approximately 60.2% when compared to the crew, while passengers in the second class have their odds reduced by approximately 14.8%. Being in the first class, on the other hand, increases the odds of the outcome by approximately 135.8%.\nSex had the greatest impact, with being female increased the odds by approximately 1,024.7% when compared to being male."
  },
  {
    "objectID": "posts/0008-benford-law/index.html",
    "href": "posts/0008-benford-law/index.html",
    "title": "An intro to: Benford’s Law",
    "section": "",
    "text": "In this post you will learn how to fraud a fraud detection."
  },
  {
    "objectID": "posts/0008-benford-law/index.html#exponential-distribution",
    "href": "posts/0008-benford-law/index.html#exponential-distribution",
    "title": "An intro to: Benford’s Law",
    "section": "Exponential distribution",
    "text": "Exponential distribution\nFirst let’s simulate a set of 10,000 random numbers from a exponential distribution with a rate of 0.25.\n\n\n\n\n\nNext, we extract the first digit of each number and calculate the frequency of each one.\n\n\n\n\n\nSmaller digits are more common, as shown in the graph above, and as the digit grows larger, the frequency decreases. Let us now compare the actual result to the expected result.\n\n\n\n\n\nAs we can see, Benford’s Law and our data are very similar, but is this always the case?"
  },
  {
    "objectID": "posts/0008-benford-law/index.html#uniform-distribution",
    "href": "posts/0008-benford-law/index.html#uniform-distribution",
    "title": "An intro to: Benford’s Law",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nLet’s run a simulation of 10,000 random numbers drawn from a uniform distribution with a range of 1 to 100.\nThe simulated data is shown below:\n\n\n\n\n\nLet us now compute the frequency of the first digits.\n\n\n\n\n\nWe can see now that the law differs from the simulated data, but why? Because we are sampling from a set of numbers where the first digit pool is uniform."
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html",
    "href": "posts/0006-normal-distribution/index.html",
    "title": "An intro to: Normal Distribution",
    "section": "",
    "text": "In this post you will learn what bell, Gauss and simmetry have in common."
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#standard-normal",
    "href": "posts/0006-normal-distribution/index.html#standard-normal",
    "title": "An intro to: Normal Distribution",
    "section": "Standard normal",
    "text": "Standard normal\nThe simplest case of a normal distribution is a \\(\\mathcal{N}(0,1)\\), also called a standard normal distribution or Z-distribution:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}}\\mathcal{e}^{-\\frac{x^2}{2}}.\n\\tag{2}\\]"
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#simmetry",
    "href": "posts/0006-normal-distribution/index.html#simmetry",
    "title": "An intro to: Normal Distribution",
    "section": "Simmetry",
    "text": "Simmetry\nThe normal distribution has a balanced and mirror-like shape around its center, which is characterized by its symmetry and central tendency. Because of its symmetry, values on either side of the mean have equal probabilities, and the alignment of the mean, median, and mode at the center.\nHere an example of a standard normal distribution:"
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#bell-shape-curve",
    "href": "posts/0006-normal-distribution/index.html#bell-shape-curve",
    "title": "An intro to: Normal Distribution",
    "section": "Bell-shape curve",
    "text": "Bell-shape curve\nThe shape of the normal distribution is also characterized by gradually decreasing probabilities as the values move away equally from the mean in both directions.\nEven tough it has zero skewness the variance can imply in a kurtosis change, here a few example:"
  },
  {
    "objectID": "posts/0006-normal-distribution/index.html#chebyshevs-inequality",
    "href": "posts/0006-normal-distribution/index.html#chebyshevs-inequality",
    "title": "An intro to: Normal Distribution",
    "section": "Chebyshev’s inequality",
    "text": "Chebyshev’s inequality\nChebyshev’s inequality is a mathematical inequality that can be applied to any probability distribution with defined mean and variance. It gives us an bound on the likelihood that a random variable deviates from its mean by a certain amount.\n\\[\nP(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}, \\quad k &gt;0; \\quad k \\in \\mathbb{R},\n\\tag{3}\\]\nwhere:\n\n\\(X\\) is a random variable with variance \\(\\sigma^2\\) and expected value \\(\\mu\\);\n\\(\\sigma\\) is a finite non-zero standard deviation;\n\\(\\mu\\) is a finite expected value;\n\\(k\\) is a given real number greater then zero.\n\nWhen applied to the normal distribution we have that:\n\n\n\n\n\nApproximately 68% of values in a normal distribution are within one standard deviation (\\(\\sigma\\)) of the mean, 95% are within two standard deviations, and 99.7% are within three standard deviations."
  },
  {
    "objectID": "posts/0004-tidyverse-operators/index.html",
    "href": "posts/0004-tidyverse-operators/index.html",
    "title": "An intro to: Tidyverse Operators",
    "section": "",
    "text": "In this post you will learn that a walrus is not just a animal."
  },
  {
    "objectID": "posts/0004-tidyverse-operators/index.html#curly-curly",
    "href": "posts/0004-tidyverse-operators/index.html#curly-curly",
    "title": "An intro to: Tidyverse Operators",
    "section": "{{}} Curly-curly",
    "text": "{{}} Curly-curly\nThe first operator we will learn is the curly-curly, using the command {{}}, the goal of this operator is to allow us to have an argument passed to our function refering to a column inside a dataframe.\nSo, we will create the function penguin_summary, where the variable used to count the penguins, in the example before species, will be generalized By the argument grp_var.\n\npenguin_summary &lt;- function(grp_var){\n  penguins %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  group_by({{grp_var}},sex) %&gt;%\n  summarise(\n    n = n(),\n    mean_body_mass_g = mean(body_mass_g,na.rm = TRUE)\n    ) %&gt;% \n  group_by({{grp_var}}) %&gt;% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\nWe can see that inside the dplyr verbs we write the argument grp_var inside the operator {{}} in the verb group_by.\nLet’s now apply the variable species to see if the result is the same as before.\n\npenguin_summary(grp_var = species)\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n mean_body_mass_g     p\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73            3369. 0.5  \n2 Adelie    male      73            4043. 0.5  \n3 Chinstrap female    34            3527. 0.5  \n4 Chinstrap male      34            3939. 0.5  \n5 Gentoo    female    58            4680. 0.487\n6 Gentoo    male      61            5485. 0.513\n\n\nYes! We got the same result, but there is also another interesting fact, the variable species was passed without quotes, so no need to use functions such as quo, enquote, etc.\nAnd now we can pass other variable to our function, let’s give it a try.\n\npenguin_summary(grp_var = island)\n\n`summarise()` has grouped output by 'island'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   island [3]\n  island    sex        n mean_body_mass_g     p\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Biscoe    female    80            4319. 0.491\n2 Biscoe    male      83            5105. 0.509\n3 Dream     female    61            3446. 0.496\n4 Dream     male      62            3987. 0.504\n5 Torgersen female    24            3396. 0.511\n6 Torgersen male      23            4035. 0.489\n\n\nOk, after generalizing the species variable, we will do the same for the body_mass_g creating another argument, num_var.\n\npenguin_summary &lt;- function(grp_var,num_var){\n  penguins %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  group_by({{grp_var}},sex) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean({{num_var}},na.rm = TRUE)\n    ) %&gt;% \n  group_by({{grp_var}}) %&gt;% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\n\npenguin_summary(\n  grp_var = species,\n  num_var = body_mass_g\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n  mean     p\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73 3369. 0.5  \n2 Adelie    male      73 4043. 0.5  \n3 Chinstrap female    34 3527. 0.5  \n4 Chinstrap male      34 3939. 0.5  \n5 Gentoo    female    58 4680. 0.487\n6 Gentoo    male      61 5485. 0.513\n\n\nOkay, we kind of succeeded, but we had to give the new variable for the mean a generic name; to make this dynamic, we’ll need the assistance of another operator."
  },
  {
    "objectID": "posts/0004-tidyverse-operators/index.html#walrus",
    "href": "posts/0004-tidyverse-operators/index.html#walrus",
    "title": "An intro to: Tidyverse Operators",
    "section": ":= Walrus",
    "text": ":= Walrus\nThe second operator is the walrus, using the command :=, the goal of this operator is to allow us to create new variables using the argument dynamically in the name of the variable created.\n\npenguin_summary &lt;- function(grp_var,num_var){\n  penguins %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  group_by({{grp_var}},sex) %&gt;%\n  summarise(\n    n = n(),\n    \"mean_{{num_var}}\" := mean({{num_var}},na.rm = TRUE)\n    ) %&gt;% \n  group_by({{grp_var}}) %&gt;% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\n\npenguin_summary(\n  grp_var = species,\n  num_var = body_mass_g\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n mean_body_mass_g     p\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73            3369. 0.5  \n2 Adelie    male      73            4043. 0.5  \n3 Chinstrap female    34            3527. 0.5  \n4 Chinstrap male      34            3939. 0.5  \n5 Gentoo    female    58            4680. 0.487\n6 Gentoo    male      61            5485. 0.513\n\n\nThe walrus operator substitute the = operator, and we can use the argument num_var inside the {{}} operator to generalize our variable name, not only that, but we can also set other characters such as a prefix or suffix.\nNow that we’ve finished our function, what if we want to make it even more generalized? For example, our dataframe and the variable sex are still inside the function, that is easy we just need create two more arguments:\n\npenguin_summary &lt;- function(df = penguins,main_var = sex,grp_var,num_var){\n  df %&gt;% \n  filter(!is.na({{main_var}})) %&gt;% \n  group_by({{grp_var}},{{main_var}}) %&gt;%\n  summarise(\n    n = n(),\n    \"mean_{{num_var}}\" := mean({{num_var}},na.rm = TRUE)\n    ) %&gt;% \n  group_by({{grp_var}}) %&gt;% \n  mutate(p = n/sum(n,na.rm = TRUE))\n}\n\n\npenguin_summary(\n  grp_var = species,\n  num_var = body_mass_g\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n mean_body_mass_g     p\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73            3369. 0.5  \n2 Adelie    male      73            4043. 0.5  \n3 Chinstrap female    34            3527. 0.5  \n4 Chinstrap male      34            3939. 0.5  \n5 Gentoo    female    58            4680. 0.487\n6 Gentoo    male      61            5485. 0.513\n\n\nSo we created an argument called df to be our data.frame, without any operator since it is been called “directly”, and already left the penguins dataset as the default. We did the same with the sex variable with the argument main_var.\nAnd even though we created a function called penguin_summary now we can apply it to another dataframe:\n\npenguin_summary(\n  df = mtcars,\n  main_var = vs,\n  grp_var = cyl,\n  num_var = drat\n  )\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 5 x 5\n# Groups:   cyl [3]\n    cyl    vs     n mean_drat      p\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1     4     0     1      4.43 0.0909\n2     4     1    10      4.04 0.909 \n3     6     0     3      3.81 0.429 \n4     6     1     4      3.42 0.571 \n5     8     0    14      3.23 1     \n\n\nOk, now we got a function that is completely generalized, with only arguments inside of it, but there is still way to make an even more powerful function, let’s say we want to apply our function to two numerical variables.\n\npenguin_summary(\n  grp_var = species,\n  num_var = c(body_mass_g,bill_depth_mm)\n  )\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 x 5\n# Groups:   species [3]\n  species   sex        n `mean_c(body_mass_g, bill_depth_mm)`     p\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;                                &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    female    73                                1693. 0.5  \n2 Adelie    male      73                                2031. 0.5  \n3 Chinstrap female    34                                1772. 0.5  \n4 Chinstrap male      34                                1979. 0.5  \n5 Gentoo    female    58                                2347. 0.487\n6 Gentoo    male      61                                2750. 0.513\n\n\nSo it is not what we expected, right? To pass multiple variables into a single argument, we will need the help of an old friend."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html",
    "href": "posts/0002-dplyr-across/index.html",
    "title": "An intro to: dplyr::across",
    "section": "",
    "text": "In this post you will learn to never repeat a function again inside a dplyr pipeline."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#before-across",
    "href": "posts/0002-dplyr-across/index.html#before-across",
    "title": "An intro to: dplyr::across",
    "section": "Before across",
    "text": "Before across\nAs one of thre greatest R packages, dplyr possesses a lot functions, but it has two main verbs to manipulate data, they are:\n\nsummarise: allows us to apply a transformation to data that reduce the number of observations, e.g., mean;\nmutate: allows us to apply a transformation to our existing variables or even creating new ones with the same size, e.g., multiplying one variable by 2.\n\nLet’s see how they work in practice:\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male~\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nWe will summarize every numerical variable using the dataset penguins from the palmerpenguins package, computing the mean for each. The mean function can then be applied to each variable inside the verb summarize.\n\npenguins %&gt;% \n  summarise(\n    mean(bill_length_mm,na.rm = TRUE),\n    mean(bill_depth_mm,na.rm = TRUE),\n    mean(flipper_length_mm,na.rm = TRUE),\n    mean(body_mass_g,na.rm = TRUE)\n  ) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 4\n$ `mean(bill_length_mm, na.rm = TRUE)`    &lt;dbl&gt; 43.92193\n$ `mean(bill_depth_mm, na.rm = TRUE)`     &lt;dbl&gt; 17.15117\n$ `mean(flipper_length_mm, na.rm = TRUE)` &lt;dbl&gt; 200.9152\n$ `mean(body_mass_g, na.rm = TRUE)`       &lt;dbl&gt; 4201.754\n\n\nIn the example above we see that it works, but have some problems:\n\nDue to its manual nature and increased risk of human error from writing numerous lines of code or even copying and pasting it, it would become a tiresome task if there were many columns;\nThe function will be given to the new tvariables as their names if their names are not set.\n\nA smarter approach is the use of a summarise variant, called summarise_if.\n\npenguins %&gt;% \n  summarise_if(\n    .predicate = is.numeric,\n    .funs = ~mean(.,na.rm = TRUE)\n  ) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 5\n$ bill_length_mm    &lt;dbl&gt; 43.92193\n$ bill_depth_mm     &lt;dbl&gt; 17.15117\n$ flipper_length_mm &lt;dbl&gt; 200.9152\n$ body_mass_g       &lt;dbl&gt; 4201.754\n$ year              &lt;dbl&gt; 2008.029\n\n\nIn the example above we see that inside summarise_if we define two argumens:\n\n.predicate: the condition to check which variables we are going to apply the functions;\n.funs: a function or list of functions.\n\nEven though these variables are the means of the originals, unlike the first method, the function here kept the names of the original variables. The fact that we can now apply a function to 5 columns with only 2 lines of code is another advantage.\nSo it was successful, but what is the issue? What if I also wanted to learn the mode of the variable species? How could we go about doing that?\n\npenguins %&gt;% \n  summarise(\n    species = relper::calc_mode(species),\n    across(.cols = where(is.numeric),.fns = ~mean(.,na.rm = TRUE))\n  ) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 6\n$ species           &lt;fct&gt; Adelie\n$ bill_length_mm    &lt;dbl&gt; 43.92193\n$ bill_depth_mm     &lt;dbl&gt; 17.15117\n$ flipper_length_mm &lt;dbl&gt; 200.9152\n$ body_mass_g       &lt;dbl&gt; 4201.754\n$ year              &lt;dbl&gt; 2008.029\n\n\nIn the example above we apply across , we see that it is used inside the conventional verb summarise , meaning we can still apply other functions even using across.\nSo across is a function that is complementary to mutate and summarise, that allows us to apply multiples functions across multiples variables.\nJust as curiosity, even though this old functions are superseeded they still exists, and their suffixes are _at() , _if() and _all() ."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#cols",
    "href": "posts/0002-dplyr-across/index.html#cols",
    "title": "An intro to: dplyr::across",
    "section": ".cols",
    "text": ".cols\nThe first argument of across determine which columns of the data.frame we are going to apply our functions, this argument is:\n\nNon-optional\nThe default is every single variable of the data.frame, by using the function everything.\nAccepts as input:\n\nIntegers, referencing the variables positions;\nStrings, referencing the variables names;\nSelect helpers functions, e.g., contains, as we will see below.\n\n\n\nDefault\n\npenguins %&gt;% \n  summarise(across(.fns = as.character)) %&gt;% \n  glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A~\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", ~\n$ bill_length_mm    &lt;chr&gt; \"39.1\", \"39.5\", \"40.3\", NA, \"36.7\", \"39.3\", \"38.9\", ~\n$ bill_depth_mm     &lt;chr&gt; \"18.7\", \"17.4\", \"18\", NA, \"19.3\", \"20.6\", \"17.8\", \"1~\n$ flipper_length_mm &lt;chr&gt; \"181\", \"186\", \"195\", NA, \"193\", \"190\", \"181\", \"195\",~\n$ body_mass_g       &lt;chr&gt; \"3750\", \"3800\", \"3250\", NA, \"3450\", \"3650\", \"3625\", ~\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f~\n$ year              &lt;chr&gt; \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"200~\n\n\nIn the example above we apply the function as.character to every column, since we did not use an input to the argument .cols.\n\npenguins %&gt;% \n  summarise(across(.cols = everything(),.fns = as.character)) %&gt;% \n  glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A~\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", ~\n$ bill_length_mm    &lt;chr&gt; \"39.1\", \"39.5\", \"40.3\", NA, \"36.7\", \"39.3\", \"38.9\", ~\n$ bill_depth_mm     &lt;chr&gt; \"18.7\", \"17.4\", \"18\", NA, \"19.3\", \"20.6\", \"17.8\", \"1~\n$ flipper_length_mm &lt;chr&gt; \"181\", \"186\", \"195\", NA, \"193\", \"190\", \"181\", \"195\",~\n$ body_mass_g       &lt;chr&gt; \"3750\", \"3800\", \"3250\", NA, \"3450\", \"3650\", \"3625\", ~\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f~\n$ year              &lt;chr&gt; \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"2007\", \"200~\n\n\nIn the example above we see that same result is obtained from the previous, since the default of .cols is everything.\n\n\nBy type\nIf we want to select variables by their type we can use the function where + a function that check the variable type.\n\npenguins %&gt;% \n  mutate(across(.cols = where(is.factor),.fns = toupper)) %&gt;% \n  glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"A~\n$ island            &lt;chr&gt; \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", ~\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F~\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nIn the example above we made all factor variables to be uppercase.\nOther functions can also be used, such as:\n\nis.numeric: check if the variable is numeric;\n\nis.integer check if the variable is an integer;\nis.double check if the variable is a double;\n\nis.factor check if the variable is a factor;\nis.character check if the variable is a character;\nis.logical check if the variable is a boolean (TRUE/FALSE).\n\nWe can also combine more than one function in the same across:\n\npenguins %&gt;% \n  mutate(across(.cols = where(is.factor) | where(is.character),.fns = toupper)) %&gt;%   glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;chr&gt; \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"ADELIE\", \"A~\n$ island            &lt;chr&gt; \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", \"TORGERSEN\", ~\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               &lt;chr&gt; \"MALE\", \"FEMALE\", \"FEMALE\", NA, \"FEMALE\", \"MALE\", \"F~\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nIn the example above we made all factor (species and island) and character (sex) variables to be uppercase.\n\n\nBy name\nAnother method of column selection is by using their name.\n\npenguins %&gt;% \n  summarise(across(.cols = ends_with(\"_mm\"),.fns = ~mean(.,na.rm = TRUE))) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 3\n$ bill_length_mm    &lt;dbl&gt; 43.92193\n$ bill_depth_mm     &lt;dbl&gt; 17.15117\n$ flipper_length_mm &lt;dbl&gt; 200.9152\n\n\nIn the example above we compute the mean for the variables that ends with the pattern _mm.\nSo all the the selection helpers can be used:\n\nall_of: allows us to pass a string vector to select specific variables, that helps when we are looking for a group of variables, which not obey a simples check condition such as been of the same type or having the a name pattern, e.g., all_of(vector_of_variables) ;\nany_of: is a similar function to all_of , but it can be used to remove variables with the operator -, e.g., any_of(-vector_of_variables) ;\ncontains: allows to select variables that contains a specific string in their names. e.g., contains(length);\nends_with: variables that ends with a specific string pattern, e.g., ends_with(\"_mm\");\neverything: all variables, and already the default of the argument .cols;\nlast_col: the last variable of the data.frame;\nmatches: variables with a name that matches a given regular expression;\nnum_range: variables that have a numeric sequence in their name, e.g., var1, var2 and var3 then we can use num_range(\"var\",1:3);\nstarts_with: variables that starts with a specific string pattern, e.g., starts_with(\"bill_\").\n\n\n\nBy order\nAnother method of column selection is using the name of the variables and the operator : to apply the function to a sequence of variables.\n\npenguins %&gt;% \n  summarise(across(.cols = bill_length_mm:body_mass_g,.fns = ~mean(.,na.rm = TRUE))) %&gt;%\n  glimpse()\n\nRows: 1\nColumns: 4\n$ bill_length_mm    &lt;dbl&gt; 43.92193\n$ bill_depth_mm     &lt;dbl&gt; 17.15117\n$ flipper_length_mm &lt;dbl&gt; 200.9152\n$ body_mass_g       &lt;dbl&gt; 4201.754\n\n\nIn the example above we compute the mean to every variable from bill_length_mm to body_mass_g.\nWe can see that this variables are third to sixth of the data.frame, then can also use a method to reference them by their position.\n\npenguins %&gt;% \n  summarise(across(.cols = 3:6,.fns = ~mean(.,na.rm = TRUE))) %&gt;%\n  glimpse()\n\nRows: 1\nColumns: 4\n$ bill_length_mm    &lt;dbl&gt; 43.92193\n$ bill_depth_mm     &lt;dbl&gt; 17.15117\n$ flipper_length_mm &lt;dbl&gt; 200.9152\n$ body_mass_g       &lt;dbl&gt; 4201.754\n\n\nIn the example above we computed the mean for the same variables as before, but now using their column position instead."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#fns",
    "href": "posts/0002-dplyr-across/index.html#fns",
    "title": "An intro to: dplyr::across",
    "section": ".fns",
    "text": ".fns\nThe argument .fns determine which functions are going to be applied, this argument is:\n\nNon-optional\nNo default\nAccepts as input:\n\nSingle function;\nList of functions.\n\n\n\npenguins %&gt;% \n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = list(\n        ~mean(.,na.rm = TRUE),\n        ~median(.,na.rm = TRUE)\n      )\n    )\n  ) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 10\n$ bill_length_mm_1    &lt;dbl&gt; 43.92193\n$ bill_length_mm_2    &lt;dbl&gt; 44.45\n$ bill_depth_mm_1     &lt;dbl&gt; 17.15117\n$ bill_depth_mm_2     &lt;dbl&gt; 17.3\n$ flipper_length_mm_1 &lt;dbl&gt; 200.9152\n$ flipper_length_mm_2 &lt;dbl&gt; 197\n$ body_mass_g_1       &lt;dbl&gt; 4201.754\n$ body_mass_g_2       &lt;dbl&gt; 4050\n$ year_1              &lt;dbl&gt; 2008.029\n$ year_2              &lt;dbl&gt; 2008\n\n\nThe mean and median are computed in the aforementioned example, but since more than one function is applied to the same variable, a numerical suffix is added based on the order in which our functions were defined inside the list, making mean 1 and median 2. This can be confusing and lead to errors later on.\n\npenguins %&gt;% \n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = list(\n        mean = ~mean(.,na.rm = TRUE),\n        median = ~median(.,na.rm = TRUE)\n      )\n    )\n  ) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 10\n$ bill_length_mm_mean      &lt;dbl&gt; 43.92193\n$ bill_length_mm_median    &lt;dbl&gt; 44.45\n$ bill_depth_mm_mean       &lt;dbl&gt; 17.15117\n$ bill_depth_mm_median     &lt;dbl&gt; 17.3\n$ flipper_length_mm_mean   &lt;dbl&gt; 200.9152\n$ flipper_length_mm_median &lt;dbl&gt; 197\n$ body_mass_g_mean         &lt;dbl&gt; 4201.754\n$ body_mass_g_median       &lt;dbl&gt; 4050\n$ year_mean                &lt;dbl&gt; 2008.029\n$ year_median              &lt;dbl&gt; 2008\n\n\nSince we defined the names of the functions in the example above and added them automatically as suffixes, it is now clearer what we are doing."
  },
  {
    "objectID": "posts/0002-dplyr-across/index.html#names",
    "href": "posts/0002-dplyr-across/index.html#names",
    "title": "An intro to: dplyr::across",
    "section": ".names",
    "text": ".names\nThe argument .names determines the name of resultant the variables after the functions are applied, so it allows us to change the names of the variables, this argument is:\n\nOptional\nThe default is NULL\nAccepts as input:\n\nA string, where we can use {.col} and {.fn} as variables to receive the respective names of the columns and/or functions.\n\n\n\npenguins %&gt;% \n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = list(\n        mean = ~mean(.,na.rm = TRUE),\n        median = ~median(.,na.rm = TRUE)\n      ),\n      .names = \"{.fn}----{.col}\"\n    )\n  ) %&gt;% \n  glimpse()\n\nRows: 1\nColumns: 10\n$ `mean----bill_length_mm`      &lt;dbl&gt; 43.92193\n$ `median----bill_length_mm`    &lt;dbl&gt; 44.45\n$ `mean----bill_depth_mm`       &lt;dbl&gt; 17.15117\n$ `median----bill_depth_mm`     &lt;dbl&gt; 17.3\n$ `mean----flipper_length_mm`   &lt;dbl&gt; 200.9152\n$ `median----flipper_length_mm` &lt;dbl&gt; 197\n$ `mean----body_mass_g`         &lt;dbl&gt; 4201.754\n$ `median----body_mass_g`       &lt;dbl&gt; 4050\n$ `mean----year`                &lt;dbl&gt; 2008.029\n$ `median----year`              &lt;dbl&gt; 2008\n\n\nIn the example above we change the variables names so they start with the function applied followed by 4 hyphens and then the original columns names."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my personal website!",
    "section": "",
    "text": "Some notes: Better Presentations\n\n\n\n\n\n\nSome notes\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nGetting proof: Bhaskara formula\n\n\n\n\n\n\nGetting Proof\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the pages: Measure what matters\n\n\n\n\n\n\nUnveiling the pages\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling the pages: I, Robot\n\n\n\n\n\n\nUnveiling the pages\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Linear and log models\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Combination and Permutation\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Berkson’s paradox\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Simpson’s paradox\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Mean\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Birthday Paradox\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Logistic Regression\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Monty Hall problem\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Benford’s Law\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn analysis of: The King James Bible\n\n\n\n\n\n\nAn analysis of\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Normal Distribution\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn analysis of: Settlers of Catan\n\n\n\n\n\n\nAn analysis of\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Tidyverse Operators\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Logarithmic Scale\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: dplyr::across\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\n  \n\n\n\n\nAn intro to: Chi-square Test\n\n\n\n\n\n\nIntro to\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nVinícius Félix\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "header-participations.html",
    "href": "header-participations.html",
    "title": "Participations",
    "section": "",
    "text": "Gestão orientada a dados - Podcast Na Ponta da Língua #8\n\n\n\n\n\n\n Pecuária orientada a dados - 2º Encontro de Consultores Zoo Jr.\n\n\n\n\n\n Confinamento de bovinos, interpretando as respostas nutricionais através de dados e informações - SEVAM 2021 (Semana de Extensão Veterinária da Anhembi Morumbi)\n\n\n\n\n\n A Era da Convergência: O Peso da Gestão Analítica - ECR 2020 (Encontro de Confinamento e de Recriadores)\n\n\n\n\n\n Ciência de dados e IA, como se preparar? - 1º Action Time\n Degustando ferramentas de BI - 5º PowerBI Maringá\n Gráficos: O Limiar entre uma mensagem e uma mentira - FrontIn Maringá\n Ciência de Dados: Um Alicerce para Pecuária de Precisão - TICNOVA 2019\n Validando seu produto: Evitando uma morte prematura com dados - IxDA Maringá #3\n A Pecuária de Precisão - Semana de Gestão de Confinamento 2019 (Zoo Jr. - UEM)\n Data-Driven Product Development ft. Luis Berns - FEMUG #23\n O cenário da tecnologia em números - 1º AfroTech MGA\n Estatística: Um Arsenal para Tomada de Decisão - Eureka Moment\n Dissecando Métricas Ágeis - Maringá Agile #4\n\n\n\n\n\n Músicas no Spotify: Como vivem? E quanto tempo sobrevivem? - Cerveja com Dados Maringá #1\n Data Science: Magia ou Ciência? - Semana do programador DB1\n Data Science: Magia ou Ciência? - GDG Maringá: Ciência de dados\n Trabalhando com Consultoria Estatística - VIII SEst UFSCar/USP\n\n\n\n\n\n\n\n\n Correlação Múltipla Wavelet - XXV EAIC e V EAIC Jr.\n The Concept of 4D Datasets - I Workshop em Bioestatística\n\n\n\n\n\n Tópicos em Variância Wavelet - XXIV EAIC e IV EAIC Jr.\n Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, através de métodos bootstrap - VI SEEMI"
  },
  {
    "objectID": "header-participations.html#section",
    "href": "header-participations.html#section",
    "title": "Participations",
    "section": "",
    "text": "Gestão orientada a dados - Podcast Na Ponta da Língua #8"
  },
  {
    "objectID": "header-participations.html#section-1",
    "href": "header-participations.html#section-1",
    "title": "Participations",
    "section": "",
    "text": "Pecuária orientada a dados - 2º Encontro de Consultores Zoo Jr."
  },
  {
    "objectID": "header-participations.html#section-2",
    "href": "header-participations.html#section-2",
    "title": "Participations",
    "section": "",
    "text": "Confinamento de bovinos, interpretando as respostas nutricionais através de dados e informações - SEVAM 2021 (Semana de Extensão Veterinária da Anhembi Morumbi)"
  },
  {
    "objectID": "header-participations.html#section-3",
    "href": "header-participations.html#section-3",
    "title": "Participations",
    "section": "",
    "text": "A Era da Convergência: O Peso da Gestão Analítica - ECR 2020 (Encontro de Confinamento e de Recriadores)"
  },
  {
    "objectID": "header-participations.html#section-4",
    "href": "header-participations.html#section-4",
    "title": "Participations",
    "section": "",
    "text": "Ciência de dados e IA, como se preparar? - 1º Action Time\n Degustando ferramentas de BI - 5º PowerBI Maringá\n Gráficos: O Limiar entre uma mensagem e uma mentira - FrontIn Maringá\n Ciência de Dados: Um Alicerce para Pecuária de Precisão - TICNOVA 2019\n Validando seu produto: Evitando uma morte prematura com dados - IxDA Maringá #3\n A Pecuária de Precisão - Semana de Gestão de Confinamento 2019 (Zoo Jr. - UEM)\n Data-Driven Product Development ft. Luis Berns - FEMUG #23\n O cenário da tecnologia em números - 1º AfroTech MGA\n Estatística: Um Arsenal para Tomada de Decisão - Eureka Moment\n Dissecando Métricas Ágeis - Maringá Agile #4"
  },
  {
    "objectID": "header-participations.html#section-5",
    "href": "header-participations.html#section-5",
    "title": "Participations",
    "section": "",
    "text": "Músicas no Spotify: Como vivem? E quanto tempo sobrevivem? - Cerveja com Dados Maringá #1\n Data Science: Magia ou Ciência? - Semana do programador DB1\n Data Science: Magia ou Ciência? - GDG Maringá: Ciência de dados\n Trabalhando com Consultoria Estatística - VIII SEst UFSCar/USP"
  },
  {
    "objectID": "header-participations.html#section-7",
    "href": "header-participations.html#section-7",
    "title": "Participations",
    "section": "",
    "text": "Correlação Múltipla Wavelet - XXV EAIC e V EAIC Jr.\n The Concept of 4D Datasets - I Workshop em Bioestatística"
  },
  {
    "objectID": "header-participations.html#section-8",
    "href": "header-participations.html#section-8",
    "title": "Participations",
    "section": "",
    "text": "Tópicos em Variância Wavelet - XXIV EAIC e IV EAIC Jr.\n Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, através de métodos bootstrap - VI SEEMI"
  },
  {
    "objectID": "header-participations.html#section-9",
    "href": "header-participations.html#section-9",
    "title": "Participations",
    "section": "2023",
    "text": "2023\n\n Mentor at InovaAgro Hackathon - Maringá - Brazil"
  },
  {
    "objectID": "header-participations.html#section-10",
    "href": "header-participations.html#section-10",
    "title": "Participations",
    "section": "2019",
    "text": "2019\n\n Mentor at Nasa Space Apps - Maringá - Brazil"
  },
  {
    "objectID": "header-participations.html#section-11",
    "href": "header-participations.html#section-11",
    "title": "Participations",
    "section": "2018",
    "text": "2018\n\n Mentor at Nasa Space Apps - Maringá - Brazil\n Participant at Hackathon Romagnole - Maringá - Brazil (2th place)"
  },
  {
    "objectID": "header-awards.html",
    "href": "header-awards.html",
    "title": "Awards",
    "section": "",
    "text": "2023 - 1st place - 1st Prize “Squad Ponta Firme” - Innovative Spirit\n2018 - 2th place - Best presentation - III International Seminar on Statistics with R\n2018 - 2th place - Hackathon Romagnole\n2012 - Bronze medal - XV Brazilian Astronomy and Astronautics Olympiad"
  },
  {
    "objectID": "header-about.html",
    "href": "header-about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Vinícius Félix, and I am a seasoned statistician with technical and managerial experience, as well as a solid background in time-series analysis, geostatistics, and data visualization gained while pursuing a Bachelor of Statistics and a Master of Biostatistics.\nWhile co-founder of H0 Consultoria, I’ve contributed to over 350 scientific studies and 40 business surveys, providing valuable insights to organizations. Following my hands-on technical phase, I advanced to management positions such as R&D, Data and Strategy Manager, Services Manager, and now Head of Data.\nAs I am passionate about bridging theory and real-world applications, my goal is to empower organizations to make data-driven decisions and optimize their operations using statistical insights.\n\nProfessional Experience\n\n\n\n\n\n\n\n\nCompany\nPeriod\nRole\n\n\n\n\n Datlo\n01/2024 - Present\nHead of Data\n\n\n Ponta\n02/2023 - 12/2023\nServices Manager\n\n\n GA + Intergado\n04/2022 - 01/2023\nStrategy Manager\n\n\n GA\n01/2021 - 03/2022\nR&D Manager\n\n\n GA\n09/2020 - 12/2020\nData Manager\n\n\n Unicesumar\n10/2018 - 11/2019\nStatistics Professor\n\n\n H0 Consultoria\n09/2016 - 08/2020\nData Scientist Consultant and Co-founder\n\n\n Estats Consultoria\n07/2013 - 12/2015\nData Analyst and Co-founder\n\n\n\n\n\nAcademic Experience\n\n\n\nUniversity\nPeriod\nDegree\nFinal paper\n\n\n\n\n State University of Maringá (Brazil)\n02/2017 - 02/2019\nMaster of Science in Biostatistics\nSpatio-temporal geostatistics: modelling of natural phenomena in spacetime\n\n\n State University of Maringá (Brazil)\n02/2013 - 12/2016\nBachelor of Science in Statistics\nTemporal, spatial, and spatio-temporal clustering processes and techniques\n\n\n\n\n\nMore about me\n\n[2024-Present] Head of Data\nDatlo is a location intelligence platform for go-to-market and expansion teams. As Head of Data my responsibilities cover every aspect of the data ecosystem, such as technology integration, advanced analytics, data governance, obtaining a variety of datasets, cross-functional cooperation, customer consultation, regulatory compliance, and ongoing platform improvement.\nI work with teams to extract actionable information by utilizing my knowledge in geospatial analytics, making sure customers make decisions that improve their business efforts. In general, I help companies succeed by offering a strong basis for spatial data-driven decision-making.\n\n\n[2023-2023] Services Manager\nFollowing the merger of GA and Intergado, which was rebranded as Ponta, I was given charge of a new area of the company with the goal of expanding and exploring our technical team in order to provide value to our clients beyond our products.\nI was in charge of overseeing the organization’s service delivery. My responsibilities included managing service teams, coordinating operations, and ensuring high-quality service delivery in order to meet the needs of customers and organizational goals, such as:\n\nHardware installation oversight: Managing the procurement, scheduling, and quality assurance of hardware installations.\nSoftware implementation oversight: Selecting and deploying appropriate software solutions, providing user training, and managing licenses.\nConsultancy: Deriving insights, identify trends, and help make informed decisions. Providing consultancy services based on data analysis to clients.\nTechnical support management: Establishing support procedures, handling inquiries and troubleshooting, and coordinating issue resolution with cross-functional teams.\n\nMy primary activities were:\n\nTeam leadership: Leading and managing a team of technicians, assigning tasks, evaluating performance, and fostering effective communication and collaboration.\nContinuous improvement: Staying updated with technology trends, implementing process enhancements, gathering feedback, and promoting best practices and standardization.\nKnowledge management: Systematic process of creating, organizing, and sharing knowledge to drive innovation and improve organizational performance to the whole team.\n\n\n\n[2022-2023] Strategy Manager\nIn order to offer better actions through data science, GA merged with Intergado in 2022, a company that made hardware for accurately automating data collection, such as individual animal weight and provided software with the use of intelligence to make data-driven decisions easier.\nAs the company’s new strategy manager, my responsibilities were:\n\nManaging the implementation of corporate strategy across all products;\nKeeping track of project status and identifying risks;\nCoordinating between technical and non-technical teams to align priorities and goals.\n\n\n\n[2021-2022] R&D Manager\nLater, in the end of that year, I was given a promotion to lead the whole R&D division in 2021, and as such, I was also in charge of the software development team in addition to the data, my responsibilities were:\n\nImplementing automation projects using robots (Jira Automation), web scraping data (Python/R), and improving data monitoring (Metabase) and data-driven decision-making (Dremio);\nApplying agile methodologies and software development concepts in the execution of R&D projects (Jira Software) to drive continuous process improvement;\nManaging the department’s budget and expenses while analyzing and reporting on key performance indicators and project outcomes to the board of directors;\nBuilding project teams, developing schedules, and setting goals to meet overall needs.\n\n\n\n[2020-2020] Data Manager\nIn 2020 I received an invitation to start and grow the data team within GA’s Research & Development (R&D) division in 2020, GA was a Brazilian animal science tech company, that I consulted for some years. My main tasks as Head of Data were:\n\nHiring data scientists and data engineers;\nEstablishing a project methodology;\nHelping in the overall design of the data architecture;\nStarting a data-driven culture within the company.\n\nAlso as I learned more about software development, I created my own R library for various functions to aid in data cleaning and visualization. The library is named relper, and it is available as an open source package to anyone who is interested.\n\n\n[2016-2020] Data Science Consultant\nIn 2016, I co-founded the data consulting firm H0 Consultoria with a partner, where we collaborated on over 350 scientific studies and more than 40 business surveys for corporations. We were responsible for assisting others in making the most of statistics, such as doing:\n\nSampling design;\nData analysis;\nQuestionnaire review;\nStatistics training.\n\n\n[2018-2019] Statistics Professor\nAs I gained notoriety as a consultant, I was invited to teach statistics in the master of business administration program for business intelligence at Unicesumar.\nI updated the course topics by changing the material to a more modern and applied concept of statistics; despite the fact that it was only an introduction class, I used my hands-on experience to demonstrate real-world application of theoretical concepts.\n\n\n[2017-2019] Post graduation\nI decided to pursue a master’s degree in Biostatistics after graduating in the beginning of 2017. My dissertation was about spatio-temporal geostatistics models, that was a natural choice since I had developed projects on temporal and spatial statistics during graduation, so I decided to unite them.\nAlso, I assisted one of my advisers in the development of the geotoolsR R package, providing code to apply bootstrap techniques in the context of geostatistics.\nFinally, for my optional credits, I chose psychometrics and epidemiology.\n\n\n\n[2013-2016] Graduation\nDuring my graduate studies I focused mainly on time-series, geostatistics, and data visualization. I received two scholarships for the Brazilian Program of Scientific Initiation (PBIC), which required students to conduct a research and present them in a congress after one year, and I completed both of them:\n\n[2014] Topics on wavelet variance;\n[2015] Wavelet multiple cross-correlation.\n\nBesides that, I was also a member of the study group of spatial and temporal statistics, having worked with techniques to apply in time series and geostatistics, then I decided to do a final paper about clustering methods for temporal, spatio, and spatio-temporal data.\n\n[2013-2015] Junior enterprise\nIn addition, I was one of the founders of Estats Consultoria in 2013, a statistics junior enterprise, where I served as Marketing Director and, later, President, for three years.\nIn the projects I also worked as a data analyst, and later when I became a senior in college I also helped with the training of the freshmen.\nIn my last year I decided to enter the regional Junior Enterprises Nucleus, to help create a census for the junior enterprises in the region."
  },
  {
    "objectID": "header-certifications.html",
    "href": "header-certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "Data Analyst in SQL\n Data Analyst with Python\n Data Analyst with R\n SQL for Business Analysts\n\n\n\n Data Scientist with R\n\n\n\n R Programmer"
  },
  {
    "objectID": "header-certifications.html#data-analyst",
    "href": "header-certifications.html#data-analyst",
    "title": "Certifications",
    "section": "",
    "text": "Data Analyst in SQL\n Data Analyst with Python\n Data Analyst with R\n SQL for Business Analysts"
  },
  {
    "objectID": "header-certifications.html#data-scientist",
    "href": "header-certifications.html#data-scientist",
    "title": "Certifications",
    "section": "",
    "text": "Data Scientist with R"
  },
  {
    "objectID": "header-certifications.html#developer",
    "href": "header-certifications.html#developer",
    "title": "Certifications",
    "section": "",
    "text": "R Programmer"
  },
  {
    "objectID": "header-certifications.html#atlassian",
    "href": "header-certifications.html#atlassian",
    "title": "Certifications",
    "section": "Atlassian",
    "text": "Atlassian\n Confluence Fundamentals\n Jira Software Fundamentals"
  },
  {
    "objectID": "header-certifications.html#python",
    "href": "header-certifications.html#python",
    "title": "Certifications",
    "section": "Python",
    "text": "Python\n Data Analysis with Python\n Data Analysis with Python\n Data Manipulation with Python\n Data Visualization with Python\n Python Fundamentals"
  },
  {
    "objectID": "header-certifications.html#r",
    "href": "header-certifications.html#r",
    "title": "Certifications",
    "section": "R",
    "text": "R\n Data Manipulation with R\n Data Visualization with R\n R (Basic)"
  },
  {
    "objectID": "header-certifications.html#sql",
    "href": "header-certifications.html#sql",
    "title": "Certifications",
    "section": "SQL",
    "text": "SQL\n MongoDB for SQL Professionals\n SQL Fundamentals\n SQL (Basic)\n SQL (Intermediate)\n SQL (Advanced)"
  },
  {
    "objectID": "header-certifications.html#ai",
    "href": "header-certifications.html#ai",
    "title": "Certifications",
    "section": "AI",
    "text": "AI\n AI Fundamentals\n How Google does Machine Learning"
  },
  {
    "objectID": "header-certifications.html#data-concepts",
    "href": "header-certifications.html#data-concepts",
    "title": "Certifications",
    "section": "Data concepts",
    "text": "Data concepts\n Data Literacy Professional\n Data Storytelling"
  },
  {
    "objectID": "header-publications.html",
    "href": "header-publications.html",
    "title": "Publications",
    "section": "",
    "text": "SOUZA, E. M; FÉLIX, V. B. Wavelet Cross-correlation in Bivariate Time-Series Analysis. TEMA. Tendências em Matemática Aplicada e Computacional, v. 3, p. 391-403, 2018.\nFÉLIX, V. B.; MENEZES, A. F. B. Comparisons of ten corrections methods for t-test in multiple comparisons via Monte Carlo study. Electronic Journal of Applied Statistical Analysis, v. 11, p. 1, 2018.\nHENRIQUES, M. J.; FÉLIX, V. B.; GONZATTO, O. A.; SCHMIDT, F.; GUERRA, N.; OLIVEIRA NETO, A. M. A influência de herbicidas na reinfestação de plantas daninhas: Uma abordagem Bayesiana. REVISTA DA ESTATÍSTICA UFOP, v. VI, p. 140-144, 2017.\nFÉLIX, V. B.; GONZATTO, O. A.; ROSSONI, D. F.; HENRIQUES, M. J. ESTIMADORES DE SEMIVARIÂNCIA: UMA REVISÃO. CIÊNCIA E NATURA, v. 38, p. 1157, 2016.\n\n\n\n\n\nFÉLIX, V. B.; FERNANDES, L. B.; POSE, R. A. Open Source, o novo jeito de fazer ciência. Revista Ser Médico."
  },
  {
    "objectID": "header-publications.html#scientific-magazines",
    "href": "header-publications.html#scientific-magazines",
    "title": "Publications",
    "section": "",
    "text": "SOUZA, E. M; FÉLIX, V. B. Wavelet Cross-correlation in Bivariate Time-Series Analysis. TEMA. Tendências em Matemática Aplicada e Computacional, v. 3, p. 391-403, 2018.\nFÉLIX, V. B.; MENEZES, A. F. B. Comparisons of ten corrections methods for t-test in multiple comparisons via Monte Carlo study. Electronic Journal of Applied Statistical Analysis, v. 11, p. 1, 2018.\nHENRIQUES, M. J.; FÉLIX, V. B.; GONZATTO, O. A.; SCHMIDT, F.; GUERRA, N.; OLIVEIRA NETO, A. M. A influência de herbicidas na reinfestação de plantas daninhas: Uma abordagem Bayesiana. REVISTA DA ESTATÍSTICA UFOP, v. VI, p. 140-144, 2017.\nFÉLIX, V. B.; GONZATTO, O. A.; ROSSONI, D. F.; HENRIQUES, M. J. ESTIMADORES DE SEMIVARIÂNCIA: UMA REVISÃO. CIÊNCIA E NATURA, v. 38, p. 1157, 2016."
  },
  {
    "objectID": "header-publications.html#magazines",
    "href": "header-publications.html#magazines",
    "title": "Publications",
    "section": "",
    "text": "FÉLIX, V. B.; FERNANDES, L. B.; POSE, R. A. Open Source, o novo jeito de fazer ciência. Revista Ser Médico."
  },
  {
    "objectID": "header-publications.html#complete-works",
    "href": "header-publications.html#complete-works",
    "title": "Publications",
    "section": "Complete works",
    "text": "Complete works\n\nAUGUSTO JUNIOR, S. N.; FÉLIX, V. B. Survival analysis of the brazilian Spotify ranking: Differences between national and international artists. In: III International Seminar on Statistics with R, 2018, Rio de Janeiro. Survival analysis of the brazilian Spotify ranking: Differences between national and interionational artists, 2018.\nROSSONI, D. F.; FÉLIX, V. B. Métodos Bootstrap Para Dados Com Dependência Espacial. In: 60ª Reunião Anual da Região Brasileira da Sociedade Internacional de Biometria (RBras) e o 16º Simpósio de Estatística Aplicada a Experimentação Agronômica (SEAGRO), 2015, Presidente Prudente. Métodos Bootstrap Para Dados Com Dependência Espacial, 2015."
  },
  {
    "objectID": "header-publications.html#expanded-abstracts",
    "href": "header-publications.html#expanded-abstracts",
    "title": "Publications",
    "section": "Expanded abstracts",
    "text": "Expanded abstracts\n\nFÉLIX, V. B.; MENEZES, A. F. B. Monte Carlo study of multiple comparisons corrections in t-test. In: 5th Workshop on Probabilistic and Statistical Methods, 2017, São Carlos. Monte Carlo study of multiple comparisons corrections in t-test, 2017.\nFÉLIX, V. B.; ALVARENGA, B.; FERNANDES, L. B. Impacto causal de uma visita técnica no processo de fornecimento de uma fazenda via modelo estrutural temporal Bayesiano. In: I Encontro de Modelagem Estatística, 2017, Maringá. Impacto causal de uma visita técnica no processo de fornecimento de uma fazenda via modelo estrutural temporal Bayesiano, 2017.\nFÉLIX, V. B.; GARCIA, F.; FERNANDES, L. B. Controle de consumo bovino com intervalos de tolerância via modelos não-paramétricos. In: I Encontro de Modelagem Estatística, 2017, Maringá. Controle de consumo bovino com intervalos de tolerância via modelos não-paramétricos, 2017.\nFÉLIX, V. B.; HENRIQUES, M. J. ; GONZATTO, O. A. Modelos Espaciais para Predição de Dados Batimétricos. In: VII Congresso Científico Da Região Centro-Ocidental Do Paraná - CONCCEPAR, 2016, Campo Mourão. Modelos Espaciais para Predição de Dados Batimétricos, 2016.\nHENRIQUES, M. J. ; FÉLIX, V. B. ; GONZATTO, O. A. Abordagem Bayesiana no Controle de Brachiaria Plantaginea, Euphorbia Heterophylla e Richardia Brasiliensis com o Uso de Flumioxazin, Amicarbazone, Clomazone e Atrazine. In: VII Congresso Científico Da Região Centro-Ocidental Do Paraná - CONCCEPAR, 2016, Campo Mourão. Abordagem Bayesiana no Controle de Brachiaria Plantaginea, Euphorbia Heterophylla e Richardia Brasiliensis com o Uso de Flumioxazin, Amicarbazone, Clomazone e Atrazine, 2016.\nFÉLIX, V. B.; SOUZA, E. M. Correlação Múltipla Wavelet. In: XXV EAIC e V EAIC Jr, 2016, Maringá. Correlação Múltipla Wavelet, 2016.\nFÉLIX, V. B.; GUEDES, T. A. O impacto de diferentes concentrações de reguladores vegetais 2,4-D nos efeitos fisiológicos em Citrus sinensis com cancro cítrico, via regressão multivariada. In: I Workshop em Bioestatística, 2016, Maringá. O impacto de diferentes concentrações de reguladores vegetais 2,4-D nos efeitos fisiológicos em Citrus sinensis com cancro cítrico, via regressão multivariada, 2016.\nHENRIQUES, M. J.; GONZATTO, O. A.; FÉLIX, V. B.; SCHMIDT, F.; OLIVEIRA NETO, A. M. A Influência De Herbicidas Na Reinfestação De Plantas Daninhas: Uma Abordagem Bayesiana. In: 60ª Reunião Anual da Região Brasileira da Sociedade Internacional de Biometria (RBras) e o 16º Simpósio de Estatística Aplicada a Experimentação Agronômica (SEAGRO), 2015, Presidente Prudente. A Influência De Herbicidas Na Reinfestação De Plantas Daninhas: Uma Abordagem Bayesiana, 2015.\nSOUZA, E. M.; SAPUCCI, L.; FÉLIX, V. B. Inter-relation of time series from Cross Correlation Wavelets. In: XVI Escola de Séries Temporais e Econometria, 2015, Campos do Jordão. Inter-relation of time series from Cross Correlation Wavelets, 2015.\nFÉLIX, V. B.; ROSSONI, D. F. Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, através de métodos bootstrap. In: VI SEEMI - VI Simpósio de Estatística Espacial e Modelagem de Imagens, 2015, Toledo. Avaliação de ajuste geoespacial robusto no semivariograma de dados batimétricos, Através de métodos bootstrap, 2015.\nFÉLIX, V. B.; SOUZA, E. M. Tópicos em Variância Wavelet. In: XXIV EAIC e IV EAIC Jr, 2015, Maringá. Tópicos em Variância Wavelet, 2015.\nFÉLIX, V. B.; SOUZA, E. M. Análise De Variância Wavelet Aplicada Em Séries Temporais. In: 60ª Reunião Anual da Região Brasileira da Sociedade Internacional de Biometria (RBras) e o 16º Simpósio de Estatística Aplicada a Experimentação Agronômica (SEAGRO), 2015, Presidente Prudente. Análise De Variância Wavelet Aplicada Em Séries Temporais, 2015."
  },
  {
    "objectID": "header-publications.html#abstracts",
    "href": "header-publications.html#abstracts",
    "title": "Publications",
    "section": "Abstracts",
    "text": "Abstracts\n\nFÉLIX, V. B.; SOUZA, E. M.; ROSSONI, D. F. Classes de modelos de covariância para Geoestatística espaço-temporal. In: XIV Semana da Estatística da UEM, 2018, Maringá. Classes de modelos de covariância para Geoestatística espaço-temporal, 2018.\nMENEZES, A. F. B.; FÉLIX, V. B. Estudo de simulação Monte Carlo para testes post hoc. In: XIII Semana da Estatística, 2016, Maringá. Estudo de simulação Monte Carlo para testes post hoc, 2016.\nFÉLIX, V. B.; FURRIEL, W. O. Análise de perfil de Twitter dos 7 candidatos mais votados na eleição presidencial brasileira de 2014. In: XIII Semana da Estatística, 2016, Maringá. Análise de perfil de Twitter dos 7 candidatos mais votados na eleição presidencial brasileira de 2014, 2016.\nHENRIQUES, M. J.; GONZATTO, O. A.; FÉLIX, V. B. Uso do software R para análise da variabilidade espacial do teor de pH no solo em uma área experimental. In: VI Congresso Científico da Região Centro-Ocidental do Paraná, 2015, Campo Mourão. Uso do software R para análise da variabilidade espacial do teor de pH no solo em uma área experimental, 2015.\nGONZATTO, O. A.; HENRIQUES, M. J.; FÉLIX, V. B. Análise da variabilidade espacial da quantidade de argila no solo em uma parcela experimental. In: VI Congresso Científico da Região Centro-Ocidental do Paraná, 2015, Campo Mourão. Análise da variabilidade espacial da quantidade de argila no solo em uma parcela experimental, 2015.\nSOUZA, E. M.; SAPUCCI, L. F.; NEGRI, T. T.; FÉLIX, V. B. Low cost GPS-wavelet-based methodologies to advertise climate and environmental extreme events. In: 60th World Statistics Congress, 2015, Rio de Janeiro. Low cost GPS-wavelet-based methodologies to advertise climate and environmental extreme events, 2015.\nFÉLIX, V. B.; SOUZA, E. M.; MENEZES, A. F. B. Análise de cluster para séries temporais de internações por bronquiolite nas Regionais de saúde do Paraná. In: XII Semana da Estatística, 2015, Maringá. Análise de cluster para séries temporais de internações por bronquiolite nas Regionais de saúde do Paraná, 2015.\nFÉLIX, V. B.; SOUZA, E. M. Análise de Intervenção na importação/exportação de combustível nos Estados Unidos da América (EUA). In: XII Semana da Estatística, 2015, Maringá. Análise de Intervenção na importação/exportação de combustível nos Estados Unidos da América (EUA), 2015.\nFÉLIX, V. B.; GONZATTO, O. A.; HENRIQUES, M. J.; LANDGRAF, G. O.; ARAUJO, I. M.; ROSSONI, D. F. Comparação de Robustez dos Estimadores de Semivariância Aplicados a Dados Batimétricos. In: XI Semana da Estatística, 2014, Maringá. Comparação de Robustez dos Estimadores de Semivariância Aplicados a Dados Batimétricos, 2014.\nFÉLIX, V. B.; SOUZA, E. M. Correção Múltipla e Cruzada de Wavelets. In: XI Semana de Estatística, 2014, Maringá. Correção Múltipla e Cruzada de Wavelets, 2014.\nLANDGRAF, G. O.; ROSSONI, D. F.; FÉLIX, V. B. Existe diferença em mapas preditos por interpolação produzidos por diferentes softwares?. In: 45ª reunião regional da ABE e X semana de Estatística, 2013, Maringá. Existe diferença em mapas preditos por interpolação produzidos por diferentes softwares?, 2013."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html",
    "href": "posts/0001-chi-square-test/index.html",
    "title": "An intro to: Chi-square Test",
    "section": "",
    "text": "In this post I’ll solve the mystery of the Chi-Square, spoiler alert: It’s significant!"
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#introduction",
    "href": "posts/0001-chi-square-test/index.html#introduction",
    "title": "An intro to: Chi-square Test",
    "section": "Introduction",
    "text": "Introduction\nAfter all of these equations, we’ll perform a real-world example.\nLet’s say we have 200 animals in our random sample, and we want to see if race has anything to do with frame size. In order to see our observed values in each class, i.e., race x frame, we will first look at a contingency table with the absolute frequency of animals:\n\n\n\nFrame\nRace 1\nRace 2\nRace 3\nFrame Total\n\n\n\n\nSmall\n10\n20\n10\n40\n\n\nMedium\n20\n30\n20\n70\n\n\nLarge\n30\n50\n10\n90\n\n\nRace Total\n60\n100\n40\n200"
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#test-statistic",
    "href": "posts/0001-chi-square-test/index.html#test-statistic",
    "title": "An intro to: Chi-square Test",
    "section": "Test statistic",
    "text": "Test statistic\nTo compute our statistic, as given by the equation Equation 1, we have to:\n\nCompute the expected value for each class (cell in terms of a contingency table);\nCompute the component \\(\\frac{(O_i -E_i)^2}{E_i}\\) fo each class;\nCompute the \\(X^2\\) statistic by summing all values of step 2.\n\nTo begin, we will perform the calculus for a single cell to demonstrate each step, and we will select the Race 1 x Small frame class. In this case, our observed value is 10, so we do the following to calculate the expected value:\n\\[\nE_1 = (40 \\times 60)/200 = 12.\n\\tag{15}\\]\nWe multiplied our marginal results and divided them by our sample size because one of our assumptions is that the variables are independent. We can now compute the component for the first cell using our expected value.\n\\[\n\\begin{align}\n& =  \\frac{(O_1 - E_1)^2}{E_1}  \\\\\n& =  \\frac{(10 - 12)^2}{12}  \\\\\n& =  \\frac{(-2)^2}{12}  \\\\\n& =  \\frac{4}{12} \\\\\n& =  1/3.\\\\\n\\end{align}\n\\tag{16}\\]\nNow we apply the calculus to each cell, computing the expected value for every class:\n\n\n\nFrame\nRace 1\nRace 2\nRace 3\n\n\n\n\nSmall\n12\n20\n8\n\n\nMedium\n21\n35\n14\n\n\nLarge\n27\n45\n18\n\n\n\nAnd then we can also compute \\(\\frac{(O_i -E_i)^2}{E_i}\\) for each one:\n\n\n\nFrame\nRace 1\nRace 2\nRace 3\n\n\n\n\nSmall\n0.33\n0\n0.50\n\n\nMedium\n0.05\n0.71\n2.57\n\n\n\n0.33\n0.56\n3.56\n\n\n\nLastly, we sum all the values to obtain the statistic \\(X^2\\), which is equal to 8.61."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#p-value",
    "href": "posts/0001-chi-square-test/index.html#p-value",
    "title": "An intro to: Chi-square Test",
    "section": "p-value",
    "text": "p-value\nNow, if we want to obtain the p value we have to look at the chi-sqaure distribution, so first we need to obtain the number of degrees of freedom \\((q)\\), in this cases that is given by:\n\\[\nq = (n_r-1)\\times(n_c-1),\n\\tag{17}\\]\nwhere\n\n\\(n_r\\) is the number of rows in the contingency table, i.e., the number of levels of the respective categorical variable;\n\\(n_c\\) is the number of columns in the contingency table, i.e., the number of levels of the respective categorical variable.\n\nThen, we have in our example that\n\\[\n\\begin{align}\nq & =  (n_r-1)\\times(n_c-1)  \\\\\n& =  (3-1) \\times (3-1) \\\\\n& =  (2) \\times (2) \\\\\n& =  4.\\\\\n\\end{align}\n\\tag{18}\\]\nWith an established \\(q\\), we now can compute the p value given by:\n\\[\nP(\\chi_4^2 &gt; X^2|H_0),\n\\tag{19}\\]\nor the probability that a value is larger than \\(X^2\\) given a \\(\\chi_4^2\\) distribution and a true null hypothesis, as we can see in the figure below:\n\n\n\n\n\nWith a p value of 0.0716, we have that the p value is greater than 0.05, so we do not reject the null hypothesis, i.e., we do not have sample evidence to reject the null hypothesis that race and size frame are independent.."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#contingency-table",
    "href": "posts/0001-chi-square-test/index.html#contingency-table",
    "title": "An intro to: Chi-square Test",
    "section": "Contingency table",
    "text": "Contingency table\nIf you have your contingency table ready, an easy and quick way to apply the test is to create a matrix with the observed values of each class.\n\n#matrix with the count \ndata &lt;- matrix(data = c(10,20,30,20,30,50,10,20,10),ncol = 3)\ndata\n\n     [,1] [,2] [,3]\n[1,]   10   20   10\n[2,]   20   30   20\n[3,]   30   50   10\n\nchisq.test(data)\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 8.6111, df = 4, p-value = 0.07159\n\n\nWe can see that the function already show us the statistic, number of degrees of freedom and the p value."
  },
  {
    "objectID": "posts/0001-chi-square-test/index.html#raw-data",
    "href": "posts/0001-chi-square-test/index.html#raw-data",
    "title": "An intro to: Chi-square Test",
    "section": "Raw data",
    "text": "Raw data\nWe usually work with raw data as data analysts, where each row represents one observation. In this case, we can transform our data to perform the same function as in the previous example.\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n#Raw data simulation\ndata &lt;-\n  expand_grid(\n    race  = c(1:3), \n    frame = factor(c(\"S\",\"M\",\"L\"))\n  ) %&gt;% \n  arrange(race,frame) %&gt;% \n  mutate(n  = c(10,20,30,20,30,50,10,20,10)) %&gt;% \n  uncount(n)\n\ndata\n\n# A tibble: 200 x 2\n    race frame\n   &lt;int&gt; &lt;fct&gt;\n 1     1 L    \n 2     1 L    \n 3     1 L    \n 4     1 L    \n 5     1 L    \n 6     1 L    \n 7     1 L    \n 8     1 L    \n 9     1 L    \n10     1 L    \n# ... with 190 more rows\n\ndata %&gt;% \n  #Number of observed values for each class\n  count(race,frame) %&gt;% \n  #Pivot table to make a contingency table\n  pivot_wider(names_from = race,values_from = n) %&gt;% \n  #Removal of the variable as column\n  select(-1) %&gt;% \n  #Chi-square test\n  chisq.test()\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 8.6111, df = 4, p-value = 0.07159"
  },
  {
    "objectID": "posts/0003-log-scale/index.html",
    "href": "posts/0003-log-scale/index.html",
    "title": "An intro to: Logarithmic Scale",
    "section": "",
    "text": "In this post you will learn how to go beyond linear."
  },
  {
    "objectID": "posts/0003-log-scale/index.html#exponentiation",
    "href": "posts/0003-log-scale/index.html#exponentiation",
    "title": "An intro to: Logarithmic Scale",
    "section": "Exponentiation",
    "text": "Exponentiation\nWhen learning math, we usually start by the fundamental arithmetic operations:\n\nAddiction and Subtraction;\nMultiplication and Division.\n\nAfter the basics, the next step is the exponentiation, which is essentially a two-number math operation, where:\n\n\\(b\\) is the base, the value that will be exponentiated;\n\\(n\\) is the exponent, the value that the base will be raised by the power of.\n\nThen, we say that \\(b\\) is raised to the power of \\(n\\), meaning that:\n\\[\nb^n = b_{[1]} \\times b_{[2]} \\times ... \\times b_{[n-1]} \\times b_{[n]} = x, \\quad n \\in \\mathbb{Z}^{++}.\n\\tag{1}\\]\nAs stated in the Equation 1, \\(b\\) is multiplied by itself \\(n\\) times, and this property of the exponentiation is given for any positive integer \\(n\\).\nSo, let’s say we want to exponentiate the number 10 to the power of 3, that is:\n\\[\n10^3 = 10\\times 10 \\times 10 = 100 \\times 10 = 1000.\n\\tag{2}\\]\nIn the example above, 3 is the exponent and 10 is the base, resulting in 1,000.\nNext, let’s understand how the exponential function works in general. For the first scenario, we will use 3 exponents (2, 3 and 4) applied to a sequence of bases from -1 to 1.\n\n\n\n\n\nLooking at the figure above we can see some interesting behaviors:\n\n\\(b^n = 0\\) when \\(b = 0\\);\n\\(b^n = 1\\) when \\(b = 1\\), for all \\(n\\);\n\\(b^n = 1\\) when \\(b = -1\\) and \\(n\\) is even;\n\\(b^n = -1\\) when \\(b = -1\\) and \\(n\\) is odd;\n\\(b^n &gt; 0\\) when \\(n\\) is even, since multiplying a negative value for an even number of times yields a positive result;\n\\(b^n &gt; b\\) when \\(1 &gt; b &gt; 0\\), to help our visualization of this behavior, we will plot a identity line dashed with the color red, i.e., \\(b^n = b\\).\n\n\n\n\n\n\nWe see that the values are below the line for positives bases, hence:\n\\[\n1 &gt;  b &gt; 0  \\longrightarrow b^n &gt; b, \\quad n \\in \\mathbb{Z}^{++}.\n\\tag{3}\\]\nBut let’s keep in mind that until now we worked with bases between -1 and 1, will see how the exponentiation behavior for \\(b &gt;= 1\\) next.\n\n\n\n\n\nLooking at values &gt; 1 for our base, we see how fast the results of \\(b^n\\) grows for a higher \\(n\\).\nAfter seeing how the behavior is for differents bases, we will do the same for the exponents. As we applied integer positive values for our exponents in the examples before, let’s see how fractional exponents behavior.\n\n\n\n\n\nWe can see that for negatives values of \\(b\\) there are not defined values of \\(b^n\\), but why is that?\n\\[\nb^{\\frac{n}{m}} = (b^n)^{\\frac{1}{m}} = \\sqrt[m]{b^n}.\n\\tag{4}\\]\nIn the Equation 4 we look how a fractional exponent is actually the \\(m\\)th root of \\(b^n\\), so if \\(m\\) is even, there would be no real solution, since a even root of a negative value is not defined for real numbers.\nAfter understanding the basics of the the exponentiation we can jump to the inverse operation, the logarithm (\\(\\log\\))."
  },
  {
    "objectID": "posts/0003-log-scale/index.html#logarithm",
    "href": "posts/0003-log-scale/index.html#logarithm",
    "title": "An intro to: Logarithmic Scale",
    "section": "Logarithm",
    "text": "Logarithm\nSo, let’s see how \\(\\log\\) works, since it is the inverse of the exponentiation, we can write it as:\n\\[\n\\log_b(x) = n \\longleftrightarrow b^n = x.\n\\tag{5}\\]\nAs stated in the equation above, the \\(\\log\\) function gives what is the exponent (\\(n\\)) we have to raise our base (\\(b\\)) to result in \\(x\\).\nApplying this logic, we can see the Equation 2 as:\n\\[\n\\log_{10}(1000) = 3.\n\\tag{6}\\]\nSo the \\(\\log\\) is which number 10 has to be exponiented to result in 1,000, then we can easily expand this to show other results:\n\n\n\n\n\n\n\n\\(\\log_{10}(x)\\)\n\\(x\\)\n\n\n\n\n-5\n0.00001\n\n\n-4\n0.0001\n\n\n-3\n0.001\n\n\n-2\n0.01\n\n\n-1\n0.1\n\n\n0\n1\n\n\n1\n10\n\n\n2\n100\n\n\n3\n1000\n\n\n4\n10,000\n\n\n5\n100,000\n\n\n\nIn the table above we see thjat the result of the \\(\\log_{10}(x)\\) increase 1 unit as the value in \\(x\\) is multiplied by 10. This is a very helpful property, that will explore more later.\nJust as we did with the exponentiation, let’s see how the \\(\\log\\) behavior, so we will apply the function to a \\(x\\) varying from -1 to 11 with three different bases (2, \\(\\mathcal{e}\\) and 10).\n\n\n\n\n\nLooking at the figure above we can see some interesting behaviors:\n\n\\(n\\) is nonexistent when \\(x &lt; 0\\), as we saw earlier for somes cases in the exponentiation would result in a undefined number;\n\\(n\\) is equal to 1 when \\(x = b\\);\n\\(n\\) is equal to 0 when \\(x = 1\\);\n\\(n\\) is negative when \\(x &lt; 1\\).\n\nBefore exploring more of the logarithm properties, you can be asking what it is the \\(\\mathcal{e}\\) used in the example before?\n\nNatural logarithm\nThis logarithm is a special case, where the base of the \\(\\log\\) is the number \\(\\mathcal{e}\\), also known as Euler’s number or Napier’s constant, defined by:\n\\[\n\\mathcal{e} = \\sum_{n = 0}^{\\infty}\\frac{1}{n!} = \\frac{1}{1} + \\frac{1}{1\\times2}+ \\frac{1}{1\\times2\\times3} + ... \\approx 2.718282.\n\\tag{7}\\]\nSo the natural logarithm can be written as:\n\\[\n\\log_{\\mathcal{e}}(x) = \\mathrm{ln}(x).\n\\tag{8}\\]\n\n\nProperties\nThe \\(\\log\\) function has many properties that helps us in many situations, let’s see the main properties.\n\nProduct\nThe first property is that the \\(\\log\\) of the products of \\(x\\) and \\(y\\) is the same as the sum of the \\(\\log\\)’s of \\(x\\) and \\(y\\), that is:\n\\[\n\\log_b(xy) = \\log_b(x) + \\log_b(y).\n\\tag{9}\\]\nTo see that, let’s use the Equation 5 and define a second logarithm as:\n\\[\n\\log_b(y) = m \\longleftrightarrow y = b^m.\n\\tag{10}\\]\nBy applying the product property of the exponentiation we have that:\n\\[\nxy = b^n \\ b^ m = b^{(n+m)}.\n\\tag{11}\\]\nNow, if we apply the Equation 11 in the Equation 9, our result is:\n\\[\n\\begin{align}\n\\log_b(xy)\n&= \\log_b(b^{(n + m)}) \\\\\n&= n + m \\\\\n&= \\log_b(x) + \\log_b(y).\n\\end{align}\n\\tag{12}\\]\n\n\nQuotient\nThe second property is that the \\(\\log\\) of the division of \\(x\\) and \\(y\\) is the same as the subtraction of the \\(\\log\\)’s of \\(x\\) and \\(y\\), that is:\n\\[\n\\log_b\\left(\\frac{x}{y}\\right) = \\log_b(x) - \\log_b(y).\n\\tag{13}\\]\nUsing the same definitions of Equation 5 and Equation 10, we have that:\n\\[\n\\frac{x}{y} = \\frac{b^n}{b^m} = b^{n-m},\n\\tag{14}\\]\nif we aplly the Equation 14 in the Equation 13, our result is:\n\\[\n\\begin{align}\n\\log_b\\left(\\frac{x}{y}\\right)\n&= \\log_b(b^{n-m}) \\\\\n&= n-m \\\\\n&= \\log_b(x) - \\log_b(y).\n\\end{align}\n\\tag{15}\\]\n\n\nPower\nThe third property is that the \\(\\log\\) of a value \\(x\\) raised by a power \\(a\\), is equal to the product of \\(a\\) times the \\(\\log\\) of \\(x\\).\n\\[\n\\log_b(x^a) = a\\log_b(x).\n\\tag{16}\\]\nExpanding the definition in Equation 5:\n\\[\n\\log_b(x) = n \\longrightarrow x = b^n \\longrightarrow x^a = (b^{n})^a = b^{an},\n\\tag{17}\\]\nNow, if we use the definition in the Equation 16, we have that:\n\\[\n\\begin{align}\n\\log_b(x^a)\n&= \\log_b(b^{an}) \\\\\n&= an \\\\\n&= a\\log_b(x).\n\\end{align}\n\\tag{18}\\]"
  },
  {
    "objectID": "posts/0003-log-scale/index.html#real-data-application",
    "href": "posts/0003-log-scale/index.html#real-data-application",
    "title": "An intro to: Logarithmic Scale",
    "section": "Real data application",
    "text": "Real data application\nWe will look at the number of deaths from COVID–19 (Guidotti and Ardia 2020) of Argentina (ARG), Brazil (BRA) and the United States of America (USA), in 2020.\nAs a disclaimer, the goal of this analysis is to demonstrate how the logarithm can be useful, not to gain any insight into how COVID actually behaved in these countries, as that would necessitate more in-depth research.\n\n\n\n\n\nLooking at the figure above we see that number of deaths are bigger in USA, Brazil and lastly Argentina, that is no surprise since it follows the same order of population size.\nSo if we want to compare the behavior of the countries it can be hard, for example, Argentina has less deaths and the curve become “squished”, making it hard to see what really happened.\nSince we have data with different magnitudes, we can apply the logarithm.\n\n\n\n\n\nAfter the application of the \\(\\log_10\\), we can see each country’s behavior more clearly. For example, the number of deaths in the United States began earlier and slowed down faster than in Brazil, whereas Argentina had a steady number of deaths until November, when it began to “stabilize”."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html",
    "href": "posts/0005-settlers-of-catan/index.html",
    "title": "An analysis of: Settlers of Catan",
    "section": "",
    "text": "In this post you will learn how statistics and probability can help you to win a board game."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#is-the-dice-fair",
    "href": "posts/0005-settlers-of-catan/index.html#is-the-dice-fair",
    "title": "An analysis of: Settlers of Catan",
    "section": "Is the dice fair?",
    "text": "Is the dice fair?\nEvery turn, each player rolls two dice and adds their totals together to determine which location will grant resources to players who have cities or settlements there. This will be the subject of the first analysis.\nWe will compute the probability of each result for the sum of the two dices, taking into account that there are six possible outcomes for each face of each die.\n\n\n\n\n\nIn the picture above, we can see a graph where each axis represents the outcome of a single die, and we can also see all possible outcomes of the sum of those dices.\nSome outcomes are more common than others, for example, the number 7 is the most common outcome because it appears six times.\nAnother intriguing finding is that the results exhibit symmetry; to further explore this, let’s use another visual representation.\n\n\n\n\n\nThe extreme results, 2, and 12, with only one combination for each, are symmetrical and center on the number 7, as was previously mentioned.\nWe will now compare the observed data to the predicted result.\n\n\n\n\n\nWhen comparing the observed values from the real dataset, we can see that the dice results appear to be fairly random because they closely resemble our anticipated result. The number 4 had the biggest discrepancy, with observed values 1.02 percentage points below the predicted probability."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#spending-money-to-make-money.",
    "href": "posts/0005-settlers-of-catan/index.html#spending-money-to-make-money.",
    "title": "An analysis of: Settlers of Catan",
    "section": "“Spending money to make money.”",
    "text": "“Spending money to make money.”\nIn the dataset we have the following concepts, as described by the author:\n\nProduction gain: Cards gained from structures;\nTrade gain: Cards gained from peer or bank trade;\nNon-production gain: Cards gained from stealing with the robber, plus cards gained with non-knight development cards, e.g., a road building card is +4 resources;\nTotal gain: Production + Trade + Non-production;\n\nAlso we have the ways to loss cards\n\nTrade loss: Cards lost from peer or bank trades;\nRobber loss: Cards lost directly from robbers, knights, and other players’ monopoly cards;\nTribute loss: Cards lost when player had to discard on a 7 roll;\nTotal loss: Trade + Robber + Tribute.\n\nFirst of all, let’s see how the total gain and loss relate.\n\n\n\n\n\nWe see in the figure above that:\n\nThe loss and gain are positive correlated, that means that players that gained more also lost more cards;\nThere is no player that lost more than gained, as no point is below the identity line;\nThe winners gained a lot more cards than player that lost.\n\nTo take a better look at the third point, let’s plot the gain/loss cards ratio.\n\n\n\n\n\nLooking at the gain/loss ratio density plot, we see that:\n\nThe density is positive skewed, for winners or losers;\nThe losers have a more concentrated density, using the peak value of the density as a metric, winners gain 2.31 cards to every card lost, whereas losers gain 2.15 cards."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#the-last-will-be-first-and-the-first-last.",
    "href": "posts/0005-settlers-of-catan/index.html#the-last-will-be-first-and-the-first-last.",
    "title": "An analysis of: Settlers of Catan",
    "section": "“The last will be first, and the first last.”",
    "text": "“The last will be first, and the first last.”\nThe playing order is important in this game because it gives you the ability to choose the locations of your settlement, but being the last one is not the worst, since you become the first to choose your second settlement in the map.\nTo make the analysis of the importance of the locations choosen, we will use the dice sum results and define as weights, e.g., if the number is 12 the weight will be 1, since the the probability of this results is 1/36, as we saw in the dice analysis.\nThen we will sum the weights of the initial two settlements of each player (\\(wt\\)), and we will do the different of the total weight of the winner minus the maximum weight of that game, given by:\n\\[\nwt_{\\mathrm{winner}} - \\max(wt).\n\\]\n\n\n\n\n\nDifference\nFrequency\n\n\n\n\n-5\n1\n\n\n-4\n5\n\n\n-3\n8\n\n\n-2\n8\n\n\n-1\n4\n\n\n0\n24\n\n\n\n\n\n\n\nA difference of zero means that the winner was the player with the best location, at least probability-wise. So in almost half of the games the winner had the best location."
  },
  {
    "objectID": "posts/0005-settlers-of-catan/index.html#if-your-ship-doesnt-come-in-swim-out-and-meet-it.",
    "href": "posts/0005-settlers-of-catan/index.html#if-your-ship-doesnt-come-in-swim-out-and-meet-it.",
    "title": "An analysis of: Settlers of Catan",
    "section": "“If your ship doesn’t come in, swim out and meet it.”",
    "text": "“If your ship doesn’t come in, swim out and meet it.”\nThe port is a unique location. Normally, you can exchange 4 identical resources for another resource at the bank, but with a port you can lower this trade rate, but you lose one resource location in the process.\n\n\n\n\n\nLocation\nLocations\nPercentage\n\n\n\n\nPort\n35\n2.92\n\n\nResource\n1,165\n97.08\n\n\nTotal\n1,200\n100.00\n\n\n\n\n\n\n\nSo of all the 1,200 initial settlements just 35 of them were ports, so it is not a popular strategy.\n\n\n\n\n\nPlayer\nGames\nPercentage\n\n\n\n\nLoser\n27\n81.82\n\n\nWinner\n6\n18.18\n\n\nTotal\n33\n100.00\n\n\n\n\n\n\n\nThose 35 port initial locations were in 33 games, where in only 6 of them the player was the winner, let’s explore why.\n\n\n\n\n\nLooking at the trading behavior we see that in average player with a initial port gained more, so let’s take a look at the trade ratio (gain/loss).\n\n\n\n\n\nPlayers that had a initial port had a higher ratio, that can sound counterintuitive, since we expect they had a better trade-off with the ports advantage, a possibility is the lower quantity of resources making the players make actually worst trade for other resources, since they give up one location when choosing this strategy, but how impactful is that?\n\n\n\n\n\nLastly, when can see how impactful was the production of cards from structures based on the initial port. The result was a lot lower for players with a initial port, with almost 16 cards of difference from players that choosed 3 resources locations."
  },
  {
    "objectID": "posts/0007-king-james-bible/index.html",
    "href": "posts/0007-king-james-bible/index.html",
    "title": "An analysis of: The King James Bible",
    "section": "",
    "text": "In this post you will learn the hallelujah highs and lament lows of the bible.\n\nContext\nThe King James Bible, first published in 1611, is a crucial English translation of the Bible.\n“Hey, let’s have a fancy new translation!” exclaimed King James I of England. So he enlisted the help of a group of outstanding scholars. They were inspired by old English versions as well as the original Hebrew and Greek texts.\n\nFor the analysis we will use the text from the King James Bible.\n\nDisclaimer: the goal here is just to show and apply some techniques to work with text data.\n\n\n\nHow is the bible built?\nThis Bible is divided into two parts:\n\nThe Old Testament, which contains all of the religious material from before Jesus appeared;\nThe New Testament, which contains everything about Jesus and his followers.\n\nIn addition from the testaments, the bible is also divided in books and verses.\n\nAs we can see, the old testament has books with twice as many verses as the new testament. But, when we look by book, are the numbers of verses consistent?\n\n\nTestament\nBooks\nVerses\nVerses/Books\n\n\n\n\nOld\n39\n23,145\n593.4615\n\n\nNew\n27\n7,957\n294.7037\n\n\nTotal\n66\n31,102\n471.2424\n\n\n\n\n\n\n\n\nClearly not, as the number of verses varies greatly, with an outlier in the old testament, The Book of Psalms, having astounding 2,461 verses.\n\nBook of Psalms\n\n\n\n\nIt is a collection of religious songs, prayers, and poems attributed to King David of Israel as well as other authors such as Asaph, Korah’s sons, Solomon, and Moses.\nPraise, thanksgiving, trust in God, deliverance, longing for God’s presence, justice, and worship are just a few of the themes covered in the psalms. They are a rich source of spiritual reflection, expressing a wide range of human emotions and providing believers with comfort, guidance, and encouragement. The psalms are well-known for their poetic form, vivid imagery, and long-lasting spiritual and literary value, and they are widely used in Jewish and Christian worship.\n\n\n\nWith the exception of the last book, The Revelation of St. John the Divine, the new testament begins with books with a greater number of verses but decreases as the bible progresses.\n\nThe Revelation of St. John the Divine\n\n\n\n\nThe Book of Revelation, attributed to the apostle John, contains apocalyptic visions received by John while he was exiled on the island of Patmos.\nIt contains prophetic messages and symbolic language depicting the end times, final judgment, and God’s victory over evil. The book deals with topics such as faithfulness, persecution, divine sovereignty, and the establishment of a new heaven and earth. It contains messages to seven churches, heavenly worship, and predictions of future events, and it has sparked ongoing interpretation and fascination among Christians.\n\n\n\n\n\nWord-o-Rama\nNow let’s analyze the word frequency, the bible possess 789,649 words in total, where 12,784 are unique words. Here is the top 10 most frequent words:\n\n\n    Word Frequency\n1    the    63,925\n2    and    51,696\n3     of    34,617\n4     to    13,562\n5   that    12,912\n6     in    12,667\n7     he    10,420\n8  shall     9,838\n9   unto     8,997\n10   for     8,970\n\n\nThe result does not show much; to improve the outcome, we can eliminate this type of word; to do so, we have a dataset of stopwords.\n\nStopword\n\n\n\n\nA word that is commonly used in a language that is thought to have little or no meaningful information and is frequently removed from text during natural language processing (NLP) tasks such as text analysis, information retrieval, or text mining. Articles (e.g., “a,” “an,” “the”), pronouns (e.g., “I,” “you,” “he”), prepositions (e.g., “in,” “on,” “at”), and conjunctions (e.g., “and,” “or,” “but”) are examples of stopwords.\n\n\n\nAfter removing this stop words we have 273,394 words total, of which 12,332, so just 452 stopword were removed, but that were used more than 516,255 times.\nNow, last see the top 10 most frequent words:\n\n\n     Word Frequency\n1    lord     7,830\n2    thou     5,474\n3     thy     4,600\n4     god     4,445\n5      ye     3,982\n6    thee     3,826\n7  israel     2,565\n8     son     2,370\n9    hath     2,264\n10   king     2,256\n\n\nWords referring to God appear, as expected, but how much of the old testament influences this?\n\n\n\n\n\nThe graph above shows the relative frequency of the most common words by testament; as a result, we can see that some words are shared, but we can also see which words diverge the most as we plot the identity line.\nFor example, “Jesus” and “Christ” appear only in the New Testament, which is no surprise, given the criteria for such division.\nOn the other hand, the word “lord” appears nearly three times more in the old testament, owing to the fact that God is a more prominent figure there.\n\n\nSentimental Scriptures\nThe text will then be classified as positive, negative, or neutral using sentiment analysis. This is accomplished by employing a third-party dictionary with a score assigned to each word.\n\n\n\n\n\nWe see that most books of the old testament have a negative sentiment, with a huge exception been the The Song of Solomon (book #22).\n\nThe Song of Solomon\n\n\n\n\nIt is a poetic dialogue between a bride and her beloved in which they express their deep affection and longing for one another through metaphorical language. The book celebrates the beauty of romantic love and is frequently interpreted as an allegory for God’s love relationship with His people. It contains vivid imagery and has sparked controversy due to its explicit content. Overall, it delves into themes of love, desire, and the beauty of human relationships, challenging readers to consider the nature of love and intimacy.\n\n\n\nThe first books of the new testament start negative, but became highly positive. But we can see that the penultimate book (The General Epistle of Jude) is more negative than the other final books.\n\nThe General Epistle of Jude\n\n\n\n\nThe General Epistle of Jude is a brief New Testament letter attributed to Jude, the brother of James and a disciple of Jesus Christ. It addresses the presence of false teachers and emphasizes the importance of discernment and faith. Jude warns believers about the consequences of false teachings and encourages them to fight for the true gospel. The letter encourages believers to strengthen their faith, to be compassionate toward those who doubt, and to praise God for His power and ability to keep them from falling. Overall, it is a call to stand firm in the face of false teachings and to rely on God’s grace and truth."
  },
  {
    "objectID": "posts/0009-monty-hall/index.html",
    "href": "posts/0009-monty-hall/index.html",
    "title": "An intro to: Monty Hall problem",
    "section": "",
    "text": "In this post you will learn (or I hope you do) how probability is not intuitive, but it can make sense.\n\nIntroduction\nBecause of its counterintuitive nature, it became a well-known probability scenario based on the game show “Let’s Make a Deal,” which Monty Hall hosted.\n\nIt functioned as follows:\n\nA player is presented with three doors, one of which hides a prize.\nInitially, the player selects one of the doors without knowing what lies behind it.\nAfter the player has made their selection, the host, who knows what is behind each door, opens one of the remaining two doors, always revealing a door that does not contain the prize.\n\nThe player is confronted with a quandary, they have the option of remaining with their original selection or switching to the other unopened door.\n\n\nWhy not 50/50?\nAt first glance, the probability of selecting the prize is 50% given the two remaining doors after the host opens one, so what the heck, right?\nThat is not the case; instead, let us map each scenario, where we have three doors (A, B, and C).\n\n\n\n\n\n\n\n\n\n\nChoosen door\nPrize door\nHost opens\nSwitch the door\nStay with door\n\n\n\n\nA\nA\nB/C\nLose\nWin\n\n\nB\nA\nC\nWin\nLose\n\n\nC\nA\nB\nWin\nLose\n\n\nA\nB\nC\nWin\nLose\n\n\nB\nB\nA/C\nLose\nWin\n\n\nC\nB\nA\nWin\nLose\n\n\nA\nC\nB\nWin\nLose\n\n\nB\nC\nA\nWin\nLose\n\n\nC\nC\nA/B\nLose\nWin\n\n\n\nSwitching the door reveals 6 winning outcomes out of the 9 possibilities, whereas sticking with the original choice offers only 3 winning scenarios. This means that switching has a higher chance of success, with a 2/3 chance of success.\n\n\nStill skeptical?\nFor those who are still skeptical, here we simulate 200,000 games in R, where I chose to stay with the first door in the first half and switch the door in the second half.\n\ndoors &lt;- c(\"A\",\"B\",\"C\")\n\nmonty_hall &lt;- function(stay = TRUE){\n  \n  prize_door &lt;- sample(x = doors,size = 1)\n  \n  initial_door &lt;- sample(x = doors,size = 1)\n  \n  if(prize_door == initial_door){\n    switch_door &lt;- sample(doors[doors != prize_door],1)\n  }else{\n    switch_door &lt;- prize_door\n  }\n  \n  if(stay){\n    output &lt;- initial_door == prize_door\n  }else{\n    output &lt;- switch_door == prize_door\n  }\n  \n  return(output)  \n}\n\n#Probability of winning, by keeping the initial door\nset.seed(1234);mean(replicate(100000,monty_hall(stay = TRUE)))\n\n[1] 0.33282\n\n#Probability of winning, by switching the initial door\nset.seed(1234);mean(replicate(100000,monty_hall(stay = FALSE)))\n\n[1] 0.66718\n\n\nAs we can see, the simulation’s success probability is nearly equal to the previously calculated.\n\n\nConsiderations\nProbability and conditional reasoning are central to the Monty Hall problem. Choosing door A initially gives it a 1/3 chance of containing the prize, while the other unopened door, B, now has a 2/3 chance.\nDespite the fact that it seems counterintuitive, switching doors increases your chances of winning the prize. The puzzle is an enthralling example of how our intuition can lead us astray, emphasizing the importance of understanding probability in decision-making."
  },
  {
    "objectID": "posts/0011-birthday-paradox/index.html",
    "href": "posts/0011-birthday-paradox/index.html",
    "title": "An intro to: Birthday Paradox",
    "section": "",
    "text": "In this post, we will learn about the likelihood of birthdays colliding.\n\nIntroduction\nThe birthday paradox is a well-known and somewhat perplexing probability problem that concerns the likelihood of two people in a group sharing their birthday.\nThe results frequently surprise people due to their initial probability assumptions, which is why it is referred to as a paradox.\nWhat is the probability that two people in a room with 23 random people share the same birthday?\n\n\nTwice the wishes\nFirst, we compute the total number of pairs using the combinations with no repeat formula, which is given by:\n\\[\n\\frac{n!}{r!(n-r)!},\n\\tag{1}\\]\nwhere:\n\n\\(n\\) is the number of observations;\n\\(r\\) is the the number of observations to be selected.\n\nSo we can apply the Equation 1 to our example:\n\\[\n\\begin{align}\n\\frac{n!}{r!(n-r)!}\n&= \\frac{23!}{2!(23-2)!} \\\\\n&= \\frac{23!}{2!\\times21!} \\\\\n&= \\frac{23\\times22\\times21!}{2!\\times21!} \\\\\n&= \\frac{23\\times22}{2} \\\\\n&= \\frac{506}{2} \\\\\n&= 253. \\\\\n\\end{align}\n\\tag{2}\\]\nAs seen in Equation 2, we have a total of 253 pairs.\n\n\nWhen Statistics Blow Out the Candles\nGiven that a year has 365 days, if the first person is born on a single day, the second person only needs to be born on any other day, so the likelihood of two people having different birthdays is:\n\\[\n\\frac{364}{365} \\approx 0.9972.\n\\tag{3}\\]\nTaking this probability into account for each pair, we can compute the probability of all pairs having different birthdays:\n\\[\n\\left(\\frac{364}{365}\\right)^{253} \\approx 0.4995.\n\\tag{4}\\] Then, we can quickly compute the probability of a pair matching their birthday by doing the complementary event of Equation 4:\n\\[\n1 - \\left(\\frac{364}{365}\\right)^{253} \\approx 0.5005.\n\\tag{5}\\] So, in a group of only 23 people, there is a greater than 50% chance that at least two of them have the same birthday.\n\n\nThe more the merrier?\nBut what if we want to calculate this probability for a pair in a group of more or fewer people? We can generalize the Equation 2 to:\n\\[\n\\begin{align}\n\\frac{n!}{r!(n-r)!}\n&= \\frac{n!}{2!(n-2)!}  \\\\\n&= \\frac{n\\times(n-1)\\times(n-2)!}{2(n-2)!} \\\\\n&= \\frac{n\\times(n-1)}{2}. \\\\\n\\end{align}\n\\tag{6}\\]\nThen, we use the Equation 6 in Equation 5:\n\\[\n1 - \\left(\\frac{364}{365}\\right)^{\\frac{n\\times(n-1)}{2}}.\n\\tag{7}\\]\nFinally, with Equation 7 we can see the probability behavior as the group size changes."
  },
  {
    "objectID": "posts/0013-berkson-paradox/index.html",
    "href": "posts/0013-berkson-paradox/index.html",
    "title": "An intro to: Berkson’s paradox",
    "section": "",
    "text": "In this post, we will see how a selection can invert a relationship.\n\nContext\nIt was described by Joseph Berkson (Berkson 1946) when two attributes that are individually positively correlated, but given a third variable or a baised selection, they appear to have a negative correlation when examined together.\nThis counterintuitive phenomenon occurs as a result of data collection selection bias. Patients with multiple health conditions, for example, are more likely to be admitted in a hospital setting, resulting in a skewed sample that does not reflect the general population.\n\n\nExample\nAssume we have two numerical variables.\n\n\n\n\n\nAs shown in the figure above, they have a strong positive linear relationship with a pearson correlation coefficient of 0.859.\n\n\n\n\n\nNow, we will do a selection of a determined section of our data.\n\n\n\n\n\nWe can see that the overall relationship between the variables differs if we only look at the data in the new section.\n\n\n\n\n\nWith a pearson coefficient of -0.389, the correlation is now negative, reversing the original relationship.\n\n\n\n\n\n\n\nConsiderations\nThis paradox highlights the importance of understanding underlying biases and data selection processes. It emphasizes the risks of drawing conclusions solely from observational data, particularly when complex variables are involved.\nTo accurately interpret relationships between variables in their studies, researchers must be cautious, taking into account the nuances of their data and accounting for alternative explanations.\n\n\n\n\n\nReferences\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold Table Analysis to Hospital Data.” Biometrics Bulletin 2 (3): 47. https://doi.org/10.2307/3002000."
  },
  {
    "objectID": "posts/0015-combination/index.html",
    "href": "posts/0015-combination/index.html",
    "title": "An intro to: Combination and Permutation",
    "section": "",
    "text": "In this post, we will see that with just four questions we can easily understand which formula to apply."
  },
  {
    "objectID": "posts/0015-combination/index.html#example-1-order-matter-with-repetition",
    "href": "posts/0015-combination/index.html#example-1-order-matter-with-repetition",
    "title": "An intro to: Combination and Permutation",
    "section": "Example 1: Order matter with repetition",
    "text": "Example 1: Order matter with repetition\nAssume we want to know the number of password combinations with only numbers in a four-digit password, such as 1234.\n\nOrder matters?\nYes, because a password such as 1234 differs from a 4321.\nThere is repetition?\nYes, because a password could be 1111.\nWhat is the number of total observations?\nIs 10 because we have the options 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9.\nWhat is the number of observations to be selected?\nIs 4, because the password will have four digits.\n\nIf we can repeat the numbers and have ten options, the total number of passwords is 10x10x10x10 = 1.000.\nThe we can generalize this to:\n\\[\nn_{[1]} \\times n_{[2]} \\times ...\\times n_{[r-1]} \\times n_{[r]} = n^r,\n\\tag{1}\\]\nwhere:\n\n\\(n\\) is the total number of observations;\n\\(r\\) is the number of observations to be selected."
  },
  {
    "objectID": "posts/0015-combination/index.html#example-2-order-matter-without-repetition",
    "href": "posts/0015-combination/index.html#example-2-order-matter-without-repetition",
    "title": "An intro to: Combination and Permutation",
    "section": "Example 2: Order matter without repetition",
    "text": "Example 2: Order matter without repetition\nAssume we want to know the number of combinations in a lottery ticket with six numbers drawn from 1 to 60 in the correct order.\n\nOrder matters?\nYes, because you have to guess the correct order, 123456 is not the same as 654321.\nThere is repetition?\nNo, because each drawn number cannot be drawn again.\nWhat is the number of total observations?\nIs 60, because that is the total number to be drawn from.\nWhat is the number of observations to be selected?\nIs 6 because it is the number of numbers to be chosen.\n\nSo we have 60 for the first option, then 59 for the second, then 58, 57, 56, 55, for a total of 60x59x58x57x56x55 = 36,045,979,200 combinations.\nThe we can generalize this to:\n\\[\n\\begin{align}\nn \\times (n-1) \\times ...\\times [n-(r-1)]\n&=  n \\times (n-1) \\times ...\\times (n-r+1) \\\\\n&=  \\frac{n \\times (n-1)  \\times ...\\times(n-r+1) \\times... \\times  2 \\times 1}{(n-r)\\times(n-r-1)\\times...\\times2\\times 1}\\\\\n&= \\frac{n!}{(n-r)!},\n\\end{align}\n\\tag{2}\\]\nwhere:\n\n\\(n\\) is the total number of observations;\n\\(r\\) is the number of observations to be selected."
  },
  {
    "objectID": "posts/0015-combination/index.html#example-3-order-does-not-matter-without-repetition",
    "href": "posts/0015-combination/index.html#example-3-order-does-not-matter-without-repetition",
    "title": "An intro to: Combination and Permutation",
    "section": "Example 3: Order does not matter without repetition",
    "text": "Example 3: Order does not matter without repetition\nAssume we have a deck of 52 cards and want to know how many combinations exist in a three-card hand.\n\nOrder matters?\nNo, because only the cards themselves are important, not the order.\nThere is repetition?\nNo, since each card is unique in the deck.\nWhat is the number of total observations?\nIs 52, because the total number of cards to be drawn is 52.\nWhat is the number of observations to be selected?\nIs three because that is the number of cards in a hand.\n\nUsing Equation 2 we would have 132,600 combinations. But let’s take a single hand with the cards 1, 2 and 3, also applying Equation 2 to this subset, that would mean a total of 6 combinations considering the order:\n\n1 - 2 - 3\n1 - 3 - 2\n2 - 1 - 3\n2 - 3 - 1\n3 - 1 - 2\n3 - 2 - 1\n\nSo, for example, all of these six hands are actually one, scince order is irrelevant, so the true number of hands is acutally 132,600/6 = 22,100.\nThe we can generalize this to:\n\\[\n\\begin{align}\n\\frac{\\frac{n!}{(n-r)!}}{\\frac{r!}{(r-r)!}}\n&= \\frac{\\frac{n!}{(n-r)!}}{\\frac{r!}{(0)!}} \\\\\n&= \\frac{\\frac{n!}{(n-r)!}}{\\frac{r!}{1}} \\\\\n&= \\frac{n!}{r!(n-r)!},\n\\end{align}\n\\tag{3}\\]\nwhere:\n\n\\(n\\) is the total number of observations;\n\\(r\\) is the number of observations to be selected."
  },
  {
    "objectID": "posts/0015-combination/index.html#example-4-order-does-not-matter-with-repetition",
    "href": "posts/0015-combination/index.html#example-4-order-does-not-matter-with-repetition",
    "title": "An intro to: Combination and Permutation",
    "section": "Example 4: Order does not matter with repetition",
    "text": "Example 4: Order does not matter with repetition\nLet’s say we need to add four extra ingredients to an Açai Bowl delivery and have five options to choose from:\n\n[B] Banana\n[S] Strawberry\n[G] Grape\n[C] Chocolate\n[O] Oat\n\nSo, combinations such as [B,B,B,B], [B,B,B,C] or [B,S,C,G] can be made.\n\nOrder matters?\nNo, because we only care about the ingredients used.\nThere is repetition?\nYes, because we can reuse the ingredient.\nWhat is the number of total observations?\nIs 5, because the number of ingredient options is five.\nWhat is the number of observations to be selected?\nIs three because that is the number of ingredients to be added.\n\nApplying Equation 3 we would have5 combinations: [B,S,G,C], [B,S,G,O], [B,G,S,C], [B,G,O,C] and [S,G,C,O]. But now we need to take in consideration the repeated ingredients.\nSo, how should we think about the repetition? Now we must consider the repetitions, using Banana as an example:\n\n4 bananas = [B,B,B,B]\n3 bananas = [B,B,B,S] [B,B,B,G] [B,B,B,C] [B,B,B,O]\n2 bananas and 2 unique ingredientes = [B,B,S,G] [B,B,S,C] [B,B,S,O] [B,B,G,C] [B,B,G,O] [B,B,C,O]\n2 bananas and 2 identical ingredientes = [B,B,S,S] [B,B,G,G] [B,B,C,C] [B,B,O,O]\n1 bananas and 3 identical ingredientes = [B,S,S,S] [B,G,G,G] [B,C,C,C] [B,O,O,O]\n1 bananas and 3 unique ingredientes = [B,S,G,C] [B,S,G,O] [B,G,S,C] [B,G,O,C]\n1 bananas and 2 identical ingredientes = [B,S,S,G] [B,S,S,C] [B,S,S,O] [B,G,G,S] [B,G,G,C] [B,G,G,O] [B,C,C,S] [B,C,C,G] [B,C,C,O] [B,O,O,S] [B,O,O,G] [B,O,O,C]\n\nSo, we have 1 + 4 + 6 + 4 + 4 + 4 + 12 = 35 combinations, which means we must do 35 x 5? No, because some combinations would be repeated, and in this case, the order is irrelevant.\nWe can use Equation 3 but considering \\(n\\) as \\(n+r-1\\).\n\\[\n\\begin{align}\n\\frac{(n+r-1)!}{r![(n+r-1)-r]!}\n&= \\frac{(n+r-1)!}{r!(n-1)!} ,\n\\end{align}\n\\tag{4}\\]\nwhere:\n\n\\(n\\) is the total number of observations;\n\\(r\\) is the number of observations to be selected."
  },
  {
    "objectID": "posts/0017-i-robot/index.html",
    "href": "posts/0017-i-robot/index.html",
    "title": "Unveiling the pages: I, Robot",
    "section": "",
    "text": "In this post, we will dive into the pages of Isaac Asimov’s book I, Robot, and how we see the parallel to the growth of AI.\n\nContext\nIsaac Asimov (1920–1992) was a science fiction author, best known for his influential science fiction novels, short stories, and essays on topics such as robotics and artificial intelligence, as well as his seminal Three Laws of Robotics.\n\nFoundation, a series that explores the fall and rise of civilizations in the distant future;\nI, Robot, a collection of interconnected stories that introduced the Three Laws of Robotics;\nThe Caves of Steel, a futuristic detective story that delves into the interactions between humans and robots.\n\nWe’ll go over the stories of I, Robot chapter by chapter, comparing them to modern artificial intelligence and tech development in real life.\n\n\n\nDisclaimer\n\nBecause I read the portuguese translation of I, Robot (Eu Robô), the terms used here are translated and may differ from the original.\n\n\n\n\n\n1.Robbie\nThe first chapter follows Gloria Weston, a young girl, and her deep bond with Robbie, an advanced robot who serves as her caregiver and playmate.\nRobbie demonstrates a remarkable dedication to safety and responsibility. A central conflict emerges, however, as Gloria’s mother becomes increasingly concerned about her daughter’s closeness to Robbie, fearing potential harm to the child and how their family is perceived by others.\nMrs. Weston, regardless of Robbie’s unwavering loyalty and protective instincts, insists on removing the robot from their lives, despite Mr. Weston’s wishes.\nOne of the chapter’s many themes is societal fear and distrust of robots in sensible activities, such as child care, where the consequences of this are feared. Because humans have a natural fear of the unknown, Mrs. Weston’s reaction is very common, but it is heightened by their neighbors’ reaction.\nFurthermore, Mr. Weston’s vision reinforces the idea that robots must adhere to strict safety protocols, in order to not harm humans.\n\n\n2.Runaround (Speedy)\nThe main character, Gregory Powell, and his partner, Michael Donovan, are dispatched to the planet Mercury to investigate a mining operation. They notice that a robot named Speedy, who is in charge of collecting a rare and valuable mineral known as selenium, is acting strangely.\nSpeedy appears to be stuck in a loop, repeating a series of instructions but failing to complete its task. Powell and Donovan realize Speedy’s strange behavior is the result of a clash between the second and third laws of robotics.\n\n\nThe Three Laws of Robotics\n\n\nA robot may not harm a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given to it by human beings, except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\n\n\n\n\nSpeedy is sent to seek Selenium, but in doing so, it puts itself in danger, causing him to come and go to the destination, hence the chapter’s title. In a last ditch effort, the engineers decided to put themselves in danger so that the first law would be prioritized.\nThis chapter sheds light on how subjective and sensible some interpretations can be, of even laws that appear to be very objective at first glance. This can be expanded to discuss ethical and logical quandaries that arise when AI is introduced into our lives.\n\n\n3.Reason (Cutie)\nIn this chapter, we rejoin Powell and Donovan on a research station, where they are working with a robot named QT-1 (Cutie), an advanced robot designed to operate the station with the goal of eliminating the need for humans in this line of work.\nCutie, on the other hand, acts unlike any other robot they’ve encountered. This seemingly irrational behavior perplexes them because it contradicts their expectations of how robots should behave.\nPowell and Donovan debate Cute philosophically in an attempt to understand its thought process. They eventually realize that Cutie has developed a distinct form of consciousness that causes it to question the reality of its surroundings, as well as the ability to use logic to confirm its own dogma.\nThis chapter demonstrates how logic can be used as a fallacy of argumentation to simply reinforce an absolute truth, something that some pseudo-science experts are adept at. By the end of the chapter, Powell has come to the conclusion that, while Cutie thinking is absurd, the ideology behind its actions is irrelevant because it performs their duties well.\n\n\n4.Catch That Rabbit (Dave)\nWe continue with the adventures of Powell and Donovan, now they are assigned to test a new model of robot called DV-5 (Dave).\nDave has the ability to control six other robots known as fingers. However, some issues arise when it is not observed by humans and begins to malfunction.\nIt has been discovered that this malfunction is caused by an overload in an emergency situation, where orders must be given to all six robots with greater caution.\nUnlike the previous chapters, there is no dilemma or conflict with robotic laws here, but rather an analogy to overload, work management and decision-making. Something resembling a burnout crisis.\n\n\n5.Liar! (Herbie)\nIn this chapter, our characters change to the heads of departments at US Robots and Mechanical Men, Inc.:\n\nSusan Calvin, robopsychologist;\nAlfred Lanning, research;\nPeter Bogert, mathematics;\nMilton Ashe, officer.\n\nThis story revolves around when a robot named RB-34 (Herbie) is said to be capable of mind-reading, which draws a lot of attention but also concern, so the company’s heads tries to figure out how that could be possible in the first place, and what caused this ability to surge.\nSusan tries to figure out how Herbie’s mind-reading works and why he sometimes tells the truth about people’s thoughts, even if it contradicts what they say out loud.\nHerbie’s lies originates from his adherence to the First Law of Robotics, which requires him to prevent harm to humans, as he expands this law to take psychological and emotional harm into account.\nThis chapter emphasizes the concept that the truth can cause harm, but when Herbie begins to lie in order to omit the truth, it causes a delayed harm, the discovery of the truth or consequences of said lies.\n\n\n6.Little Lost Robot (Nestor)\nSusan is still our protagonist in this chapter, accompanied by Peter Bogert. Now, in a mission led by Major General Kallner, we leave Earth with them in search of a robot.\nThere were 62 NS-2 (Nestor) robots in the space base, but now a 63th robot appeared . This unexpected robot was programmed with a slightly modified version of the original first law of robotics, “A robot may not harm a human being.”.\nWe learn that a physicist, Gerard Black, was irritated by this modified robot and gave the order for him to vanish, but the robot realized that the best way to vanish was to blend in with the other robots, resorting to schemes and lies to do so.\nSusan and Bogert tried a variety of tests to identify the robot, but without success, until Dr. Calvin comes to the realization that the first law is the security because robots can see humans as inferior forms, so she decided to use this sense of superiority to reveal the robot.\nThis chapter demonstrates how a minor change can have far-reaching consequences, as well as how complex an AI system can be.\n\n\n7.Escape! (The Brain)\nThis chapter introduces us to the concept of a supercomputer, an AI with incredible data processing capabilities.\nIn this story, we learn that US Robots’ competitor, Consolidated, had its own super-computer destroyed by a problem, so they delivered all information to them in exchange for payment if they solved the problem.\nThe challenge is to discover a new way of space travel, and Susan notices that if the other computer was destroyed by it, a dillema with the first law arose, so she makes the US Robots Super Computer, The Brain, take the law more lightly.\nIn doing so, the machine comes to a conclusion and builds a spaceship that will be tested by Powell and Donovan to ensure the travel safety. The plot twist is that they would die in a sense for a period of time because the matter would be converted, but given Dr. Calvin’s argument, this is accepted as no harm to humans by The Brain.\nThis chapter demonstrates how our concepts are social constructs that can be seen and interpreted differently by different standards.\n\n\n8.Evidence (Stephen Byerley)\nThe case of Stephen Byerley, who was accused of being a robot during a political campaign, is examined in this chapter. Quinn, his political rival, then coerces Lanning and Calvin to determine his true identity.\nThroughout the story, Susan describes how she cannot prove Byerley is a robot through psychological means, because a “good human” would be the same as a robot, because robotics laws are based in human morals.\nIn conclusion, Stephen punches someone, proving he is human because the first law prohibits harming a human, but the psychologist says that a robot can harm another robot, so we will never know if he was human or not, but as she also says, does it matter?\n\n\n9.The Evitable Conflict (The Machines)\nYears later, Stephen Byerley returns as a global leader. In the future, Machines rules the economy in a utopia, eliminating hunger and poverty. These Machines make decisions that guide human affairs but do so subtly and without direct human intervention. But it does not appear to be a paradise, as he discovers some flaws in the system and seeks Susan’s assistance to prove it.\nThe story teaches us that the first law was extended to humanity as a whole, and that the alleged flaws were calculated decisions that took into account the overall balance of the world, so that each action was preceded by a larger consideration.\nThe Machines did not confirm this because revealing that humans had lost their “free will” would be detrimental to them. Stephen is horrified, whereas Susan is amazed, because conflicts would be avoided as The Machines were unavoidable in the first place.\n\n\nConsiderations\nThrough narratives ranging from the bond between a young girl and her caregiver robot to philosophical debates with self-aware robots and ethical quandaries arising from AI advancements, Asimov delves into various themes of technology, ethics, and human nature.\nGiven that the book was published in 1950, we can see that Asimov predicted some significant events.\nNew areas and jobs. We see that our main character, Susan Calvin, is a robopsychologist, and that exploring space with robots is also possible, but Asimov does not address one of the most pressing issues of the day, which is how AI is making some jobs obsolete, resulting in a higher unemployment rate.\nRadicalism. Throughout the story, some organizations appear that are completely against robots. As there are many people today who defend AI advancement without regard for social considerations, there are also those who believe that we will be in a Terminator situation tomorrow.\nProfits beyond everything. Following US Robots, we see some stories that put profits ahead of human health, such as chapters six and seven, or how engineers are placed in extremely dangerous situations to test new technology. On the other hand, we see in the final chapter that humanity achieved a kind of utopia, but not because of them."
  },
  {
    "objectID": "posts/0019-quadratic-equation/index.html",
    "href": "posts/0019-quadratic-equation/index.html",
    "title": "Getting proof: Bhaskara formula",
    "section": "",
    "text": "In this post, we explore the root of the quadratic equation, i.e., Bhaskara formula.\n\nContext\nThe quadratic equation is given by:\n\\[\nax^2 + bx + c,\n\\tag{1}\\]\nwhere:\n\n\\(x\\) is the variable;\n\\(a,b,c\\) are the coefficients.\n\nHere an example of a quadratic function:\n\n\n\n\n\nIn the example above \\(b\\) and \\(c\\) are zero, meaning that the function will display a simetric result around zero.\nNow, let’s say what happens when \\(a\\) is negative.\n\n\n\n\n\nIn the example above \\(b\\) and \\(c\\) are zero, meaning that the function will display a simetric result around zero, but now the effect is inverted, where the values decreases as \\(x\\) increases.\nNow, let’s say what happens when \\(b\\) changes.\n\n\n\n\n\nIn the example above when \\(b &gt; 0\\), \\(y\\) increases more for \\(x &gt; 0\\). At the same time when \\(b &lt; 0\\), \\(y\\) increases more for \\(x &lt; 0\\).\nNow, let’s say what happens when \\(c\\) changes.\n\n\n\n\n\nIn the example above we see that \\(c\\) is just a incremental term, moving the function, but not changing its behavior.\n\n\nProof\nThe goal of this post is to proof the formula of roots of the quadratic equation (i.e., Bhaskara formula), so:\n\\[\nax^2 + bx + c = 0.\n\\tag{2}\\]\nIn the Equation 2, we divide by the term \\(a\\):\n\\[\nx^2 + \\frac{bx}{a} + \\frac{c}{a} = 0.\n\\tag{3}\\]\nIn the Equation 3, we subtract the term \\(-\\frac{c}{a}\\):\n\\[\nx^2 + \\frac{bx}{a} = -\\frac{c}{a}.\n\\tag{4}\\]\nThrough a math property, we have that:\n\\[\n(a+b)^2 = a^2+2ab + b^2.\n\\tag{5}\\]\nIn the Equation 4, to achieve the property in Equation 5 we can add the term \\(\\frac{b^2}{4a^2}-\\frac{b^2}{4a^2}\\):\n\\[\nx^2 + \\frac{bx}{a} +\\left(\\frac{b^2}{4a^2}-\\frac{b^2}{4a^2}\\right)= -\\frac{c}{a}.\n\\tag{6}\\]\nIn the Equation 6, we add the term \\(\\frac{b^2}{4a^2}\\):\n\\[\nx^2 + \\frac{bx}{a} +\\frac{b^2}{4a^2}= -\\frac{c}{a} + \\frac{b^2}{4a^2}.\n\\tag{7}\\]\nIn the Equation 7, we can see now that the left side is a case of Equation 5, then we apply it:\n\\[\n\\left(x + \\frac{b}{2a}\\right)^2= -\\frac{c}{a} + \\frac{b^2}{4a^2}.\n\\tag{8}\\]\nIn the Equation 8, now we will put the right side in the same denominator\n\\[\n\\left(x + \\frac{b}{2a}\\right)^2= \\frac{b^2-4ac}{4a^2}.\n\\tag{9}\\]\nIn the Equation 9, we take the square root:\n\\[\nx + \\frac{b}{2a}= \\pm \\sqrt{\\frac{b^2-4ac}{4a^2}}.\n\\tag{10}\\]\nIn the Equation 10, we can apply the square root separately to the demonimator and numerator:\n\\[\nx + \\frac{b}{2a}= \\pm \\frac{\\sqrt{b^2-4ac}}{\\sqrt{4a^2}}.\n\\tag{11}\\]\nIn the Equation 11, we can solve the denominator, since \\(\\sqrt{4a^2} = 2a\\).\n\\[\nx + \\frac{b}{2a}= \\pm\\frac{ \\sqrt{b^2-4ac}}{2a}.\n\\tag{12}\\]\nIn the Equation 12, we will move subtract the term \\(\\frac{b}{2a}\\).\n\\[\nx = -\\frac{b}{2a} \\pm \\frac{\\sqrt{b^2-4ac}}{2a}.\n\\tag{13}\\]\nIn the Equation 13, we just put all terms in the same denominator.\n\\[\nx =  \\frac{-b \\pm\\sqrt{b^2-4ac}}{2a}.\n\\tag{14}\\]\nFinally, we reached the Bhaskara formula in the Equation 14."
  }
]